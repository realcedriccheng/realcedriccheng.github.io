[{"content":"指针和引用 指针是占用内存地址的独立变量，内存中存储的就是一个地址。而引用是某个变量的别名，不占用独立内存空间。\n指针定义时可以不初始化，但是最好初始化，指向的内容可以变，存在空指针。引用定义时必须初始化，绑定后不可改变，不存在空引用。\n使用指针访问地址内容时需要星号解引用，引用不需要。\n指针一般用于动态内存分配，数组操作以及函数参数传递。引用通常用于函数传参，操作符重载和创建别名。\n数据类型 short至少16位、int至少与short一样长、long至少32位、long long至少64位。一般int就是32位和long一样。为了避免歧义可以使用uint32_t/int32_t\n关键字 const关键字用于修饰只读变量 必须定义时就赋初值。\n对于指针：\n底层const是const int *a = 地址或者int const *a =地址，指针可以指向别的地址，但是a的内容不能变 顶层const是int *const a = 地址，指针只能指向这个地址，但是地址存放的内容可以变 常量引用：const int \u0026amp;a = b引用的值是常量，不能通过引用修改。\n常量成员函数：\n1 2 3 4 class Hero { public: int getBlood() const {...} }; 大括号之前写const，表示不会修改对象的非静态成员变量，但是可以修改静态成员变量。只能调用其他常量成员函数，不能调用非const成员函数（避免事实修改）\n常量对象：const Hero hero(\u0026hellip;);必须在创建对象时调用构造函数，使用初始化列表。创建后不可修改成员变量，只能调用常量成员函数。\n常引用参数：void foo(const string\u0026amp; str)确保参数不会被修改，可以绑定右值。\n常量指针参数：也分为底层和顶层，保护指针指向地址不变或指针地址指向值不变。\nconst和constexpr const表示只读，而constexpr表示常量。\n• const的值可以由函数返回值等在运行时初始化，而constexpr必须在编译时由常量表达式初始化\n• const可以修饰成员函数，表示不改变非静态成员变量的值，但是不能修饰普通函数；constexpr可以修饰函数，意思时能在编译期调用并返回函数值，注意不能动态分配内存，必须非常简单。\n1 2 3 4 constexpr int factorial(int n) { return (n \u0026lt;= 1) ? 1 : n * factorial(n - 1); // C++14起支持递归[1,6](@ref)。 } constexpr int result = factorial(5); // 编译期计算结果为120[1,4](@ref)。• constexpr可以将一些计算提前到编译期做，避免运行时开销。constexpr还可以用于检查一个表达式是否是常量表达式 static关键字定义静态变量和函数 静态变量：static修饰的局部变量或全局变量，生命周期和进程一样长，不会因离开函数作用域而销毁，默认初始化为0。可以用于计数函数调用次数、在递归函数中缓存中间结果等。\n全局静态变量和全局变量的区别在于全局静态变量不能在其他文件中通过extern关键字引用。\n静态函数是static修饰的普通函数，作用域仅限当前文件，不可被其他文件通过extern声明。\n静态成员变量是类内用static修饰的成员变量，属于类而不是对象。所有实例共享同一份数据，必须在类外单独初始化。\n静态成员函数是类中使用static修饰的成员函数，静态成员函数不能直接访问非静态成员或调用非静态成员函数，可以通过类名调用而需要创建类的示例.\n静态成员或静态成员函数可以通过实例调用，但是不合适。1. 容易混淆静态和非静态成员2.最好使用类名调用\nconst和static不能一起使用，因为静态成员函数不含有this指针，而const成员函数必须具体到某一实例。\nMutable const函数不修改对象成员，但是某些成员如计数器等要求修改，则可以用，mutable修饰，在const中可以修改这些成员。\ndefine和typedef • define只是字符串替换，没有类型检查，作用于编译的预处理阶段，可以防止头文件重复引用（写头文件的开头结尾有#ifndef和#endif）。不分配内存，有多少次使用就有多少次替换\n• typedef为类型定义别名，作用于编译处理阶段，有对应的数据类型，在静态区分配空间，程序运行过程中内存中只有一个拷贝\ndefine和inline • inline用于定义内联函数，在调用函数处将函数展开，因此能够避免调用函数带来的压栈、跳转等开销\n• 必须是小的函数体，不能存在循环，必须减少条件判断\n• inline先编译再插入，在编译阶段。define在预处理阶段展开\ndefine和const const和define都可以用于定义常量\n• const在编译阶段、define在预处理阶段\n• const定义的常量放在内存中，define直接替换\n• const常量有类型，define没有\nusing和typedef 都可以定义别名\ntypedef int Integer\nusing Integer = int建议使用using，因为可读性更强，更简洁。\nusing还能用来引入命名空间，引入基类成员。\n通过using namespace \u0026hellip;将命名空间内所有成员引入当前作用域，因此不需要前缀。但是最好不要在头文件中使用，以免导致命名冲突。过度使用会降低可读性，不好确定成员是哪个命名空间的。\n通过using std::cout可以引入特定成员，减少命名冲突风险。\n（要不跳过吧）通过在派生类中声明using Base::Base 继承基类构造函数，避免重复定义。在派生类中使用Base::func可以将基类中private的func提升为派生类的public成员。当派生类定义同名成员时可以显式保留基类成员。\n命名空间 命名空间通过将代码元素封装在特定作用域内避免变量的命名冲突。\n例如定义两个命名空间，可以通过两个冒号解析\n1 2 3 4 namespace NS1 {int *a}; namespace NS2 {int *a}; NS1::a; NS2::a; new和malloc都可以用于动态内存分配 • new分配对象大小的内存后调用构造函数初始化对象，返回对象指针；malloc需要指定分配大小，并且返回void指针\n• 空间不足时，new抛出异常，而malloc返回空指针\n• new必须用delete释放，调用析构函数；malloc必须用free释放，不调用析构函数\n• malloc在堆上分配空间，new在自由存储区分配（允许自定义内存池，但一般也是堆）。\nmalloc在底层使用malloc分配，但是会额外完成构造逻辑，通过重载new可以将自由存储区指向其他区域。\ndelete和free • delete会调用析构函数，free仅释放内存，可能导致资源泄漏，如未关闭文件等。\n• delete可以正确释放new直接创建的数组，free必须手动循环释放数组的每一项\nextern 声明在其他文件定义的非静态变量或非静态函数\ni++, ++i\ni++返回的是i，++i返回的是i+1。\ni++创建了一个临时对象保存i，并返回临时对象。\nstd::atomic 确保对变量的读写、修改等操作不可分割，操作过程中不会被其他线程打断。\nstruct和class struct和class的功能几乎完全一样，只是默认行为不同。struct默认成员、继承方式都是公有的，class默认私有。\n函数指针 指针指向的是函数的地址（代码段），允许在运行时动态选择要调用的函数。\nint (*func)(int, int)\n如果省略括号就会编程返回指针的函数\n可以用于实现1. 回调函数机制，将函数指针作为参数传递2.根据不同参数选择不同函数处理\n强制类型转换 四种强制类型转换的关键字：static_cast，dynamic_cast，reinterpret_cast和const_cast\n• static_cast用于截断或扩展整数、上行转换（将派生类转换为基类） ◦ 编译时检查，无运行开销 ◦ 下行转换不安全，因为没有动态类型检查 ◦ 支持void*和整数之间的互换\n• dynamic_cast用于动态类型转换，可以将基类指针转换为派生类指针 ◦ 运行时有类型检查，转换失败返回空指针，引用的话抛出异常 ◦ 通过虚函数表获取对象的实际类型信息，检查目标类型是否是实际类型\n• const_cast用于去除const属性，但是如果对象是真正的常量，则会导致未定义行为。编译器也有可能将const优化成字面量，导致修改无效。\n• reinterpret_cast用于无关类型的转换，如int转为char，基于内存的位重新解释，绕过类型系统，高风险。\nstatic是按类型规则逻辑处理数据，但reinterpret是物理上重新解释内存\nC++内存管理 堆和栈 栈是由编译器自动管理，存放局部变量、函数参数以及返回地址，通过移动栈指针分配内存，栈上变量的生命周期和所在函数相同。\n堆由用户手动管理，存放动态分配数据，生命周期由用户控制。\n栈分配得块，因为进程本身就有一段连续的段空间，仅需调整栈指针就可以分配和回收。每个线程有独立的栈，栈的容量较小。\n堆不是先进后出的，因此每次从堆中分配内存时需要先通过空闲内存块链表搜索合适大小的内存块，如果没有再移动堆顶指针。可能还会触发碎片整理。同一个进程的不同线程共享堆空间，因此可能需要同步机制。\n内存分区 栈\n堆\n全局/静态：bss未初始化，data初始化\n只读数据段.rodata\n代码段\n内存泄漏 内存泄漏是程序分配内存后失去了对该内存的控制，因此无法释放。\n• 堆内存泄漏：通过malloc，new等动态分配的对象忘记释放\n• 系统资源泄漏：程序使用系统分配的资源，如socket等，却没有释放或关闭。\n• 对象泄漏：没有正确定义析构函数或没有将基类的析构函数定义为虚函数，这样导致基类指针指向的派生类对象会调用基类的析构函数，从而无法正确释放派生类资源\n防止内存泄漏的方法有：将内存的分配和释放封装在类中，从而避免忘记释放；使用智能指针。\n智能指针 智能指针用来避免内存泄漏，生命周期后会自动释放资源，不需要用户手动释放。\n3种智能指针： • 独占智能指针unique_ptr：一个独占指针唯一拥有资源，别的指针不能指向他管理的内存。没有额外开销，性能接近于原生指针 • 共享智能指针shared_ptr：允许多个指针指向同一个对象，利用计数器机制管理。只有最后一个指针销毁时才释放。但是可能导致循环引用 • 弱引用指针weak_ptr：不增加引用计数，因此不会导致循环引用，例如父节点通过shared_ptr引用子节点，子节点通过weak_ptr引用父节点\n野指针和悬浮指针 野指针是指向无效或位置内存地址的指针，使用野指针会导致不可预知的行为。\n未初始化的指针，其值为随机内存地址，如栈和堆中的垃圾地址\n释放后未置空的指针，内存值已不可用，如释放了参数指针\n返回局部变量的指针（变量在函数作用域外被销毁）\n指针越界访问（数组越界、分配内存之外等）\n悬空指针是指向已释放内存的指针，是野指针的一种。\n内存对齐 内存对齐是指数据在内存中的存储起始地址是某个值的倍数。\n结构体中可能包含不同类型的变量，变量按照声明的顺序放置，第一个成员和整个结构体的地址相同。\n出于CPU访问效率的考虑，变量的起始地址应当在长度的整数倍上，比如4字节的int，起始地址应该是4字节的整数倍。\n乱序执行 • CPU层面，允许不共享数据的指令并行执行\n• 编译器层面允许调整无数据以来的指令顺序、将循环体展开为多个迭代以减少分支预测、分支预测优化。\n单线程场景可以提升执行效率，多线程场景可能导致数据竞争，线程同步关系颠倒可能导致逻辑错误\n可以采用原子类型std::atomic保证操作的原子性、插入内存屏障指令限制某些重排序。\nstl string实际不属于stl，而是C++标准库的独立组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 构造 string s1; // 空，默认构造函数 string s2(\u0026#34;Hello\u0026#34;); // 默认构造函数 string s3(5, \u0026#39;c\u0026#39;); // 默认构造函数 string s4(s2); // 拷贝构造 // 子串 string s5 = s2.substr(0,3); // 起始，偏移量 getline(istream \u0026amp;in, string \u0026amp;s);size(), length()返回字符个数 empty()判断是否空 capacity()返回分配的存储容量，不含\u0026#39;\\0\u0026#39; resize(int len, char c)就是truncate clear()清空字符串内容，但是保留容量 reserve()预分配内存减少扩容开销 []通过下标访问，不检查越界。\nat()通过下标访问并检查越界\nfront(), back()访问首尾字符\nc_str()返回以\\0结尾的c风格字符串\n+= / append()追加字符串或字符\npush_back()追加字符\ninsert(pos, str)在pos处插入str\nreplace(pos, len, str)替换从pos开始的len个字符为str\nerase(pos, len)删除从pos开始的len个字符\nfind(str, pos)从pos开始查找str，返回首次出现的位置\nrfind(str, pos)反向查找\nfind_first_of(str)返回包含str任意字符的首个位置\n查找失败返回npos（-1的无符号形式）\n==和\u0026lt;\u0026gt;直接比较（字典序）\ncompare(str)返回0（相等），正数（大于），负数（小于）\nbegin(), end(), rbegin(), rend()等迭代器\n程序编译过程 • 源代码 • 预处理 ◦ 宏替换、包含头文件（将头文件内容插入源文件）、删除注释 • 编译 ◦ 生成汇编代码.s文件。词法分析、语法分析、语义分析、中间代码生成与优化 • 汇编 ◦ 将汇编代码翻译为机器指令，生成目标文件.o，有代码段、数据段、符号表 • 链接 ◦ 链接器将多个目标文件和库文件合并为可执行文件 ◦ 跨文件符号解析、地址重定位 ◦ 静态链接将库代码拷贝到可执行文件、动态链接在运行时加载共享库\n初始化 int a = 1; int a(1)和int a = {1}的区别\nint a = 1是复制初始化，编译器先将右值转换为目标类型对象，然后调用拷贝构造函数初始化左侧对象。\nint a (1)调用了int类的构造函数，1是参数s。\nint a = {1}是列表初始化，可以避免隐式类型转换带来的窄化，因为编译器会检查是否窄化。\n右值引用用来实现移动语义和完美转发 左值是占用一定内存空间的，可以取地址的值。右值是不占用地址空间的，一般是立即数等。\n左值的生命周期取决于变量类型，而右值一般执行完所在语句后就死亡。\n右值引用，例如int \u0026amp;\u0026amp; a = 1；可以将1这个右值的生命周期延长到作用域结束。\n右值引用可以实现移动语义，也就是将资源从一个所有者移动到另一个所有者，而不需要拷贝。例如某个函数的返回值是一个对象，而函数的返回值是一个右值，因为函数结束后，变量所在作用域就结束了。因此本来调用函数的语句执行完后返回值就死亡了，是一个右值。然而使用右值引用可以延长其生命周期，可以用其初始化另一个对象。该类具有移动构造函数的情况下，会把对象的成员直接转移给新对象，而不是调用拷贝构造函数复制。\n完美转发一般用于模板函数，可以在传递参数时原样转发，而不是构造一个新的参数。通过std::forward实现\n移动语义只适合实现了移动构造函数的类，对于int，double等还是会复制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 int\u0026amp;\u0026amp; rv = 42; // 绑定右值 std::string s = \u0026#34;hello\u0026#34;; std::string\u0026amp;\u0026amp; s_rv = std::move(s); // 转换为右值引用 class MyString { public: // 移动构造函数 MyString(MyString\u0026amp;\u0026amp; other) noexcept : data(other.data), size(other.size) { other.data = nullptr; // 原对象资源置空 other.size = 0; } private: char* data; size_t size; }; std::vector\u0026lt;MyString\u0026gt; vec; MyString tmp(\u0026#34;test\u0026#34;); vec.push_back(std::move(tmp)); // 调用移动构造函数而非拷贝 void process(int\u0026amp; x) { /* 处理左值 */ } void process(int\u0026amp;\u0026amp; x) { /* 处理右值 */ } template \u0026lt;typename T\u0026gt; void relay(T\u0026amp;\u0026amp; arg) { process(std::forward\u0026lt;T\u0026gt;(arg)); // 保持arg的原始类型 } int main() { int a = 10; relay(a); // 调用process(int\u0026amp;) relay(20); // 调用process(int\u0026amp;\u0026amp;) } 面向对象特性 CPP的面向对象特性包括 封装、继承、多态\n封装 将数据和对数据的操作捆绑在一个类中，通过访问控制隐藏内部实现，对外暴露有限接口。作用：数据安全，避免外部直接修改或访问私有成员；模块化，类便于复用和维护；简化使用，用户无需了解具体实现 访问权限\npublic - 公有成员可以被外部访问\nprotected - 保护成员只能被类内部、友元函数和派生类函数访问\nprivate - 私有成员只能被类内部访问\n友元：允许非成员函数或其他类访问protected和private。单向的，不具有传递性。\n构造函数和析构函数 构造函数在对象创建时自动调用初始化数据成员，与类名完全相同，没有返回值，可以重载。\n构造函数不能是虚函数。\n• 默认构造函数 ◦ 无参数时自动构造，不会初始化int等内置类型。string有自己的构造函数所以初始化为空字符串 ◦ 如果没有定义任何构造函数则会生成默认构造函数，如果有构造函数则需要自己定义不带参数的构造函数 ◦ 同时定义无参构造函数和全缺省构造函数会产生冲突（a(){}和a(int a = 0){}）\n• 带参数构造函数 ◦ 自己定义的构造函数\n• 拷贝构造函数 ◦ 使用一个对象构造一个新对象 ◦ 浅拷贝：对指针成员仅复制指针地址，若原对象销毁则指针内容会释放 ▪ 需要自定义深拷贝函数，创建空间并将值复制过来\n• 移动构造函数 ◦ 通过资源转移将原对象的成员转移到自身，不需要拷贝。原对象不再拥有资源，可以安全删除。 ◦ 参数必须是右值引用，如果要处理左值需要用std::move显式转换\n析构函数在对象生命周期结束时释放资源，包括局部对象离开作用域、delete删除对象。函数名为波浪号加类名，不允许带参数，不可重载。自动调用。默认析构函数不会释放资源，因此必须自己定义析构函数。基类的析构函数需要定义为虚函数。\n构造顺序：基类构造函数 - 成员构造函数 - 自己的构造函数\n析构顺序反向：自己析构函数 - 成员析构函数 - 基类析构函数\n构造函数不能是虚函数，基类的析构函数需要设为虚函数\n构造函数不能是虚函数，因为虚函数有虚函数表动态绑定，而虚函数表由对象的虚指针指向。虚指针在构造函数中初始化，因此构造函数无法使用虚指针。另一方面，虚函数的语义是根据对象的实际类型实现运行时多态，而构造函数正是用于确定对象的具体类型，因此有语义冲突。\n基类的析构函数需要是虚函数，否则通过基类指针或引用释放派生类对象时会调用基类的析构函数，导致无法正确释放资源。但是如果是子类指针指向则可以正确析构。\n继承 允许派生类继承基类的非私有属性和方法。作用：派生类可以复用基类代码减少重复实现、派生类可以扩展基类代码实现更多功能\n• 继承方式\n◦ public：表达特例关系，例如学生是一种人，因此学生具有人的所有成员 ▪ 成员属性保持不变 ◦ protected：可以用于实现多级继承中的中间层，既不对外暴露成员，又允许再次继承 ▪ public和protected变为protected ◦ private：实现新功能，例如用链表实现栈，但是不暴露链表接口，只保留栈的接口 ▪ public和protected变为private ◦ 三种继承方式，派生类都不能访问基类的private成员 多重继承 允许一个派生类同时继承多个基类，基类构造函数的调用顺序取决于派生类声明时的排列顺序，而不是参数列表顺序。析构函数的调用顺序于构造函数相反。\n允许组合不同基类的功能，例如基类手机、电脑等和基类输出电池容量（纯接口类）。\n• 二义性\n◦ 多个基类有重名成员，需要用两个冒号指明哪个基类 • 菱形继承\n◦ 多个中间基类来自同一个基类，则派生类继承多个相同成员副本 ◦ 确保中间基类使用虚继承，则派生类仅保留一份副本 ▪ 虚继承就是继承时加virtual关键字，虚继承的用处就是解决菱形继承 1 2 3 4 5 6 7 class A { int a; int b; } class B : virtual public A {} class C : virtual public A {} class D : public B, public C {} • 最好用组合代替多重继承，避免深层次多重继承导致难以维护\n派生类会调用基类构造函数和析构函数\n创建派生类对象时会调用基类构造函数。如果有默认基类构造函数会隐式调用，否则必须显式调用。\n1 2 3 4 5 6 7 8 9 10 11 class A { int a; public: A(int x) : a(x) {} }; class B : public A { int b; public: B(int x) : b(0) {A(x);} B(int x, int y) : b(y) {A(x);} }; 构造函数调用顺序：基类构造函数 - 派生类成员构造函数 - 派生类构造函数\n析构时会自动调用派生类的析构函数，再自动调用基类的析构函数。注意，如果用基类指针指向派生类，而基类的析构函数未声明为虚函数，则导致派生类析构函数未调用。因此最好将基类的析构函数声明为虚函数。\n多态 同一操作作用于不同对象表现不同特性，分为编译时多态和运行时多态\n• 编译时多态：无运行时开销，但无法根据运行情况调整行为 ◦ 函数/运算符重载/模板 ▪ 函数重载：同一作用域中定义多个同名函数，参数列表不同 ▪ 运算符重载：自定义类对象的运算操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // 左操作数是当前类的对象，则用成员函数重载 class vector { private: int x; int y; public: vector operator+(const vector \u0026amp;v) { // 隐含this-\u0026gt;x return vector(x+v.x, y+v.y); } }; // 左操作数不是当前类对象，用友元函数重载。注意友元关系无法继承 class array { private: int *s; int size; public: friend ostream \u0026amp;operator\u0026lt;\u0026lt;(ostream \u0026amp;os, const array \u0026amp;arr) { for (int i = 0; i \u0026lt; arr.size; ++i) { os \u0026lt;\u0026lt; arr.s[i]\u0026lt;\u0026lt;\u0026#34; \u0026#34;; } } }; ▪ 模板：泛化参数类型从而定义可以处理多种数据类型的函数或类 1 2 3 4 5 6 7 8 9 10 template \u0026lt;typename T\u0026gt; class stack { T data[100]; // 函数操作 }; T add(T a, T b) { // 如果a是整型，b是浮点就会编译失败，注意这是编译时多态，不会等到运行时 return a + b; } • 运行时多态：允许程序通过对象的实际类型调用对用的函数 ◦ 重写 ▪ 派生类重写并覆盖基类的函数，根据对象实际类型调用该对象的函数。 ▪ 基类的函数必须声明为虚函数，参数列表、返回类型等声明必须完全一致，但是访问权限可以不同（变得更开放） ▪ 不能重写private函数（根本看不见），静态方法不能重写为非静态方法 ▪ 重写时可以在大括号前写override，但是也可以不写 • 不写override则编译器不会检查签名是否一致，若不一致不会报错，而是将重写的函数视为新函数（重定义），导致基类函数隐藏，无法使用多态（基类指针指向派生类对象，只能使用基类函数， 派生类指针指向派生类对象，只能使用派生类函数，但可以用::显式调用基类函数。将基类函数声明为virtual，则为省略override的重写）。\n不将基类函数声明为虚函数，则为重定义。\n基类指针可以指向派生类，但派生类指针不能指向基类\n在派生类对象的内存布局中，基类部分在派生类部分前部，因此基类指针可以访问派生类对象中属于基类的成员而不会越界。\n通过虚函数机制，基类指针可以调用派生类重写的虚函数。\n因此，基类析构函数不设置为虚函数的话，基类指针无法调用派生类的析构函数。\n派生类指针无法指向基类对象，因为基类对象不包含派生类特有的成员，因此会有越界风险。不允许隐式转换，强制转换可能导致运行时错误。\n虚函数 虚函数表存放 虚函数表指针和对象一起存储，放在对象的起始地址。\n虚函数表在编译时生成，本身放在只读数据段。\n虚函数内容放在代码段。\n构造函数会隐式初始化对象的虚函数表指针。\n通过基类指针调用虚函数时程序会通过对象的虚函数表指针找到虚函数表，根据函数在表中的偏移量调用正确的派生函数。\n派生类会继承基类的虚函数表，若重写虚函数则替换虚表中的地址。\n虚函数表是所有的类共享，还是每个类有自己的虚函数表\n每个类有自己的虚函数表，编译器会为每个包含虚函数的类单独生成虚函数表，同一类的不同对象共享一个虚表，不同的类不共享一张虚表。\n纯虚函数\n纯虚函数没有函数体，只有=0声明。含有纯虚函数的基类是虚基类（抽象类），虚基类无法实例化，只能用于派生类，派生类必须要实现否则派生类也是虚基类。用于提供一种接口规范。\n1 2 3 4 5 6 7 8 class A{ public: virtual void func() { // 实现，允许重写 } virtual void func2() = 0; // 纯虚函数，必须重写 } 不能被声明为虚函数\n构造函数\n普通非成员函数（只能重载，不能重写）\n静态成员函数（无法访问虚函数表（虚函数指针在对象首部））\n友元函数（友元函数不能继承）\n内联成员函数（内联在编译时就展开了，而重写是运行时的多态）\n空类的大小 空类是不包含非静态成员变量、虚函数或虚基类的类\n空类会默认生成默认构造函数、拷贝构造函数、默认析构函数、赋值运算符和取地址运算符。\n空类可以用来作为接口占位，具体逻辑由不同子类完成。\n• 默认为1字节，因为每个对象必须有唯一的地址\n• 当空类作为基类时，编译器可能会用空基类优化，就是将占位字节和派生类成员合并。但是如果继承了多个空基类，则不能这样优化\n• 如果包含虚函数，则需存储虚函数表指针，大小是指针的大小（64位系统8字节）\n• 空类作为其他类成员则可能需要内存对齐\n• 静态成员变量存放在数据段，成员函数存放在代码段，因此不影响类大小。\n运算符重载 重载运算符本质上是重载函数，只是函数名为operator+或其他运算符。\n使用运算符比函数调用的形式更直观。\n注意：\nC++中作用域解析::， 成员访问.，三目条件运算符?:等不可重载\n重载运算符不能改变优先级，比如+的优先级低于*\n不能改变运算符的操作数，二元运算符必须有两个参数\n重载的运算符必须至少有一个操作数是用户自定义类型，也就是int double等内置类型不能重载\n需要注意操作符在语义上要直观，相关的操作符也要重载，比如重载了==，也应该重载!=\n段错误Segmentation Fault 段错误的本质是程序运行访问了非法地址或违反了访问权限，导致被操作系统强行终止。\n• 访问未分配的内存（释放指针后又使用）\n• 越界访问\n• 修改只读数据\nMMU检测到非法访问后，向操作系统发送错误信号sigsegv，进程终止并生成core文件，这叫做核心转储。\n要注意指针的使用和边界检查、是否栈过深等。\nlambda表达式 lambda表达式是一种匿名函数工具，对于比较简短的逻辑，可以写一个lambda表达式代替定义一个函数。\nlambda表达式的完整形式是捕获列表mutable-\u0026gt;返回类型{函数体}\n捕获列表表示能够访问哪些外部变量，=：按值捕获所有外部变量、\u0026amp;：按引用捕获所有外部变量、x, \u0026amp;y：x按值，y按引用、this：捕获当前对象的成员变量\nmutable：允许修改按值捕获的变量部分，默认是const不能改变\n-\u0026gt; int 返回值是int，也可以不写，让编译器推导\n既然捕获了所有变量，就可以不写参数列表。\n多线程、条件变量condition variable和mutex 条件变量是实现线程间同步的工具，引入condition_variable头文件，和mutex配合使用。 通过wait等待，通过notify_one和notify_all唤醒 cv.wait(lock)或者cv.wait(lock, 一个lambda表达式谓词)。前者：释放lock并等待notify；后者：释放lock并等待notify，只有在lambda表达式为真才唤醒。 std::thread用来创建线程，接收函数名、lambda表达式。线程也是一种对象。thread不可复制，可以移动。这是为了防止多个线程对象管理同一线程资源造成竞争。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; using namespace std; void hello(int val) { cout\u0026lt;\u0026lt;\u0026#34;hello, \u0026#34;\u0026lt;\u0026lt;val\u0026lt;\u0026lt;endl; } // 线程参数按值传递，线程在构造函数调用时立即启动 thread t1(hello, 1); thread t2([](){cout\u0026lt;\u0026lt;\u0026#34;lambda\u0026#34;\u0026lt;\u0026lt;endl;}); class MyClass { public: void func(int val) { cout\u0026lt;\u0026lt;\u0026#34;MyClass: \u0026#34;\u0026lt;\u0026lt;val\u0026lt;\u0026lt;endl; } }; // 绑定对象函数 MyClass obj; thread t3(MyClass::func, \u0026amp;obj, 34); int main() { // join的意思是主线程等待该线程完成再继续进行 t1.join(); t2.join(); t3.join(); return 0; } thread的join方法是让主线程阻塞等待该线程完成，用来同步。detach方法是将子线程从主线程的管理分离，在后台独立运行。由C++库自己管理回收。detach后无法用原来的thread对象管理线程。 thread在销毁之前必须被join或者detach，必须保证线程函数运行期间线程对象有效或者detach\nlock_guard和unique_lock lock_guard创建就上锁，作用域结束析构时候解锁，不能中途手工解锁，也不能复制\nunique_lock允许创建时不上锁而是延迟上锁，可以随时手动加锁解锁(unique_lock lock(mtx); lock.unlock(); lock(lock);)，析构时自动释放，不可复制可以移动，条件变量必须配合unique_lock使用\n互斥锁和条件变量程序例子 多线程交替打印数字\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #include \u0026lt;iostream\u0026gt; #include \u0026lt;mutex\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;condition_variable\u0026gt; using namespace std; mutex mtx; condition_variable cv; bool flag = true; void print_odd() { // 打印奇数 for (int i = 1; i \u0026lt; 100; i += 2) { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); // 在使用条件变量之前必须持有锁，否则对条件变量的检查不是原子的 cv.wait(lock, [](){return flag;}); cout\u0026lt;\u0026lt;\u0026#34;odd: \u0026#34;\u0026lt;\u0026lt;i\u0026lt;\u0026lt;endl; flag = false; cv.notify_one(); } } void print_even() { // 打印偶数 for (int i = 2; i \u0026lt;= 100; i += 2) { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); cv.wait(lock, [](){return !flag;}); cout\u0026lt;\u0026lt;\u0026#34;even: \u0026#34;\u0026lt;\u0026lt;i\u0026lt;\u0026lt;endl; flag = true; cv.notify_one(); } } int main() { thread t1(print_odd); thread t2(print_even); t1.join(); t2.join(); return 0; } 生产者消费者问题\n只用了一个cv，可能导致无效唤醒。因此可以用两个条件变量，分别专门唤醒消费者和生产者。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #include \u0026lt;iostream\u0026gt; #include \u0026lt;mutex\u0026gt; #include \u0026lt;condition_variable\u0026gt; #include \u0026lt;atomic\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;thread\u0026gt; using namespace std; const int TOTAL_ITEMS = 20; const int BUFFER_SIZE = 5; queue\u0026lt;int\u0026gt; q; atomic\u0026lt;int\u0026gt; done = false; mutex mtx; condition_variable cv; void producer() { for (int i = 0; i \u0026lt; TOTAL_ITEMS; ++i) { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); cv.wait(lock, [](){return q.size() \u0026lt; BUFFER_SIZE;}); q.push(i); cout\u0026lt;\u0026lt;\u0026#34;producer: \u0026#34;\u0026lt;\u0026lt;i\u0026lt;\u0026lt;\u0026#34; q.size: \u0026#34;\u0026lt;\u0026lt;q.size()\u0026lt;\u0026lt;endl; cv.notify_one(); } done = true; cv.notify_all(); } void consumer() { while (true) { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); cv.wait(lock, [](){return !q.empty() || done;}); if (q.empty() \u0026amp;\u0026amp; done) { break; } cout\u0026lt;\u0026lt;\u0026#34;consumer: \u0026#34;\u0026lt;\u0026lt;q.front()\u0026lt;\u0026lt;\u0026#34; q.size\u0026#34;\u0026lt;\u0026lt;q.size()\u0026lt;\u0026lt;endl; q.pop(); cv.notify_one(); } } int main() { thread t1(producer); thread t2(consumer); t1.join(); t2.join(); return 0; } 读写锁\nC++17的shared_mutex可以直接实现读写锁（写者优先）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #include \u0026lt;iostream\u0026gt; #include \u0026lt;shared_mutex\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; class ThreadSafeResource { private: int data = 0; shared_mutex sm; public: void read() { shared_lock\u0026lt;shared_mutex\u0026gt; l (sm); cout\u0026lt;\u0026lt;\u0026#34;read: \u0026#34;\u0026lt;\u0026lt;data\u0026lt;\u0026lt;endl; } void write(int d) { unique_lock\u0026lt;shared_mutex\u0026gt; l(sm); data = d; cout\u0026lt;\u0026lt;\u0026#34;write: \u0026#34;\u0026lt;\u0026lt;data\u0026lt;\u0026lt;endl; } }; int main() { ThreadSafeResource TSR; vector\u0026lt;thread\u0026gt; v; for (int i = 0; i \u0026lt; 5; ++i) { // 创建5个读者线程 v.emplace_back([\u0026amp;](){TSR.read();}); } // 创建写者线程 for (int i = 0; i \u0026lt; 5; ++i) { v.emplace_back([\u0026amp;](){TSR.write(i);}); } for (int i = 0; i \u0026lt; 10; ++i) { v[i].join(); } return 0; } C++11中，使用条件变量和mutex实现读写锁。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 #include \u0026lt;iostream\u0026gt; #include \u0026lt;mutex\u0026gt; #include \u0026lt;condition_variable\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; using namespace std; class RWL { private: bool write_active; int nr_reader; mutex mtx; condition_variable cv; public: RWL() : nr_reader(0), write_active(false) {} void readLock() { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); cv.wait(lock, [this](){return !write_active;}); nr_reader++; } void readUnlock(){ unique_lock\u0026lt;mutex\u0026gt; lock(mtx); nr_reader--; if (nr_reader == 0) { cv.notify_one(); } } void writeLock() { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); // 不能有其他写者，不能有读者 cv.wait(lock, [this](){return !write_active \u0026amp;\u0026amp; nr_reader == 0;}); write_active = true; } void writeUnlock() { unique_lock\u0026lt;mutex\u0026gt; lock(mtx); write_active = false; cv.notify_all(); } }; int main() { RWL rwl; int data = 0; vector\u0026lt;thread\u0026gt; v; for (int i = 0; i \u0026lt; 5; ++i) { // 5个读线程 v.emplace_back([\u0026amp;, i]{ rwl.readLock(); cout\u0026lt;\u0026lt;\u0026#34;reader \u0026#34;\u0026lt;\u0026lt;i\u0026lt;\u0026lt;\u0026#34;: \u0026#34;\u0026lt;\u0026lt;data\u0026lt;\u0026lt;endl; rwl.readUnlock(); }); } // 5个写线程 for (int j = 0; j \u0026lt; 5; ++j) { v.emplace_back([\u0026amp;, j]{ rwl.writeLock(); data = j; cout\u0026lt;\u0026lt;\u0026#34;writer \u0026#34;\u0026lt;\u0026lt;j\u0026lt;\u0026lt;\u0026#34;: \u0026#34;\u0026lt;\u0026lt;data\u0026lt;\u0026lt;endl; rwl.writeUnlock(); }); } for (int k = 0; k \u0026lt; 10; ++k) { v[k].join(); } return 0; } 原子变量 对原子变量的读写修改时不可中断的，两个线程同时递增一个原子变量不会导致出错，这是硬件上的CPU指令保证的。\n函数名是地址 函数名是函数的地址，在代码中会隐式转换成指向函数的指针\nSTL STL包括算法、容器和迭代器、仿函数、适配器和分配器。\n• 容器是存放数据的各种数据结构，包括vector，deque、set、map等\n• 算法通过迭代器操作容器数据，包括sort、find等常用算法\n• 迭代器是访问容器元素的的抽象或者广义上的指针，重载了*、-\u0026gt;和++等运算符\n• 仿函数是重载了小括号的对象或者结构体，可以像调用函数一样使用这种对象。\n◦ 内置的仿函数有算数运算（plus\u0026lt;T\u0026gt;，multiplies\u0026lt;T\u0026gt;等）、关系和逻辑运算（less\u0026lt;T\u0026gt;, equal_to\u0026lt;T\u0026gt;等，用于排序和条件判断） ◦ sort(v.begin(), v.end(), greater\u0026lt;int\u0026gt;())可以实现降序排列，也可以自定义一个bool cmp(int a, int b) {return a\u0026gt;b;}然后调用sort(v.begin(), v.end(), cmp); • 适配器通过容器实现更高级的功能，例如stack和queue都是借助deque容器实现的\n• 分配器用于管理内存的动态分配和释放\nSTL的优点在于具有高可重用性（采用模板类和模板函数，更通用）、高性能（精心设计的低复杂度算法，利用红黑树等复杂底层结构）、高移植性（使用STL编写的模块可以方便移植到其他项目，而不需要顺带移植很多实现函数）。\n常用容器 • pair\u0026lt;T1, T2\u0026gt; p\n◦ 定义为一个struct，通过pair.first和pair.second访问两个成员 ◦ 用来操作关联容器，例如可以将一个pair插入map • vector\n◦ 在堆中分配了一段连续的内存空间存放元素 ◦ 迭代器first，end。first是起始元素位置，end是最后一个元素之后的位置。还有rbegin和rend。 ◦ 扩容：vector.capacity()表示不分配新内存的情况下最多可以保存的元素个数（预分配的大小），vector.size()表示当前已经存储的元素个数，capacity永远大于等于size，相等时就会扩容。扩容就是动态 申请一段新的连续空间并把数组迁移过去 ▪ 固定扩容是每次在原capacity的基础上增加固定的容量，比如每次都加20，优点是浪费较少，缺点是可能需要多次扩容 ▪ 加倍扩容是每次将capacity翻倍，则需要预留较多空间，但是减少迁移开销 ▪ 注意扩容会使得原来的迭代器失效 ◦ resize：改变当前容器中含有元素的数量，如果resize(len)中的len\u0026lt;=capacity则只调整size=len，如果len \u0026gt;capacity则将size和capacity均设置为len。新增元素调用默认构造函数。 ◦ reserve(len)改变capacity，如果len \u0026lt; capacity就不做任何改变，len\u0026gt;capacity就扩容，包括迁移 ◦ emplace_back直接在容器的内存空间内使用对象的构造函数，无需临时对象，而pushback需要先构造一个临时对象再拷贝到容器尾部，因此emplaceback更高效 ◦ 操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 vector\u0026lt;int\u0026gt; v1; vector\u0026lt;int\u0026gt; v2(5, 10);// 5个10 int arr[] = {1, 2, 3}; vector\u0026lt;int\u0026gt; v3(arr, arr+3);// 迭代器或指针范围初始化 vector\u0026lt;int\u0026gt; v4(v3); // 拷贝构造 vector\u0026lt;int\u0026gt; v5 = {1, 2, 3, 4}; // 列表初始化 vector\u0026lt;int\u0026gt; v6 {1, 2, 3, 4}; // 等号可以省略 v1.push_back(4); // 尾部插入 v1.emplace_back(5); // 尾部插入，但是emplace_back更高效 v1.insert(v1.begin()+1, 10); // 把10插入到第二个位置 v1.pop_back(); // 删除尾部最后一个元素 v1.erase(v1.begin()+1); // 删除第二个元素 v1.erase(v1.begin()+1, v1.begin()+3); // 删除第2，3个元素（左闭右开） v1.clear(); // 删除所有元素 v1.at(1); // 检查越界 v1[2]; // 不检查越界 v1.resize(5); // 调整大小 v1.resize(7, 42); // 调整大小并将 新增元素 初始化为42 v1.reserve(100); // 预留空间 v1.shrink_to_fit(); // 释放预留的未使用空间 v1.size(); // 元素个数 v1.capacity(); // 预留空间可以存放的元素个数 for (int i = 0; i \u0026lt; v1.size(); ++i) { // 传统遍历 } for (auto it = v1.begin(); it != v1.end(); ++it) { // 迭代器遍历 } for (int i : v1) { // 简洁写法 } sort(v1.begin(), v1.end()); // 升序遍历 sort(v1.begin(), v1.end(), greater\u0026lt;int\u0026gt;()); // 降序遍历• list 环状双向链表 ◦ 在任意位置插入删除元素不会影响其他元素的迭代器，迭代器只支持++和\u0026ndash;，不能下标或跳跃（i+5）访问\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 vector\u0026lt;int\u0026gt; v = {1,5,5,6,8}; list\u0026lt;int\u0026gt;l1; list\u0026lt;int\u0026gt;l2(5, 10); // 5个10 list\u0026lt;int\u0026gt;l3(v.begin(), v.end()); // 其他容器的迭代器范围初始化 list\u0026lt;int\u0026gt;l4(l3); // 拷贝构造 l1.push_back(1); // 尾部插入 1 l1.push_front(2); // 头部插入 2 1 l1.insert(l1.begin(), 3); // 3 2 1 l1.pop_back(); // 尾部删除 3 2 l1.pop_front(); // 头部删除 2 l1.erase(l1.begin()); //删除迭代器位置 l1.sort(); // 必须用成员函数sort，不能用sort(l1.begin(), l1.end())，因为迭代器不支持随机访问 l2.sort(); l1.merge(l2); // 合并两个有序的链表，将l2合并到l1 l1.reverse(); // 反转链表 l1.unique(); // 删除链表相邻重复元素，因此需要先排序，否则只删除相邻的 l1.unique([](int a, int b){return abs(a-b\u0026lt;5);}); // 删除相邻绝对值相差小于5的• list和vector ``` ◦ list是双向链表，vector是数组 ◦ list只能顺序访问，vector可以随机访问 ◦ vector插入会导致扩容和迁移、迭代器失效，list不会 ◦ vector只有扩容时申请内存，list每次插入都需申请内存 • deque双端数组（队列） ◦ 支持快速随机访问，但是没有vector快 ◦ 内部是分段的连续空间，迭代器比vector复杂，为了提高效率可以先将内容复制到vector，排序后再拷贝回deque ◦ 底层由多个固定大小的连续内存块组成，通过一个中控数组管理这些缓冲区的指针 ◦ 头尾的插入和删除可能涉及到分配新缓冲区，O1，中间的插入删除可能涉及缓冲区迁移 ，On ```cpp deque\u0026lt;int\u0026gt; dq; dq.push_front(1); //也可以emplace，同样原理emplace性能好， //push只适合加入现成对象，emplace适合用参数构造 dq.push_back(2); dq.front(); dq.back(); dq.pop_front(); dq.at(0); dq[0]; deque\u0026lt;int\u0026gt; dq2(5, 10); deque\u0026lt;int\u0026gt; dq3(dq2); deque\u0026lt;int\u0026gt; dq4(dq.begin(), dq.end()); ``` • stack和queue ◦ 是基于deque的容器适配器,push 和 pop 操作均为 O(1)（访问首尾），不能通过迭代器访问 ```cpp #include \u0026lt;queue\u0026gt; #include \u0026lt;stack\u0026gt; stack\u0026lt;int\u0026gt; s; s.push(1); // 入栈 s.pop(); // 出栈 int top = s.top(); // 获取栈顶元素 bool isEmpty = s.empty(); // 判断栈是否为空 size_t size = s.size(); // 返回元素数量 std::queue\u0026lt;int\u0026gt; q； q.push(10); // 入队元素 10 q.emplace(20); // 直接构造元素 20 q.pop(); // 删除队首元素 int head = q.front(); // 获取队首元素 int tail = q.back(); // 获取队尾元素 if (!q.empty()) { std::cout \u0026lt;\u0026lt; \u0026#34;队列元素数量：\u0026#34; \u0026lt;\u0026lt; q.size(); } if (!q.empty()) { // 不检查空可能导致未定义行为 int val = q.front(); q.pop(); }``` • heap和priority_queue ◦ 堆是一种完全二叉树，大根堆是每个节点值一定大于子节点，小根堆是每个节点值一定小于子节点。 ◦ heap一般配合vector使用 ```cpp vector\u0026lt;int\u0026gt; v{5,8,3,7,1}; // 默认生成大根堆 make_heap(v.begin(), v.end()); make_heap(v.begin(), v.end(), greater\u0026lt;int\u0026gt;()); // 小根堆 // 向堆插入元素 v.push_back(99); push_heap(v.begin(), v.end()); push_heap(v.begin(), v.end(), greater\u0026lt;int\u0026gt;()); // greater\u0026lt;int\u0026gt;() 需要保持一致 // 将堆顶元素移动到尾部 pop_heap(v.begin(), v.end()); v.pop_back(); // 将堆转化为有序序列 sort_heap(v.begin(), v.end()); ``` ◦ priority_queue默认底层容器是vector，自动维护堆结构（注意不是顺序结构），没有迭代器 ◦ 堆的调整算法是Ologn • map和set ◦ map和set底层采用红黑树实现，红黑树是一种不严格的平衡二叉搜索树，保证最长路径不超过最短路径的两倍，而AVL是严格的二叉平衡树。红黑树的旋转操作比较简单，适合有大量插入删除操作的场景，比如vma的管理 ◦ 都是C++的关联容器，只通过接口访问元素，底层都是红黑树实现 ◦ set判断一个元素是否存在，mao因是个好 ◦ 插入删除查找时间Ologn，可以iter遍历 On ◦ map不可修改键，可以修改值；set不可修改键 ◦ 默认按键升序排列，键唯一 ```cpp map\u0026lt;int, string\u0026gt; m; m.insert(make_pair(1, \u0026#34;Alice\u0026#34;)); // 推荐，避免临时对象构造[6](@ref) m[2] = \u0026#34;Bob\u0026#34;; set\u0026lt;int\u0026gt; s; s.insert(10); auto it = m.find(1); // 返回迭代器，未找到返回`end()`[1,6](@ref) if (m.count(1) \u0026gt; 0) { ... } m.erase(1); // 按键删除，返回删除元素数量[6](@ref) s.erase(s.begin()); for (auto it = m.begin(); it != m.end(); ++it) { cout \u0026lt;\u0026lt; it-\u0026gt;first \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; endl; } for (const auto\u0026amp; kv : m) { ... } // 自动解引用为键值对[7](@ref)• unordered_map和unordered_set ◦ 哈希表实现，无序关联容器 ◦ 插入查找删除复杂度O1\nvector是线程安全的吗 vector不是线程安全的，多个线程同时修改线程会导致数据竞争。需要锁机制保护。\n线程不安全的情况有：同时修改容器结构，如push_back、insert、erase等；混合读写操作，如一个线程读、另一个线程写同一个元素，或者一个线程读、另一个线程修改容器结构\n泛型编程 泛型编程是通过模板机制允许在编写代码时不指定具体的数据类型，而是在编译时根据具体类型生成特定代码，避免重复编程。\n类模板是创建通用类的机制，使用template声明，类内部用T表示通用类型参数，函数模板用于定义通用函数，也是声明然后用T。但是类模板实例化时必须指定类型参数（例如stack s），不能自动推导，函数模板可以根据调用的参数类型自动推导（例如add(3, 5)会生成一个int版本的add）\n全特化是为模板的所有参数指定具体类型，也就是没有用泛型编程，偏特化是部分指定模板参数或者添加类型修饰，如指针和引用。偏特化只适用于类模板，不适合函数模板。\n1 2 3 4 5 6 7 8 // 偏特化 template \u0026lt;typename T\u0026gt; class Stack\u0026lt;T*\u0026gt; { // 特化为指针类型 private: T** data; public: void push(T* item) { /* 指针特殊处理 */ } }; 类型推导 auto可以让编译器在编译期推导出变量的类型。必须马上初始化，定义多个变量时不能有二义性，不能用作函数参数，类中不能用作非静态成员变量（非静态成员变量的初始化发生在对象创建时，即构造函数执行阶段，不是编译期。而静态成员需要在类外定义的时候初始化），不能定义数组，可以定义指针，不能推导模板参数。单纯使用auto时推导会忽略引用和cv（const， volatile）限定，也就是推导出对象的原始类型。但是使用auto\u0026amp;或者auto*的时候会保留引用和cv限定\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 int main() { int x = 10; const int\u0026amp; crx = x; auto a = crx; // a 是 int（忽略引用和 const） a = 20; // 合法：a 是普通 int // crx = 20; // 非法：crx 是 const 引用 } int main() { int x = 10; const int\u0026amp; crx = x; auto\u0026amp; b = crx; // b 是 const int\u0026amp;（保留引用和 const） // b = 20; // 非法：b 是 const 引用 } decltype用于推导表达式类型，但是不会计算表达式\nnullptr nullptr用来代替NULL，因为NULL就是0。\nvoid foo(char *c) {}\nvoid foo(int x) {}参数是指针和整数时重载会有二义性\nC/CPP编译流程 编译流程是预处理、编译、汇编、链接\n• 预处理：展开define、包含头文件、条件编译、处理特殊符号等与井号相关的，输出.i文本文件\n• 编译：词法分析、语法分析、语义分析、中间代码生成，并且优化一些处理，如展开循环等。输出.s汇编语言文件\n• 汇编：将汇编代码转换成机器指令，生成符号表记录函数和变量地址等，输出.o可重定位目标文件\n• 链接：通过调整代码和数据的相对位置生成最终的内存布局，将静态链接嵌入可执行文件，动态链接仅记录映射信息，在运行时加载共享库。输出可执行文件\n静态链接 vs 动态链接 静态链接将库文件嵌入可执行文件中，因此每个程序包含副本，占用空间大。动态链接共享库的副本没节省空间。静态链接更新需要重新编译整个程序，但是动态链接只需要替换库文件。静态链接运行更快，动态链接需要运行时加载，静态链接没有外部依赖，动态链接需要保证库文件存在且版本兼容。\nundefined reference 属于链接阶段错误，表示编译器在链接时找不到函数或变量的具体实现。可能是缺少某些库文件或者目标文件，或者库之间有依赖关系而连接顺序错误，或者函数变量声明与定义不一致\n","date":"2025-04-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/cpplilunzhishi/","title":"C++理论知识"},{"content":"https://arthurchiao.art/blog/intro-to-io-uring-zh/\n• How io_uring and eBPF Will Revolutionize Programming in Linux, ScyllaDB, 2020 • An Introduction to the io_uring Asynchronous I/O Framework, Oracle, 2020\nio_uring 是 2019 年 Linux 5.1 内核首次引入的高性能异步 I/O 框架，能显著加速 I/O 密集型应用的性能。 但如果你的应用已经在使用 传统 Linux AIO 了，并且使用方式恰当，那 io_uring并不会带来太大的性能提升 —— 根据原文测试（以及我们自己的复现），即便打开高级特性，也只有 5%。除非你真的需要这 5% 的额外性能，否则切换成 io_uring代价可能也挺大，因为要重写应用来适配 io_uring（或者让依赖的平台或框架去适配，总之需要改代码）。\n既然性能跟传统 AIO 差不多，那为什么还称 io_uring 为革命性技术呢？\n它首先和最大的贡献在于：统一了 Linux 异步 I/O 框架， ◦ Linux AIO 只支持 direct I/O 模式的存储文件（storage file），而且主要用在数据库这一细分领域； ◦ io_uring 支持存储文件和网络文件（network sockets），也支持更多的异步系统调用 （accept/openat/stat/\u0026hellip;），而非仅限于 read/write 系统调用。\n在设计上是真正的异步 I/O，作为对比，Linux AIO 虽然也是异步的，但仍然可能会阻塞，某些情况下的行为也无法预测；\n似乎之前 Windows 在这块反而是领先的，更多参考： ◦ 浅析开源项目之 io_uring，“分步试存储”专栏，知乎 ◦ Is there really no asynchronous block I/O on Linux?，stackoverflow\n灵活性和可扩展性非常好，甚至能基于 io_uring 重写所有系统调用，而 Linux AIO 设计时就没考虑扩展性。 eBPF 也算是异步框架（事件驱动），但与 io_uring 没有本质联系，二者属于不同子系统，并且在模型上有一个本质区别：eBPF 对用户是透明的，只需升级内核（到合适的版本），应用程序无需任何改造；\nio_uring 提供了新的系统调用和用户空间 API，因此需要应用程序做改造。 eBPF 作为动态跟踪工具，能够更方便地排查和观测 io_uring 等模块在执行层面的具体问题。\n本文介绍 Linux 异步 I/O 的发展历史，io_uring 的原理和功能，并给出了一些程序示例和性能压测结果（我们在 5.10内核做了类似测试，结论与原文差不多）。\n很多人可能还没意识到，Linux 内核在过去几年已经发生了一场革命。这场革命源于两个激动人心的新接口的引入：eBPF 和 io_uring。\n我们认为，二者将会完全改变应用与内核交互的方式，以及应用开发者思考和看待内核的方式。\n本文介绍 io_uring（我们在 ScyllaDB 中有 io_uring 的深入使用经验），并略微提及一下 eBPF。\n1 Linux I/O 系统调用演进\n1.1 基于 fd 的阻塞式 I/O：read()/write()\n作为大家最熟悉的读写方式，Linux 内核提供了基于文件描述符的系统调用，这些描述符指向的可能是存储文件（storage file），也可能是 network sockets： ssize_t read(int fd,void* buf,size_t count); ssize_t write(int fd,const void* buf,size_t count);二者称为阻塞式系统调用（blocking system calls），因为程序调用这些函数时会进入 sleep 状态，然后被调度出去（让出处理器），直到 I/O 操作完成： • 如果数据在文件中，并且文件内容已经缓存在 page cache 中，调用会立即返回； • 如果数据在另一台机器上，就需要通过网络（例如 TCP）获取，会阻塞一段时间； • 如果数据在硬盘上，也会阻塞一段时间。 但很容易想到，随着存储设备越来越快，程序越来越复杂，阻塞式（blocking）已经这种最简单的方式已经不适用了。\n1.2 非阻塞式 I/O：select()/poll()/epoll() 阻塞式之后，出现了一些新的、非阻塞的系统调用，例如 select()、poll() 以及更新的 epoll()。\n应用程序在调用这些函数读写时不会阻塞，而是立即返回，返回的是一个已经 ready 的文件描述符列表。\n但这种方式存在一个致命缺点：只支持 network sockets 和 pipes ——epoll() 甚至连 storage files 都不支持。\n1.3 线程池方式 对于 storage I/O，经典的解决思路是 thread pool：主线程将 I/O 分发给 worker 线程，后者代替主线程进行阻塞式读写，主线程不会阻塞。\n这种方式的问题是线程上下文切换开销可能非常大，后面性能压测会看到。\n1.4 Direct I/O（数据库软件）：绕过 page cache 随后出现了更加灵活和强大的方式：数据库软件（database software）有时 并不想使用操作系统的 page cache，而是希望打开一个文件后，直接从设备读写这个文件（direct access to the device）。这种方式称为直接访问（direct access）或直接 I/O（direct I/O），\n• 需要指定 O_DIRECT flag； • 需要应用自己管理自己的缓存 —— 这正是数据库软件所希望的； • 是 zero-copy I/O，因为应用的缓冲数据直接发送到设备，或者直接从设备读取。\n1.5 异步 IO（AIO） 前面提到，随着存储设备越来越快，主线程和 worker 线性之间的上下文切换开销占比越来越高。\n现在市场上的一些设备，例如 Intel Optane，延迟已经低到和上下文切换一个量级（微秒 us）。换个方式描述，更能让我们感受到这种开销：上下文每切换一次，我们就少一次 dispatch I/O 的机会。\n因此，Linux 2.6 内核引入了异步 I/O（asynchronous I/O）接口，\n方便起见，本文简写为 linux-aio。AIO 原理是很简单的：\n• 用户通过 io_submit() 提交 I/O 请求，\n• 过一会再调用 io_getevents() 来检查哪些 events 已经 ready 了。\n• 使程序员能编写完全异步的代码。\n近期，Linux AIO 甚至支持了epoll()：也就是说不仅能提交 storage I/O 请求，还能提交网络 I/O 请求。照这样发展下去，linux-aio似乎能成为一个王者。但由于它糟糕的演进之路，这个愿望几乎不可能实现了。\nLinux AIO 确实问题缠身，\n只支持 O_DIRECT 文件，因此对常规的非数据库应用 （normal, non-database applications）几乎是无用的；\n接口在设计时并未考虑扩展性。虽然可以扩展 —— 我们也确实这么做了 —— 但每加一个东西都相当复杂；\n虽然从技术上说接口是非阻塞的，但实际上有 很多可能的原因都会导致它阻塞，而且引发的方式难以预料。\n1.6 小结\n以上可以清晰地看出 Linux I/O 的演进：\n• 最开始是同步（阻塞式）系统调用；\n• 然后随着实际需求和具体场景，不断加入新的异步接口，还要保持与老接口的兼容和协同工作。\n另外也看到，在非阻塞式读写的问题上并没有形成统一方案：\nNetwork socket 领域：添加一个异步接口，然后去轮询（poll）请求是否完成（readiness）；\nStorage I/O 领域：只针对某一细分领域（数据库）在某一特定时期的需求，添加了一个定制版的异步接口。\n这就是 Linux I/O 的演进历史 —— 只着眼当前，出现一个问题就引入一种设计，而并没有多少前瞻性 —— 直到 io_uring 的出现。\n2 io_uring\nio_uring 来自资深内核开发者 Jens Axboe 的想法，他在 Linux I/O stack 领域颇有研究。\n从最早的 patch aio: support for IO polling可以看出，这项工作始于一个很简单的观察：随着设备越来越快，中断驱动（interrupt-driven）模式效率已经低于轮询模式（polling for completions） —— 这也是高性能领域最常见的主题之一。\n• io_uring 的基本逻辑与 linux-aio 是类似的：提供两个接口，一个将I/O 请求提交到内核，一个从内核接收完成事件。\n• 但随着开发深入，它逐渐变成了一个完全不同的接口：设计者开始从源头思考如何支持完全异步的操作。\n2.1 与 Linux AIO 的不同\nio_uring 与 linux-aio 有着本质的不同：\n在设计上是真正异步的（truly asynchronous）。只要设置了合适的 flag，它在系统调用上下文中就只是将请求放入队列，不会做其他任何额外的事情，保证了应用永远不会阻塞。\n支持任何类型的 I/O：cached files、direct-access files 甚至 blocking sockets。\n由于设计上就是异步的（async-by-design nature），因此无需 poll+read/write 来处理 sockets。\n只需提交一个阻塞式读（blocking read），请求完成之后，就会出现在 completion ring。\n灵活、可扩展：基于 io_uring 甚至能重写（re-implement）Linux 的每个系统调用。 2.2 原理及核心数据结构：SQ/CQ/SQE/CQE\n每个 io_uring 实例都有两个环形队列（ring），在内核和应用程序之间共享：\n• 提交队列：submission queue (SQ)\n• 完成队列：completion queue (CQ)\n这两个队列：\n• 都是单生产者、单消费者，size 是 2 的幂次；\n• 提供无锁接口（lock-less access interface），内部使用内存屏障做同步（coordinated with memory barriers）。\n使用方式：\n• 请求\n◦ 应用创建 SQ entries (SQE)，更新 SQ tail； ◦ 内核消费 SQE，更新 SQ head。 • 完成\n◦ 内核为完成的一个或多个请求创建 CQ entries (CQE)，更新 CQ tail； ◦ 应用消费 CQE，更新 CQ head。 ◦ 完成事件（completion events）可能以任意顺序到达，到总是与特定的 SQE 相关联的。 ◦ 消费 CQE 过程无需切换到内核态。 2.3 带来的好处\nio_uring 这种请求方式还有一个好处是：原来需要多次系统调用（读或写），现在变成批处理一次提交。\n还记得 Meltdown 漏洞吗？当时我还写了一篇文章解释为什么我们的 Scylla NoSQL 数据库受影响很小：aio 已经将我们的 I/O 系统调用批处理化了。\nio_uring将这种批处理能力带给了 storage I/O 系统调用之外的其他一些系统调用，包括：\n• read\n• write\n• send\n• recv\n• accept\n• openat\n• stat\n• 专用的一些系统调用，例如 fallocate\n此外，io_uring 使异步 I/O 的使用场景也不再仅限于数据库应用，普通的非数据库应用也能用。这一点值得重复一遍：\n虽然 io_uring 与 aio 有一些相似之处，但它的扩展性和架构是革命性的：\n它将异步操作的强大能力带给了所有应用（及其开发者），而不再仅限于是数据库应用这一细分领域。\n我们的 CTO Avi Kivity 在 the Core C++ 2019 event 上 有一次关于 async 的分享。\n核心点包括：从延迟上来说，\n现代多核、多 CPU 设备，其内部本身就是一个基础网络；\nCPU 之间是另一个网络；\nCPU 和磁盘 I/O 之间又是一个网络。\n因此网络编程采用异步是明智的，而现在开发自己的应用也应该考虑异步。\n这从根本上改变了 Linux 应用的设计方式：\n• 之前都是一段顺序代码流，需要系统调用时才执行系统调用，\n• 现在需要思考一个文件是否 ready，因而自然地引入 event-loop，不断通过共享 buffer 提交请求和接收结果。\n2.4 三种工作模式\nio_uring 实例可工作在三种模式：\n中断驱动模式（interrupt driven） 默认模式。可通过 io_uring_enter() 提交 I/O 请求，然后直接检查 CQ 状态判断是否完成。\n轮询模式（polled） Busy-waiting for an I/O completion，而不是通过异步 IRQ（Interrupt Request）接收通知。\n这种模式需要文件系统（如果有）和块设备（block device）支持轮询功能。\n相比中断驱动方式，这种方式延迟更低（连系统调用都省了）， 但可能会消耗更多 CPU 资源。\n目前，只有指定了 O_DIRECT flag 打开的文件描述符，才能使用这种模式。当一个读或写请求提交给轮询上下文（polled context）之后，应用（application）必须调用 io_uring_enter() 来轮询 CQ 队列，判断请求是否已经完成。\n对一个 io_uring 实例来说，不支持混合使用轮询和非轮询模式。\n内核轮询模式（kernel polled） 这种模式中，会 创建一个内核线程（kernel thread）来执行 SQ 的轮询工作。\n使用这种模式的 io_uring 实例， 应用无需切到到内核态 就能触发（issue）I/O 操作。\n通过 SQ 来提交 SQE，以及监控 CQ 的完成状态，应用无需任何系统调用，就能提交和收割 I/O（submit and reap I/Os）。\n如果内核线程的空闲时间超过了用户的配置值，它会通知应用，然后进入 idle 状态。\n这种情况下，应用必须调用 io_uring_enter() 来唤醒内核线程。如果 I/O 一直很繁忙，内核线性是不会 sleep 的。\n后面略\n","date":"2025-04-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/linuxio/","title":"Linux IO 系统调用专题"},{"content":"操作系统结构 操作系统内核 内核负责抽象并管理硬件设备，应用程序只需要直接交互内核，不需要关心硬件的具体实现。\n内核的功能有进程管理、内存管理、硬件设备管理和向应用程序提供服务（系统调用）。\n宏内核和微内核 宏内核结构中，内核的所有功能，如进程管理、内存管理、文件系统、硬件管理都在内核态中运行。不过，linux可以动态加载内核模块，比如设备驱动等可以动态加载。\n微内核架构中，内核只保留最基本的能力，如进程调度、进程间通信、内存管理等。将其他功能放到用户空间，因此将不同服务隔离，单个服务出现故障或遭受攻击不会影响其他服务。\n混合类型内核是一种大一些的微内核，将部分开销大的关键服务，如文件系统、网络协议栈放回内核态，以减少用户态和内核态之间的切换开销。是一种折中方案\n宏内核：内核态模块之间交互方便，驱动直接交互硬件延迟低。但是模块耦合度高，单个模块故障或受攻击可能导致系统级故障，且维护更新难度大，不易扩展。\n微内核：一个服务崩溃不会影响其他服务，容易动态加载更新和扩展；但是需要频繁进程间通信和上下文切换（访问硬件时），需要设计复杂的消息传递机制\n混合内核：将图形驱动等系统调用频繁的功能放在内核态减少切换开销。比微内核的通信和切换开销小，比宏内核更灵活。\n硬件结构 图灵机和冯诺依曼模型 图灵机模型：由纸带和读写头组成。纸带相当于内存，读写头上有计算、存储单元（寄存器）和控制单元。在纸带上写入1、2、+，对于数字，读写头按照顺序读入到存储单元中。读取到+时，控制单元发现是运算符，就按照运算操作计算存储单元中的数字，并将结果放到存储单元中。最后将结果写到纸带上。\n冯诺依曼模型基于图灵机的设计：将计算机分为运算器、控制器、存储器、输入设备、输出设备。其中运算器和控制器在CPU中，存储器、输入输出设备通过总线连接到CPU。\n常见的寄存器有通用寄存器（参与计算的操作数）、程序计数器（指向下一条指令的地址）、指令寄存器（存放当前执行的指令本身）\n总线一般分为数据总线、地址总线和控制总线\nCPU位宽尽量不要小于总线位宽，否则32位CPU计算64为数据，需要分高低位计算，效率低。\nCPU采用4级流水线，4个阶段是取指、译码、执行、回写。统称为指令周期。\nCPU的时钟频率，如1GHz代表1秒产生1G次脉冲信号，倒数是时钟周期。一个时钟周期不一定能执行完一条指令，如乘法指令和加法指令都是一条指令，但乘法比加法慢得多。\nCPU的执行时间=时钟周期数时钟周期时间=指令数CPI(指令的平均时钟周期数)*时钟周期时间\n优化：编译器优化 - 减少指令数；CPU架构优化 - 流水线 - CPI；超频 - 减少时钟周期时间（散热压力大，容易崩溃）\n32位和64位 32位计算机一次只能计算32位数据，超过32位需要分高低位计算，而64位计算机一次可以计算64位数据。32位计算机一般地址总线也是32位，因此只能寻址4G。64位计算机一般可以寻址48位。\n从数据计算的角度，如果需要计算的数据不超过32位，则64位没有优势。\n对于软件，32位和64位是指令格式的区别。32位的软件可以以兼容模式运行在64位电脑上，但64位软件无法运行在32位电脑上。\n缓存命中 对于2维数组，按行遍历比按列遍历块，因为一行是连续的，一列不是连续的。连续的可以预读命中。\n计算密集型线程可以绑定到某一个CPU核心上，避免缓存命中率下降，因为不同的核心有不同的缓存。\n缓存一致性 多核心有不同的cache，一个核心修改数据后写到cache，但还未写回。导致不同核心无法读取到最新的数据，这叫缓存不一致。\n解决缓存一致性：写传播+事务串行化\n• 写传播：每个CPU更新缓存后，必须同步到其他cache • 事务串行化：保证不同核心收到同步的顺序相同，如A、B都修改了某一变量，则所有核心都应该先收到A的修改然后收到B的修改，或者反过来。需要引入锁机制。\n基于总线嗅探的MESI协议：总线嗅探就是每个核心监听总线上是否有核心广播自己修改了cache，从而实现写传播。MESI是4个状态缩写，即已修改、独占、共享、失效。已修改是一种脏标记，表示已经修改缓存项但还未写回内存。失效代表缓存项已经失效，不能读取。独占和共享代表缓存项是干净的，独占是数据指缓存在一个CPU的cache中，因此修改了不需要广播。共享代表缓存项存在多个核心的cache中，修改之前必须先广播一个请求，将其他cache中该项置为无效，然后更新当前缓存，并将当前缓存标记为已修改。读取失效缓存项需要先检查其他核心cache是否有副本，没有则读主存。\n伪共享问题：多个线程看似操作不同变量，但是由于这些变量存储在同一缓存行中，一个变量的修改会影响同一缓存行的其他变量，导致缓存行频繁失效 。因此需要将高频修改的变量与其他变量隔离。\n补码、小数 负数用补码表示是为了和正数统一加减法操作。十进制转换为二进制用除二取余法，二进制转换为十进制用乘二取整法。\n计算机以浮点数形式存储小数，包含符号位、指数位（小数点在数据中的位置）、尾数位（小数点右侧数字）尾数长度决定数的精度。单精度是32位（尾数23位），双精度是64位（尾数52位）。\n0.1+0.2！=0.3是因为小数无法用完整的二进制表示，必须根据精度舍入。因此浮点数存储的是近似值，不能直接比较，必须使用误差范围比较。金融场景必须用高精度库。\n内存管理 内存是与CPU直接交换数据的存储器 冯诺依曼体系的计算机是存算分离的，也就是数据放置在内存上。计算时加载到CPU的寄存器中，计算结束后把结果输出到内存上。\n与CPU越接近的存储器，读写速度必须越快，否则会导致CPU等待IO时间长，影响CPU效率。然而对于存储器来说，一般越快的存储器，容量越小，价格越高。另一方面，快速存储器常常是易失性的，也就是掉电数据就没有了，比如寄存器，SRAM，DRAM等；而非易失性的存储器往往比较慢，比如磁盘，磁带等。当然，也有非易失性的内存，但是并不常用。\n因此，CPU只操作内存数据，任何外存上的数据都必须先加载进内存，然后才能被CPU加载到寄存器。\n存储的金字塔层次结构 因此，现代计算机系统会有一个金字塔型的存储结构，由上到下依次是寄存器（一个CPU时钟周期，纳秒级）、L1、2、3级缓存（SRAM，几十个时钟周期，纳秒级）、内存（DRAM，百纳秒级别）、外存（HDD、SSD、磁带、光盘等，SSD的读延迟在几十微秒，写延迟在几毫秒；HDD的读写延迟在十几毫秒）、远程存储（分布式文件系统、服务器，延迟取决于网络连接）。\n每一层都可以看做是下一层的缓存，根据局部性原理，这样可以使得存储系统在性能上接近金字塔上层，在容量和价格上接近金字塔下层。\n内存需要操作系统管理 如果操作系统不提供内存的管理，程序员就必须使用绝对物理地址，会出现两个问题：\n编写程序低效：程序员必须手动管理内存，保证自己的程序能够独占一段内存。 不安全：每个进程都有可能访问到其他进程的数据，缺乏隔离性。 虚拟内存是操作系统给每个进程提供的独立连续地址空间 为了高效安全编写程序，引入虚拟内存的概念。\n操作系统给每个进程都提供一个独立、连续的虚拟地址空间。空间的大小由地址长度决定，例如，32位机器上进程地址空间为4G，用户态3G，内核态1G；64位机器上由于地址空间太大，进程只用其中48位，就是内核态、用户态各128TB。注意内核态的地址空间是各个内核进程共享的，task_struct中的mm_struct用于管理虚拟地址空间，mm_struct中的task_size用于定义内核态与用户态的分界线（地址值）。\n虚拟地址空间可以比真实物理内存更大，这是基于局部性原理，认为每个进程在最近一段时间更可能反复使用一部分页面。多余的页面如果分配了就交换到外存。\n虚拟地址空间按一定的方式映射到物理地址上，有分段、分页、段页式。Linux采用多级页表管理。\n虚拟内存具有隔离性，解决了编程效率和安全性问题：每个进程认为自己独占内存空间，不用担心内存分配的问题，也不用管理页面的物理布局；每个进程只能访问自己的地址空间，不能访问其他地址空间，因此免受恶意进程或故障进程的破坏。\n但是，内核态进程都是共享地址空间的。从高效和安全两个方面来分析：\n• 操作系统内核是操作系统开发人员精心设计的，了解内核地址布局，通过锁和信号量等机制确保了不会产生数据竞争问题。反而隔离内核态进程会导致进程通信的开销，妨碍内核的高效实现。 • 内核态的进程执行的代码是操作系统本身提供的，因此认为不会有恶意。\n缺点在于内核也是有漏洞的，病毒程序可能利用操作系统内核漏洞发起攻击。\n分段机制导致内外碎片，需要频繁交换和紧凑 分段机制是一种内存管理机制，原理是将虚拟地址空间分为若干段，例如代码段、数据段等。每个段独立映射到物理内存上的一段空间，通过段基址加偏移量索引段内内容。\n可以实现物理内存的离散分配，但是会导致外碎片，也就是多个段之间的空闲空间是分散的，即使总大小足够分配也无法分配。其次，我认为分段机制也有内碎片问题，比如应用程序不一定需要一个段这么大，并且可能本来是需要的，后来因为某些原因释放了，但是段内的空间又无法分配给别的程序使用。\n因此需要频繁紧凑，也就是挪动段的位置让他们相邻。具体是先将原来段占用的内存写回磁盘，然后再装载回紧跟着另一个段的地方。\n分页机制有内碎片无外碎片，交换效率高 分页机制是将虚拟地址空间和物理地址空间都划分为一系列大小相同的页，页的大小一般是4K，但是也有大页。操作系统将这些虚拟页映射到物理页上，这些物理页可以是离散的，从而不需要分配连续的物理内存空间。因此没有外碎片问题。\n但是进程可能不需要一个页，例如只需要几百字节，却也必须分配4K。但是页本身是比较小的，因此内碎片总体来看开销小。换出时也仅需要换出部分页，而不是一整个段，交换效率高。\n分页机制还允许进程无需一次性将所有内容加载进内存，只有需要用到的时候再载入。\n单级页表导致页表太大，造成内存开销。Linux采用多级页表机制减少页表大小。原理是如果用不到某个上级页表的表项，就不需要创建下级页表。也就是说不用覆盖整个虚拟地址空间。\n虚拟地址通过MMU映射到物理地址 https://blog.csdn.net/lyndon_li/article/details/135392221 https://juejin.cn/post/7269276349654155275#heading-15\nMMU是CPU内部的单元，专门用于虚拟地址到物理地址的翻译。\n在多级页表模式下，页表本身也是放在内存中的，页表中的条目称为页表项，页表项包括该虚拟页的物理页帧号（PPN）、该页是否为脏页、保护权限等信息。查询页表项可以得到虚拟页的物理页框号，和虚拟地址的页内偏移量一起可以组成物理内存的地址。\n虚拟地址分为虚拟页号和页内偏移量两部分。虚拟页号用于定位虚拟页所在页表的页表项，页内偏移量用于和物理页框号组成物理地址。这是因为内存页通常是4KB的，而地址是按字节寻址的。因此某个地址会在所在内存页的某个偏移量处。而物理页和虚拟页的偏移量是相同的。\n在多级页表模式下，虚拟页号也可分为多个部分，如常见的4级页表分为PGD、PUD、PMD和PTE。只要知道PGD的物理内存地址，就可以通过虚拟页号的各部分一直查到物理页框号。\n每个进程的虚拟地址空间是独立的，也就意味着每个进程有自己的页表。在Linux中，页表基地址保存在进程结构体task_struct下用来表示地址空间的mm_struct中的pgd，MMU在翻译某个进程的地址之前，先将该进程的页表基址加载到页表基址寄存器上。\nTLB是CPU中的一种高速缓存，专门用于存放地址转换的映射，命中后就不用逐级查询页表了。因为高速缓存采用SRAM，延迟就是几个时钟周期，但是内存是DRAM，一次访问内存需要几百个时钟周期，更不用说逐级查询了。\n内存大页 内存大页是指内存页的大小不止4K，有可能是2M甚至更大。\n大页的作用在于减少页表层级和表项数目，更容易在TLB中命中，加快地址转换过程，并且可以减少缺页异常。例如，应用需要2M内存，直接分配一个大页就够了。\n大页的缺点在于存在空间浪费，因为程序未必需要一个大页的大小。其次，对于局部性很好的应用来说并不能改善性能。\n大页需要大段的连续物理内存，由于系统长时间运行后会产生碎片，很难找到连续大段物理内存，因此会在系统启动时分配足够的大页内存，放在大页内存池中。\n大页内存池是锁定的，不能分配给其他虚拟页，也不允许换出。大页内存池耗尽之后无法分配大页。\nLinux整个都是一个段 Linux本来想用分页管理，但是由于英特尔处理器的历史原因必须有段机制，因此就把整个地址空间看成一个大段，相当于没有用段机制。\nmm_struct是进程管理虚拟内存空间的结构体 每个进程由进程控制块管理，在Linux中进程控制块是task_struct结构体。\n进程控制块中用于管理虚拟内存空间的是mm_struct结构体，由于不同线程共享地址空间，因此线程共享mm_struct结构体。进程和线程都是task_struct，进程和线程的唯一区别是有没有独立的地址空间。\n注意，不同的线程并不共享栈，因此线程的栈指针并不在mm_struct中。与线程有关的资源应该保存在一个叫thread_union的结构体中，里面有线程的栈和寄存器等相关资源。task_struct通过一个宏可以找到thread_union。\n内核栈也不在mm_struct中，而是在task_struct-\u0026gt;stack。\nVMA表示一段连续的虚拟地址范围 vm_area_struct（VMA）是用于描述和管理进程虚拟内存空间的结构体，每一个task_struct使用mm_struct结构体管理进程的虚拟地址空间，每个VMA代表一段连续的虚拟地址空间，mm_struct通过VMA链表管理所有用到的虚拟地址空间。\nVMA同时以红黑树的形式组织，mm_struct有指向链表的头指针mmap和红黑树的根节点mm_rb。VMA在链表中顺序串联，在红黑树中可以高效查找。 • 应用可能频繁申请内存空间，有大量的VMA，红黑树方便增删改查，平均时间复杂度为Ologn。 • 链表方便遍历VMA（比如用户通过proc/pid/maps文件查看虚拟内存使用情况时），红黑树则要用栈或者递归，有空间复杂度。\nVMA中包括区域的起始地址、结束地址、上一个和下一个VMA的指针（形成链表）、用于连接到红黑树的成员、所属的mm_struct、访问权限、操作函数集等。\n进程的虚拟地址空间布局 32位地址按3G/1G划分用户和内核虚拟地址空间\n32位地址的最大寻址范围是4G，按照3：1划分。64位地址则是用户态内核态各占128T，中间还有很大的空洞canonical space，可以根据地址头部快速判断是用户态还是内核态。\n用户态虚拟空间布局 进程用户态虚拟地址空间的布局是代码段、数据段、bss段、堆、文件映射与匿名映射区、栈。各区域的起始地址在mm_struct中。\n内核态虚拟空间布局 32位内核态虚拟空间只有1G，因此要精打细算使用\n• 直接映射区占用896M：直接映射到0-896M的物理内存空间上 ◦ 前16M：DMA区域，以前ISA总线的DMA控制器只能使用物理内存的前16M ◦ 普通区域：内核代码段、数据段、BSS段、进程相关数据结构、内核栈（非常小且固定，溢出危害巨大） • 高端内存：128M动态映射 ◦ vmalloc动态映射区 ◦ 永久映射区 ◦ 固定映射区 ◦ 临时映射区：例如操作page cache时，需要将page cache的物理地址映射到临时映射区操作\n64位内核态虚拟空间有128T • 8T空洞 • 64T直接映射区 • 32Tvmalloc动态映射区（类似于用户态的堆） • 1T虚拟内存映射区，存放struct page • 512M直接映射区，存放代码段、全局变量、BSS等\n库函数malloc通过系统调用brk或mmap为进程分配虚拟内存 进程需要使用内存时，就向操作系统申请分配一些虚拟页使用，这叫做分配内存。分配的虚拟内存不一定对应了物理内存，只有访问时才会触发page fault映射到物理内存。 内存可以在创建进程时静态分配，例如代码段、全局变量、静态变量等。也可以在进程运行过程中动态分配，比如通过brk在堆中分配或通过mmap在文件映射区分配。\n一般通过brk分配小块内存，通过mmap分配大块内存。\nmalloc会分配更大的空间 malloc在分配虚拟地址空间时，会预分配更大的空间作为内存池，这是为了避免频繁系统调用，并且减少内存碎片。\nbrk在堆上分配内存 brk系统调用的原型只有一个参数，就是堆顶的地址。如果高于当前堆顶就是分配了新的地址空间，如果低于当前堆顶就是回收地址空间。还有一个更方便的系统调用sbrk，参数是堆顶偏移量，也就是只需要指定堆顶指针移动的偏移量。\nbrk是堆不是栈，因此可能堆顶的还没释放，堆里面的已经释放了。但是堆顶这时还不能往下移动，因此free并不会马上归还，而是会留在堆里面分配给新的请求。只有进程终止或者堆顶释放的时候才会归还。\n堆的分配方式会导致碎片化，因此只适合分配小块内存。\nmmap在文件映射区分配虚拟内存 https://www.cnblogs.com/Courage129/p/14232306.html\nmmap允许将文件或设备的内容从内核的page cache映射到进程的地址空间中，这样进程可以像直接访问内存一样访问文件，而不是通过read/write系统调用接口访问。\n如果没有mmap，进程要访问文件就要先用read系统调用把文件内容先复制到用户态的缓冲区中，修改缓冲区再写回page cache。有了mmap就可以直接操作page cache，减少了一次复制。\n除了映射文件之外，mmap还可以映射匿名页，以达到动态分配内存空间的目的。\nmmap的参数有起始地址、分配长度、一些标志位比如访问权限，可读可写等，是私有还是共享映射，文件描述符和文件内偏移量等。 • 起始地址只是一个暗示，如果可以分配就分配，如果不可以就由操作系统自己找一个（一般进程可以指定NULL） • 分配长度是必提供 • 只有文件页需要文件描述符，匿名页指定-1，偏移量为0\n每次调用mmap都会产生一个VMA，返回值就是指向起始地址的指针。\nfree会归还mmap分配的空间。\nmmap的文件通过munmap和msync刷盘 mmap将page cache映射到用户空间，默认情况下操作系统会有线程异步刷盘（根据脏页比例或者时间阈值）。\nmsync函数可以将内存中的修改强制同步到磁盘文件，确保数据持久性 • MS_SYNC同步刷盘，意思是mync函数阻塞直到数据落盘 • MS_ASYNC异步刷盘，意思是只是通知内核调度刷盘，不等待完成\nmunmap解除映射或者close关闭文件描述符时会触发脏页刷盘，但不是立刻的，依赖于内核的调度。\n注意：msync同步指定内存范围的脏页，默认不同步文件的元数据，适合局部数据更新场景。fsync以文件为单位强制下刷文件所有的脏页和元数据\n匿名页和文件页 内存中的页有两种：匿名页和文件页。\n匿名页是没有关联到文件系统，不需要持久化的页。这些页通常保存进程运行过程中的临时变量、和动态分配的堆等数据，理论上是不需要落盘的。但是实际上有可能在内存紧张的时候被换出到磁盘上。\n匿名页通过映射机制映射到虚拟地址空间的某页。\n文件页是与磁盘存在映射关系的某页。一般来说应当是先从磁盘上将文件读入内存中的page cache，然后再映射到进程的文件映射区。\n区别：文件页是持久化到磁盘上的数据，而匿名页仅仅存在于内存。内存紧张时匿名页需要交换到磁盘上的交换分区或者创建一个交换文件，而文件页则写回脏页或者不脏就直接丢弃。\n联系：匿名页和文件页都使用struct page结构体，只是文件页的mapping指向page cache，而匿名页的mapping指向anon_vma结构用来追踪对应的VMA。并且匿名页不会是脏页。通过检查page结构mapping的最低位判断是否是匿名页（1：匿名，0：文件）\n注意，匿名页在换入换出时会暂时使用page cache，但是正常使用的时候不涉及page cache\n私有映射和共享映射 私有映射是隔离性的一部分，意思是映射的地址空间不能被其他进程访问。例如，一个文件可以通过私有文件映射映射到了多个进程的文件映射区，但是多个进程可以独立读写映射而不会相互影响。这是依赖于写时拷贝。而共享映射是一种进程间通信的方式，例如一个文件通过共享映射被多个进程映射，则需要锁或信号量机制避免数据竞争的问题。\n私有映射的文件是不会写回盘的，而共享映射会。\n库函数mmap和系统调用mmap 库函数mmap实际上是对系统调用的封装，方便用户编程。\nmmap的使用：首先用open打开一个文件，获得文件描述符；然后用stat或fstat获取文件大小，从而知道需要映射空间的大小；接着调用mmap指定起始地址（通常为NULL）、长度、保护标志、文件描述符、偏移量；mmap会返回映射区域的指针；用完后使用munmap解除映射；最后使用close关闭文件。\nstat和fstat用于获取文件的状态信息，包括文件大小、访问时间等元数据。两者区别在于前者使用路径名，后者使用文件描述符。\n分配和映射的虚拟内存不会立刻访问 分配和映射内存后，不会立刻将虚拟内存与物理内存关联，也不会立刻将文件读上来。意思是PTE表项是空的。只有在访问虚拟页时，才会触发page fault为虚拟内存页分配并映射物理内存页。\n私有文件映射 进程的task_struct结构体通过file_struct管理打开的文件，file_struct中有文件描述符数组，文件描述符就是这个数组的下标，数组的每一项都是struct file结构。通过file可以获取文件的索引，如果进程想read/write有这个就够了。但是如果要映射到虚拟内存区域，还需要把文件从page cache映射过来。\n映射的本质就是使用PTE表项，将文件映射区的虚拟地址空间翻译成page cache对应的物理地址空间，因此操作映射就是直接操作page cache。优势在于用户态进程本来不能直接访问内核态的page cache，而映射后就可以直接在用户态访问，不需要拷贝。\n私有文件映射采用写时拷贝技术避免数据竞争。私有文件映射的PTE表项设置为只读的，当进程写入的时候会产生写保护类型的缺页中断。这时会分配一个新的物理页并且写入修改后的数据，并且将PTE映射到新页。\n私有文件映射中，脏页不会回写，其他进程看不到。\n私有文件映射避免了拷贝，主要用于进程加载数据，比如代码段、数据段等。\n共享文件映射 共享文件映射和私有文件映射的唯一区别在于不采用写时拷贝技术，PTE表项直接就是可写的。因此一个进程修改文件后另一个进程也可以看到。因此需要锁或信号量等机制避免数据竞争。\n共享文件映射可以写回脏数据，对文件做真正的修改。\n共享文件映射是一种进程间通信的机制。\n私有和共享匿名映射 匿名页不对应磁盘上的文件，也没有page cache。\n对于私有匿名映射来说，使用mmap分配一段虚拟内存空间之后，真正使用到这段空间时，没有相应的PTE表项，因此MMU会触发缺页中断。分配一个物理页后再更新页表。\n对于共享匿名映射，由于另一个进程不知道本进程将虚拟地址映射到了哪一个物理页，因此共享匿名映射是借助文件实现的。\n内核有一个tmpfs虚拟文件系统，内核在tmp方式中创建一个匿名文件。因此进程通过匿名文件仿照共享文件映射完成共享匿名映射。\n共享匿名映射只适合父子进程之间的通信，这是因为其他进程无法看到本进程创建的匿名文件，只有子进程可以共享虚拟内存空间和页表。\ndirect io不能使用文件映射 文件映射mmap是将page cache中的内存页映射到用户空间，而direct io没有page cache，一般是应用程序自己实现了一个缓存，所以不会使用mmap机制。\nRCU机制允许读者无锁访问 传统的读写锁中，写者持有写锁时读者不能读取。\n在RCU机制中，读者总是可以读取数据，如果写者要写入数据，则先拷贝一份副本去修改副本，修改完后将指向旧数据的指针指向新副本\n适用于读远大于写的场景。\n3种物理内存模型 FLATMEM平坦内存模型 将物理内存空间看作一块连续的大数组，数组的每一项是page结构，下标就是物理页号\nDISCONTIGMEM非连续内存模型 没必要为没用到的物理页分配page结构，经常要管理小片不连续的物理内存。\n用node表示一小段连续的物理内存，node内部用page数组管理这段连续物理内存页。\nSPARSEMEM稀疏内存模型 node粒度大，node内部可能不连续。node多了开销大。\n将物理内存空间划分为一系列section，每个section内部有page数组。section自身构成一个连续数组。\n每个section可以上线或下线从而支持内存的热插拔。\n当section下线之前，内核会将这部分地址隔离开，并将该section的内容迁移至其他section。包括复制和重新映射。迁移成功后将section标记为不可用并拔掉内存模块。而上线则仅需要初始化section并标记为可用。\n对于用户态地址空间，所有的虚拟内存地址都可以重新映射。对于内核态地址空间，直接映射区对应的物理地址不能改变，因此这段物理内存是不可迁移的，也就是不可拔出的。\n多核CPU的内存访问架构：一致性和非一致性 对称多处理器架构SMP\n所有CPU访问内存的距离是一样的。内存和所有的核心在总线两侧。因此是一致性内存访问UMA\n缺点：总线成为性能瓶颈。\n非一致性内存访问NUMA架构 将内存划分为多个内存结点，每个CPU有自己的本地内存。CPU访问本地内存更近并且不用竞争总线。CPU也可以通过QPI跨节点访问其他内存但是比较慢。因此每个CPU最好优先使用自己的物理内存，也就是执行映射到自己物理内存的进程。（处理器亲和性）\n每个numa节点用pglist_data结构体管理，pglist_data放在全局数组node_data中，通过nodeid索引。\n每个pglist_data主要包括：numaid，指向物理页page的数组，第一个页的pfn，可用物理页总数等。还有一个自旋锁用于数据竞争。\n物理页的PFN是全局唯一的。\nNUMA节点的状态用位图表示，包括上线、有高端内存、有普通内存等\n物理内存的回收 虚拟内存没有映射到物理页上，访问的时候就会触发缺页中断。缺页中断如果没有足够的物理内存，就会触发内存回收。\n物理内存紧张时，会唤醒内核线程kswapd来后台异步回收。如果仍不够就会同步直接回收。\n如果kswapd和同步回收都不够，就会触发OOM机制杀死一些高占用进程。\n回收内存的步骤：文件页和匿名页是可以回收的，对于文件页，可以写回或丢弃。对于匿名页，需要swap换出。文件页和匿名页的回收基于LRU算法。有active和inactive两个双向链表，可以优先回收不活跃的页。\n优先回收文件页而不是回收匿名页：干净文件页可以直接丢弃，藏文件页写回只有一次磁盘写入。匿名页的换入换出均为随机IO，无法合并也无法预读，而文件页通常对应磁盘的连续逻辑块，写回时可以合并IO实现顺序访问\n只有直接丢弃文件页不会影响性能，写回和swap都是会影响性能的。\n为了减少性能影响，可以调整更倾向回收文件页，也可以尽早触发kswapd来避免触发直接同步回收。\n内存水位（阈值）用来触发后台回收线程，分为页高阈值、页低阈值和页最小阈值。 高于页高阈值：内存充足 页低阈值-页高阈值：有一定压力 页最小阈值-页低阈值：触发kswapd线程后台回收，直到高于页高阈值 小于页最小阈值：触发主动同步回收\n页高阈值和页低阈值根据页最小阈值计算，可以调整页最小阈值。调高了：早触发回收，预留过多空闲内存导致频繁换入换出甚至OOM。预留少了则会影响内存分配性能。\n在NUMA架构下，本地内存不足时除了回收本地内存，还可以使用其他node的内存。可以设置倾向。默认设置为先找远端内存，因为访问远端内存（内存IO）的开销比回收内存（磁盘IO）的影响要小。\nOOM根据算法选择victim OOM选择算法为可用页面数*当前进程校准值除以1000+当前进程已用物理页数\n也就是和校准值、已用页面数正相关。因此可以调低校准值保护某个进程不被杀死。但是假如某个进程发生了内存泄漏而不能被杀死，会导致逐渐占满物理空间。\n在4G物理内存机器上申请8G虚拟内存空间 • 在32位机器上直接申请失败，因为用户态进程最多可以申请3G。 • 在64位机器上，理论上可以分配128T ◦ 如果设置了overcommit_memory参数，则会检查请求分配的合理性，有可能分配失败 ◦ 由于虚拟内存的VMA等管理资源也是占用空间的，因此8G应该可以申请，但没办法申请到128T。会在某个时刻触发OOM杀死当前进程。 ◦ 如果开启了swap机制，内核可以将部分没有用到的管理资源换出，因此可以申请到127T。但是进程本身的运行也会占用一定地址空间，因此不可能正好申请完128T。\n预读失效和缓存污染 每一级存储都是下一级的缓存。缓存基于局部性原理。当数据的访问模式不符合局部性原理时，会导致缓存失效。\n缓存的逐出通常采用FIFO、LRU、时钟或者LFU算法。\nFIFO算法并不考虑数据最近使用情况，因此性能不佳。\nLRU、LFU和时钟算法等考虑到了数据最近使用情况，基于局部性原理。\nLRU：活跃/非活跃\nLRU算法将最近最少使用的数据逐出，基于最近访问时间。当缓存命中时就提升到首部，当加载新缓存时逐出尾部。使用双向链表和unordered_map实现。 • 预读失效：对于随机IO，基于空间局部性的预读就失效了，因此效果不佳 • 缓存污染：对于大规模顺序IO，每个页面虽然最近访问过，但不会再次访问，因此时间局部性失效，效果不佳。\n改进方法：在Linux中，page cache实现了两个链表：活跃LRU和非活跃LRU。第一次访问数据时会将数据放在非活跃链表中，第二次访问才会将其提升到活跃LRU链表中。因此提高了进入活跃LRU的门槛，不会污染活跃LRU链表。逐出时会优先逐出非活跃LRU链表中的数据页。\nLFU：年龄老化\nLFU算法根据页面的最近访问次数，逐出最近最少访问的页面。问题在于工作集切换时，会导致新工作集的页面频繁逐出。因此可以给每个页面记录年龄，年龄越高排序越低。或者对于所有的页面，每隔一段时间将访问次数减半。\nLinux采用4KB作为内存页的默认大小 页面大小必须是2的整数幂，为了方便计算(1. 通过左移右移的方法得到各级页表地址和页内偏移量2. 如果不是2的幂次会产生地址浪费，如5B页面，用2位2进制表示页内偏移量会有1KB无法访问，用3位会浪费3个地址)。\n如果页太小，则页表等管理开销大。\n如果页太大，会浪费内存页的空间。\nstruct folio用于表示大页和复合页 Linux中的默认内存页大小是4K，但是可以创建更大的页面来减少缺页次数、减少页表条目，提高分配效率。\n复合页是连续的小页组合成的，但仍是小页的组合。\nLinux引入folio用来表示大页和复合页，能够简化代码逻辑，比如不用每次都检查一个page是不是大页的第一个页。并且释放的时候只需要一次调用就可以。\n内核分配物理页的结构 alloc_pages用于分配2的order次幂个物理内存页。返回的是物理地址。由于进程只能直接使用虚拟地址，所以提供了另一个__get_free_pages函数直接返回虚拟内存地址供进程使用。由于物理页中可能还有原来的数据，因此get_zeroed_page能够返回填充0的物理页，更安全。\n释放物理页时，free_pages传入虚拟地址和阶。转换物理地址之后调用__free_pages释放。\n内存分配根据gfp掩码指定内存分配行为，比如高优先级、允许swap、必须原子等。\nalloc_pages底层使用伙伴系统分配物理内存。 伙伴系统将内存划分为多个块，每个块都是2的幂次大小。进程请求一定量页面时，伙伴系统会优先分配所需阶数的连续块，如果该阶没有就把更大阶数的伙伴块平分成低阶，直到分到所需的大小。\n释放内存的时候检查相邻伙伴块是否空闲，如果空闲就合并成更高阶。\n通过合并伙伴块能够减少外部碎片，快速分配2的幂次大小连续物理空间。\n但是会有内碎片，因为所需的大小不一定是2的幂次，会浪费。并且不适合小内存块分配，因为会频繁划分小的伙伴块。并且伙伴系统最少分配一个物理页，如果只是new一个对象，可能根本用不到4K。\n小内存块结合slab分配器使用 针对小内存块的分配，内核采用slab内存池。\nslab分配器首先会向伙伴系统一次性申请多个物理内存页面组成slab内存池。并将这些连续物理空间划分为多个相同大小的小内存块，内核会针对多种尺寸预先创建多个slab内存池。比如会对task_struct、page等常用结构体建立slab内存池。这是为了方便频繁分配和释放这些结构体。\n内核使用kmalloc接口分配小块内存 kmalloc基于slab分配器实现，kmalloc会创建一系列大小的内存池，根据用户指定选择一个最合适的内存池slab cache。也可以用户自己通过kmem_cache_create创建一个合适的内存池。 内核通过vmalloc接口分配大块连续虚拟内存\nvmalloc不保证内存在物理上连续，只是虚拟地址连续。\n内核通过get_free_pages函数获得大量连续物理内存\nget_free_pages通过伙伴系统分配大量连续物理内存。\n文件系统 文件及文件系统的概念 文件是一系列数据和元数据的集合。其中，元数据定义了文件的属性信息，数据则是文件存储的实际内容。\n文件的元数据一般包括所有者，访问权限，访问模式，创建时间，修改时间，大小等属性，还包括文件数据块的索引，即文件中某个数据块的逻辑地址。\n文件最重要的元数据是inode。inode是文件系统中的一个结构，唯一标识了一个文件。inode通过inode号相互区别。在同一个文件系统中，inode号是唯一的。不同的文件系统可能不唯一。进程打开文件后，可以通过struct file结构中的vfsmount成员确定文件系统。\ninode号一般从3开始分配，2号是根目录，1号在ext中是lost+found（恢复无主文件的目录），0号保留（比如删除一个文件之后可以将文件名映射到0）\n注意：在Linux中，文件名并不属于inode，因为不同的硬链接可以有不同的文件名，但是他们共享一个inode。文件名与inode的映射关系存储在目录文件的目录项中。目录文件就由该目录下目录或文件的目录项组成，目录项在目录文件中组织成哈希表从而便于查询。为了避免逐级从目录文件中查询，操作系统会将最近读取到的目录项缓存在dentry中。dentry是内存中的结构，用于缓存文件名到inode之间的映射，避免逐级查找。\n文件的数据是文件存储的实际内容，数据可以是有结构的，也可以是无结构的。结构化数据是具有某种标准格式的数据，例如数据库中的某个表格。结构化数据便于计算机处理和分析。非结构化数据是指没有标准格式的数据，比如文本文件，二进制文件等。非结构化数据通常占数据量的绝大多数。半结构化数据一般是一些自描述文件，数据的结构和内容混在一起，需要解析标签。例如json文件，html文件等。\n文件可以进行一系列操作，比如创建，打开，读取，写入，删除，截断等。不同的文件系统实现操作的方式不同，Linux中除了具体的文件系统，还有对文件系统的抽象也就是虚拟文件系统VFS。VFS可以给应用提供统一的操作接口，便于用户编程。\n文件系统是操作系统中用于组织和管理文件的部分。文件系统一方面提供了对文件的操作管理方法，另一方面是对底层磁盘的抽象，使得用户无需关心数据在盘上是如何放置的，而将存储空间的管理交给文件系统完成。\n文件系统的主要功能包括提供对文件的操作，提供目录结构，管理文件的数据块在磁盘上的放置。保护文件的一致性和崩溃恢复等。\n文件的操作 文件的操作包括创建，打开，读取，写入，删除和截断等。\n要打开或者创建一个文件，可以使用open系统调用。open系统调用有3个参数。第一个是文件的路径和文件名，第二个是文件的访问模式和操作，第三个是在创建文件时，定义文件的访问权限。\n首先，内核需要解析路径名。解析路径名的方式就是从根目录开始，逐级读取目录文件，并确定下一级目录文件的位置。如果目录较长，这个过程可能会很慢。为了加速，可以为深层目录文件创建一个软链接。另外，文件系统提供dentry缓存机制加快解析。如果最近访问过某个文件或目录，就会把该文件对应的目录项缓存在内存中，不会逐目录查找。\n其次，open第二个参数需要指定文件打开以后的访问模式，包括只读，只写和读写。如果文件不存在，可以用O_CREAT指定创建一个文件，还可以指定是否要追加写、截断等。O_EXCL和O_CREAT合用，表示如果文件本身存在就返回创建失败。O_TRUNC表示如果文件存在就把长度截断到0。O_APPEND表示文件打开后总是往尾部追加写。\n创建一个文件的流程包括：路径解析（已经提到），检查该目录是否有权限创建文件，为文件分配inode结构，将文件的名字和inode号更新到目录中。如果需要写入还要分配数据块并更新元数据索引。如果是direct io或者同步写还需要将数据块落盘。\nopen第三个参数代表权限。在创建文件的模式下，第三个参数指定了文件的访问权限，包括文件创建者，所在组和其他成员的读取，写入，执行权限。\nread和write系统调用负责文件的读写。均有三个参数。第一个参数是文件描述符，第二个是缓冲区指针，第三个是读或写的字节数。\nread和write都从文件的读写指针开始，可以通过lseek修改读写指针。lessk的三个参数是文件描述符，偏移量和偏移起始地址。偏移起始地址可以是文件开头，末尾或者当前读写指针。\nhttps://blog.csdn.net/weixin_44698673/article/details/125729055\n删除文件用unlink系统调用，参数是文件路径。之所以参数是文件路径，是因为不同的文件可能对应同一个inode，也就是硬链接。如果通过文件描述符找到文件，就不知道具体想要删除哪一个硬链接。当删除最后一个硬链接的时候，文件的空间才会真正释放。删除文件的过程包括从目录中删除该文件的表项，减少硬链接计数。如果是最后一个硬链接，就调用具体文件系统的删除操作。\n删除目录用rmdir系统调用，参数是目录路径，因为打开目录不会返回一个整数形式的描述符。只能删除空目录，目录不空就要递归删除。\n截断文件是指定一个长度，如果文件超过这个长度就把多余部分删除，如果不足这个长度就用\\0填充到这个长度。truncate和ftruncate系统调用可以截断。前者的参数是路径名和长度，后者的参数是文件描述符和长度。\n区别：truncate之前无需先打开文件，ftruncate需要在打开文件的情况下截断。truncate可能会收到符号链接攻击，意思是高权限的文件被链接到低权限的链接，如果文件系统不完善，那么低权限的用户可能可以操作高权限的文件。\n文件在进程中的形式 进程通过open可以打开一个文件，打开以后通过文件描述符操作这个文件。\n在linux中，进程是task_struct结构体表示的，其中file_struct成员存放了进程打开文件的信息。\nfile_struct中的主要成员有：引用计数、读写锁，文件描述符表。\n引用计数是说，子进程会共享父进程的file_struct，不同线程也会共享file_struct。因此还需要读写锁来避免数据竞争。\n文件描述符表有两种形式，一种是fd_array数组，还有一种是fdt链表。当打开的文件不太多时，用数组（可以随机访问）。打开比较多时用链表。文件描述符实际上就是数组下标或链表中的索引。每一项指向一个struct file结构体，保存了文件在当前进程中的使用情况。主要成员有文件的inode指针，page cache指针（叫做address_space），vfsmount指针，dentry指针，操作函数集指针。这些是打开这一个文件的所有file结构共享的。此外还有文件的读写指针f_pos，访问模式（只读只写还是读写），权限等。\n文件的存储 文件在盘上的占用形式：连续存放和非连续存放\n对于HDD来说，最好连续访问数据以减少寻道时间。因此数据在盘上最好是连续存放的。\n连续存放方式是指文件在磁盘上连续存放，这样只需要在inode中指定文件的起始地址和长度就可以表示文件的存放。优点在于访问效率高，并且不需要索引。缺点在于不方便扩展和存在碎片。 不方便扩展：如果文件的末尾以后已经被占用，则没有办法继续申请连续的空间，必须要将文件整个搬移到新的更大的连续空间。 碎片：文件中的某些块不再使用，或者文件被删除，就会在磁盘上留下空缺。那么有可能磁盘本来有足够的空间，但是放不下大于空缺的数据。因此需要频繁整理紧凑空间 因此需要引入非连续存放方式。\n• 链表方式 采用链表的方式就可以充分利用空间，不会产生碎片，还可以动态扩展文件大小。分为隐式链接和显式链接。缺点：只要某个数据块损坏或者链接表损坏，就会导致文件丢失 ◦ 隐式链接：inode保存第一个数据块和最后一个数据块的地址，每一个数据块保存下一个数据块的地址。优点：没有链接表开销；缺点：一个数据块损坏以后整个文件丢失，必须按照顺序读取目标数据块之前的每一个数据块 ◦ 显式链接：inode保存首位数据块的地址，同时将每个数据块中的指针放在一个全局链接表中。链接表放在内存中，查询快速。缺点：链接表占用内存开销大，显式链接也要顺序扫描表 • 索引方式 ◦ 为每个文件创建索引数据，存放每个数据块的位置。inode指向索引数据块。 ▪ 优点：没有碎片，也可以扩展文件，还可以快速定位数据块而不是顺序读取。 ▪ 缺点：索引额外占用空间，索引块的大小限制了整个文件的大小，索引块损坏导致文件丢失 ◦ 链式索引 ▪ 允许存在多个索引块，以链表组织 • 优点：不限制文件大小 • 缺点：指针损坏导致文件丢失 ◦ 多级索引块 ▪ 索引块存放二级索引的地址，甚至可以有更多级别索引。\next2/3中结合了直接索引和间接索引：inode中直接存放一些数据块的索引，多余的采用间接索引，更多的采用二级或高级索引。因此对于小文件可以直接查找，大文件可以多级索引。\next4引入了extent机制，一个extent表示连续的数据块的集合而不是单独的地址。利用extent可以减少索引条目。如果一个大文件完全是顺序的，只需要一个extent就可以表示。\next4中extent组织成树形结构。文件更倾向于连续存储合并extent。\n空闲空间的管理 在为文件分配地址时需要快速找到可用的空闲空间。常用的方法有空闲表，空闲链表和位图\n• 空闲表 ◦ 建立一张全局空闲表，每一个条目包括空闲空间的起始位置和空闲块数量，通过顺序扫描找到足够的空闲空间，适合分配连续空间 ◦ 如果有很多细碎的空闲空间，性能会变差，表也会变大。 • 空闲链表 ◦ 每一个空闲块指向下一个空闲块，需要分配时从链头上取下足够数量的块 ◦ 不适合连续分配，大型文件系统中链表会很大 • 位图 ◦ 用位图表示每一个块的使用情况。 ◦ linux中用位图管理数据空闲块和inode空闲块 • 成组链接 ◦ 将空闲块分成若干个组，每一组指向下一组的头部，这样可以减少链接表的大小。并且可以在组内分配连续空间，类似于ext4里面的extent。本来文件系统管理数据需要记录所有数据块的索引，但是如果数据块是连续的就只需要记录起始位置和长度，那么就减少了索引量。\n符号链接（软链接）和硬链接 符号链接（软链接）类似于快捷方式，有自己的inode，其中内联地存放链接指向文件的路径。软链接可以对文件或目录创建，可以跨文件系统创建。\n硬链接没有自己的inode，或者说所有的硬链接共享一个inode，只是路径和文件名不一样。也就是说硬链接实际上是所在目录中的一个条目。inode中保存硬链接计数，只有删除最后一个硬链接的时候才真正删除文件。\n不能跨文件系统建立硬链接，因为不同文件系统的inode号是冲突的。\n每个目录本身包含自己的硬链接. 和父目录的硬链接..，从而方便用户引用当前目录和导航到上一级。但是不能人为对目录建立硬链接，为了避免循环引用导致目录无法删除。\n文件系统的结构 EXT系列文件系统 https://www.cnblogs.com/f-ck-need-u/p/7016077.html\next系列文件系统将盘上逻辑地址空间划分为多个块组。\n为了在块组中分配数据块，每个块组有自己的数据块位图，用于表示数据块是否空闲\n为了索引一个文件，文件的inode会存放每个数据块的地址。这是直接索引，此外还会有间接索引\n一个inode的大小比块小。因此一个块可以存多个inode。\n每个块组会管理一定序号的inode，inode的位置存放在该块组的inode表中。\n为了分配inode号，每个块组还有inode位图，用于记录该块组中的空闲inode号。\n之所以要划分块组，是因为如果不划分块组，就会导致块位图、inode位图和inode表过大，不方便查询。\n块组的元数据保存在块组描述符GD中，包括一个块组管理块的范围、块位图、inode位图、inode表的地址、空闲块和inode数量。\n每个块组的GD组织成一个全局的块组描述符表GDT。还有一个保留GDT为了防止扩容以后块组描述符放不下。\n文件系统的全局元数据除了块组描述符以外，还有超级块，包含inode总数、块总数、块组总数等。\n全局元数据非常重要，损坏会导致全盘数据丢失。早期存放在每个块组中，后来采用稀疏技术，就是只写入0、1和其他序号为3，5，7的幂次的块组中。只有前面的元数据无法访问才会读取后面的副本。\ninode结构中没有文件名和inode号。要查询一个inode，实际上是先从目录文件或者dentry中通过文件名查询到对应的inode号，每个块组管理哪些inode号在格式化的时候就已经确定，可以直接计算出所在块组。根据块组中inode表查询到inode位置。\n有一些文件比较特殊，如软链接（符号链接）、套接字、设备文件等只需要inode本身就可以内联地存放，所以不占用数据块。\next4文件系统引入了extent机制，一个extent是地址连续的数据块集合，索引这些数据块时只需要所以extent起始地址和偏移量。减少了元数据中索引的大小。\n文件系统的崩溃一致性 ext3中引入了日志机制，将盘划分为数据区，元数据区和日志区。每次存储数据时，将数据块落盘后，再将元数据写入日志区并提交，最后再将元数据转写到盘上。 断电时机：写数据时断电 - 根本没有更新元数据，相当于没有写入 写日志时断电 - 没有提交，因此不会恢复 日志提交后断电 - 可以重放日志恢复\nEXT系列文件系统中文件的操作 读取：读取某个文件之前需要确定inode号。如果内存中有相应的dentry缓存，就会从dentry缓存中查询，否则就要通过读取目录文件逐级查询。\n读取目录文件：找到块组描述符表（开机时已经加载到内存中），找到inode表并查询根目录的inode（inode号为2）所在的数据块。从数据块中取出根目录的inode并查询下一级目录的inode号，并再查表。直到查到文件所在目录，就从inode表中找到文件inode，并定位数据块，再将数据块复制到用户空间。当然，每次查询的时候都会检查inode中的权限，看是否有权限读取。\n删除：\n删除普通文件：找到文件所在inode和数据块，如果硬链接计数大于1就减少硬链接计数，否则才真正删除文件。接下来从inode表中将inode的表项删除，并且更新inode map，再从所在目录数据块中删除该文件条目，最后在块位图bmap中更新该文件数据块所在位，设置为可用。\n删除目录文件：需要首先递归删除目录下所有文件和目录，目录空后再删除该目录。首先在inode表中删除目录文件的inode条目，然后更新imap和bmap将目录占用的inode和数据块设置为可用，最后在该目录所在目录文件中删除目录条目。\n重命名：不会更改inode号\n同目录内重命名（仅修改文件名）：找到所在目录的数据块并更新条目。如果有冲突，会让用户选择是否覆盖，覆盖的过程就是先删除冲突项，然后重命名。\n非同目录内重命名（修改路径）：修改目标目录的数据块，添加该文件记录，然后从源目录删除文件记录。\n将文件移动到不同文件系统则需要先复制再删除。\n存储文件：\n读取GDT，找到有空闲inode号的块组，并且查imap分配一个inode号，在bmap中查空闲块并位文件的inode分配，再把空闲块地址写入inode表中。最后查bmap表分配数据块并填充数据块、更新inode中索引。只能按4K粒度分配\nEXT系列文件系统的缺点 ext系列文件系统本身经过很多年，已经非常可靠和稳定了，只是说相比于一些新的文件系统，他缺少一些更新的功能，比如数据去重，数据压缩、加密和快照等。\nF2FS文件系统 https://www.cnblogs.com/Linux-tech/p/12961293.html\nF2FS文件系统的优缺点\n缺点：作为一种日志结构文件系统，必须要经常垃圾回收，这样带来写放大。\n冷热温机制比较简单，是静态定义的。比如对于冷数据，就是通过后缀名来判断，或者是GC以后的数据就是冷数据。那么可以尝试设计一些更准确的冷热区分算法。另外，也可以让用户区指定一个冷热参数。\nLinux的存储软件栈 应用程序 - VFS - 文件系统 - 块层 - 设备驱动 - HDD/SSD\nPage Cache Linux上会将文件先读入内存，作为page cache映射到进程的虚拟地址空间。page cache就是struct page中的address_space结构体。每个文件会在内核中只有一份page cache。但是可以有多个进程中的struct file指向。\npage cache不仅是缓存基于文件的数据页，还用于元数据的缓存。不同的page有不同的操作函数，定义在address_space_operations操作集中。\npage cache的数据结构是基数树，用于快速定位某些状态，如脏的页。基数树是一种利用前缀索引快速检索的数据结构，检索复杂度是Ok，k是所有字符串最大长度\n使用page cache和不使用page cache有什么区别\n使用page cache的就是buffer io，不使用page cache的是direct io。他们之间的区别有几点。第一是buffer io的读写性能更好，因为可以在内存中命中，而direct io每次都要访存。第二是buffer io需要占用一定内存存放page cache，但是direct io不需要。第三个是buffer io 不用管理一致性问题和崩溃恢复问题，因为page cache已经实现了这个机制，但是direct io需要自己管理。buffer io适合大多数场景，但是数据库等需要精细控制IO或者是已经实现了自己的缓存的应用就可以跳过page cache。\n使用direct IO需要注意什么 首先需要手动管理数据的一致性，因为direct io只是保证数据落盘，但不会保证元数据都落盘，需要频繁调用fsync保证所有的元数据都落盘。其次还需要自己实现崩溃恢复，出错重发等机制，然后为了保证性能还需要自己实现批量写入和预读的策略，自己实现读写缓冲区。最后还需要把写入数据块的大小和文件系统数据块的大小对齐。\n使用direct io需要在打开文件的时候设置O_DIRECT标志。\nPage Cache的一致性与可靠性 更新page cache上的页就成了脏页。一方面操作系统内核线程定期写回脏页，另一方面用户可以主动调用fsync和sync同步某个文件和整个文件系统的数据。最后内存压力大时会导致回收文件页（丢弃或回写）\n3个系统调用：fsync：将fd文件的所有脏数据和脏元数据写回磁盘；fdatasync：将fd文件所有的脏数据和必要的元数据写回磁盘。必要的元数据是对访问文件有关键作用的元数据，比如文件大小。但是文件修改时间就不写回。sync：将整个文件系统的脏数据和脏元数据写回磁盘。\n写回和写穿 page cache就是写回模式，direct io就是写穿模式。上面比较过优缺点\n写回实现机制\n每个存储设备会有一个脏文件链表，需要回写时会遍历这个链表确定需要回写的文件。文件的struct page中有脏页标志。\n当write等系统调用修改文件的内容后，内核会调用set_page_dirty将对应脏页置脏。\n目录的存储 文件名与inode的映射关系存储在目录文件的目录项中。目录文件就由该目录下目录或文件的目录项组成，目录项在目录文件中组织成哈希表从而便于查询。为了避免逐级从目录文件中查询，操作系统会将最近读取到的目录项缓存在dentry中。dentry是内存中的结构，用于缓存文件名到inode之间的映射，避免逐级查找。\n文件IO 缓冲和非缓冲IO 缓冲IO是指利用标准库的缓冲区加速文件访问并累积写回，为了减少系统调用次数。适合大量小规模的数据传输，可以累积写回减少系统调用次数。 非缓冲IO是每次都用系统调用访问文件，可以及时更新。适合数据库等一致性较高的场景。\nbuffer io和direct io 已经说过\n阻塞、非阻塞、同步、异步 阻塞io是指一个进程发起IO请求后就挂起等待，直到IO请求返回。\n非阻塞IO是指一个进程发起IO请求后，如果不能立刻得到IO结果，就返回错误或提示，并执行其他任务。择机重新发起IO。\n同步io是指进程发起IO后必须等待IO操作完成才能执行后续代码，阻塞和非阻塞IO都可以是同步的。阻塞的同步IO就是等待，非阻塞的同步IO就是轮询或者其他IO多路复用技术。关键在于进程是否需要等待IO的结果。\n异步io就是允许进程发起io后立即执行其他任务，而不用等待IO结果。IO完成后内核会通过回调函数或者信号机制通知进程。\n无论是阻塞、非阻塞轮询还是非阻塞多路复用都是同步的，因为他们接下来的操作依赖于IO的结果，在数据从内核拷贝到用户态的过程中都要等待。\n真正的异步io是内核数据准备好之后从内核态拷贝到用户态，都不用等待。到了用户态以后再通知进程。\n网络系统 网络系统是操作系统中负责管理网络通信、协议处理和资源共享的模块，通过分层架构实现对物理网络设备的抽象化，并为上层提供统一的接口。\n零拷贝技术 传统的网络工作方式：硬盘读取数据 - （DMA）- 内核缓冲区 - 用户缓冲区 - 内核socket缓冲区 - （DMA）- 网卡\n2次系统调用（read、write），4次用户态内核态切换（1个系统调用2次），4次数据拷贝（硬盘到内核读缓冲区，读缓冲区到用户缓冲区，用户缓冲区到socket缓冲区，socket缓冲区到网卡）。\n通过零拷贝技术优化：减少系统调用次数 - 减少内核态切换和不使用用户缓冲区 - 减少内存拷贝次数。\nmmap+write实现三次拷贝 mmap将内核缓冲区数据映射到用户空间，不需要拷贝，应用程序将数据从内核缓冲区拷贝到socket缓冲区，然后再拷贝到网卡缓冲区。\n4次内核态切换（一次mmap一次write），拷贝三次（硬盘 - 内核缓冲区 - socket缓冲区 - 网卡）\nsendfile 一次系统调用，直接将内核缓冲区复制到socket缓冲区。\n2次内核态切换（一次sendfile），拷贝三次\nSG-DMA零拷贝 如果网卡支持聚散DMA技术，可以直接用DMA将磁盘数据复制到内核缓冲区，再用SGDMA将内核缓冲区数据拷贝到网卡。0拷贝是指不需要CPU来拷贝，实际上发生了2次数据拷贝。\nPage Cache也有预读失效和缓存污染的问题 凡是cache就有预读失效和缓存污染，因此page cache不适合大文件的传输，而只适合热点小文件。\n高并发大文件传输应该使用异步IO+直接IO。工作模式为：前半部分，内核向磁盘发起读请求，但是不等待数据就位就返回。后半部分，当内核将磁盘中的数据拷贝到进程用户缓冲区中，通知进程处理。\n直接IO的应用场景有：应用程序自己实现了缓存、大文件传输（无法命中，还会挤占缓存空间）。\n直接IO无法享受page cache的优化：需要自己调度合并IO请求以减少磁盘寻道\n总之，传输小文件应该使用零拷贝技术、大文件用异步IO+直接IO\nIO多路复用 基本socket模型\n网络通信实际上是进程通信中的跨主机进程通信。\n对于TCP socket，服务端需要先调用socket函数创建一个IPv4以及TCP协议的socket，再调用bind函数将器绑定到IP地址和端口号上。\n绑定IP地址：一台机器可能有多个网卡，每个网卡都有对应IP地址。内核收到指定网卡的数据包才会转发给进程，绑定端口：指定通信进程\n绑定IP地址和端口后，调用listen监听。因此可以使用netstat命令查看指定端口有无监听判断网络服务是否启动。\n服务端调用accept获取客户端连接，并从全连接队列中拿出一个已经完成三次握手的socket用于应用程序的网络传输。注意，监听的socket和已连接socket是不同的。\nsocket文件的inode指向内核中的socket结构，结构体中有两个队列：发送队列和接收队列。队列中保存的是struct sk_buff，用链表的形式串起来。\n为了减少数据拷贝，网络栈各层次都用一个sk_buff结构描述数据包。通过调整结构体中data的指针剥离和增加各级协议的首部。\n多进程模型 服务器连接数受文件描述符数量和系统内存的限制。\n如果服务器为每个客户端都分配一个进程处理请求，则进程数量越多，占用资源也越多，并且切换进程开销大。\n多线程模型\n为每个连接分配一个线程服务请求。线程的频繁创建和销毁占用系统开销，虽然可以采用线程池的方法避免频繁创建和销毁线程，但创建线程数量过多，服务器仍然服务不过来。\nIO多路复用\n用一个进程监控多个IO事件（网络套接字、文件描述符等），从而以更低的资源消耗实现高并发处理。\nselect/poll\nselect实现多路复用的方式是将需要监听的文件描述符放在一个集合中，调用select函数将集合拷贝入内核，内核每次扫描所有文件描述符，当某个文件描述符上有IO事件则标记该描述符。扫描完后将文件描述符集合拷贝回用户态，用户态接着遍历文件描述符集合找出有标记的并处理。\n因此，select需要2次遍历，2次拷贝文件描述符集合。\nselect使用固定长度位图表示文件描述符集合，因此受到位图长度限制。\npoll采用链表形式的动态数组表示文件描述符集合，能存更多文件描述符，但是还收到系统文件描述符个数限制。\n但poll和select的原理几乎相同，因此开销还是很大。\nepoll\nepoll在内核中维护了一颗红黑树，用来跟踪进程所有待检测的文件描述符。进程新增监控socket时只需要传入这一个socket，而不需要复制整个集合。\nepoll使用事件驱动机制，维护了一个记录就绪IO链表。当某个socket或文件描述符上有IO事件发生时，通过回调函数将其加入就绪事件链表中。用户调用epoll_wait时只会返回有IO事件的文件描述符，而不需要遍历。\n使用epoll：epoll_create创建epoll对象，epoll_ctl将待检测的socket传入红黑树，epoll_wait得到有IO事件的文件描述符。\n因此epoll可以解决c10k问题，因为监听socket数量增加时不会大幅降低性能。\n边缘触发和水平触发\n边缘触发是指当监控的文件描述符上有IO事件时，epoll_wait只会返回fd一次，因此需要保证一次性处理完IO事件，例如将数据从内核缓冲区读取完。\n水平触发是指监控的文件描述符上有IO事件时，每次epoll_wait都会返回该fd，直到处理完IO事件。\n边缘触发适用于高并发、高性能系统，能够减少重复通知，但是开发时需要小心事件丢失。并且边缘触发只能用于非阻塞IO，以避免没有一次性处理完数据导致事件丢失。\n例如使用阻塞IO读取数据，　客户端发送 2000 字节数据，触发一次 EPOLLIN 事件。 服务端调用 read 读取 1024 字节（缓冲区剩余 1024 字节）。 由于未处理完数据且使用阻塞 I/O，线程将阻塞在下一个 read 调用上。 后果：　• 剩余字节数据滞留在内核缓冲区，但 ET 模式不会再次触发 EPOLLIN 事件。当前线程阻塞，但其他的线程也无法感知该fd。 如果循环调用read，可能会使得缓冲区读完后read卡在空缓冲区上。\n水平触发适用于并发量较低，需要快速调试，容错性强的应用，容易实现。但是性能不如边缘触发，因为存在不必要的事件通知、每次通知都存在上下文切换开销。\nReactor模式 reactor模式是对IO多路复用的封装，是一种高性能网络模式。\nreactor模式包含3种组件：reactor，acceptor和handler\nreactor负责监听IO事件（epoll监控文件描述符）并分发给对应处理器\nacceptor负责接收新连接并注册到reactor的监控列表中\nhandler负责处理具体业务逻辑\n工作流程是应用程序将感兴趣的事件，如某些文件描述符上的读、写等注册到reactor；reactor监听事件；事件触发后reactor根据事件类型分发给响应handler，handler通过非阻塞IO执行业务逻辑，或者委派给某个线程池中线程执行，完成后重新注册事件（边缘触发：如有剩余数据则可以下一次事件到来时继续处理；水平出发：可以明确事件监听范围，如从读切换到写，减少无效事件通知）\n单线程reactor模型：三种组件的功能在一个线程中执行，因此没有多线程竞争问题，避免线程切换。但是无法利用多核CPU，并且线程崩溃会导致服务不可用。适合客户端数量少，业务快速的场景。\n多线程reactor模型：主线程负责事件的监听与分发，业务逻辑由线程池中线程异步处理。可以利用多核CPU处理计算密集型任务，使用线程池可以减少创建销毁线程的开销。但是主线程可能成为瓶颈，多线程存在数据竞争。适用客户端数量适中，业务处理较长的场景。\n主从reactor模型：引入负责连接的主reactor和多个负责IO的子reactor，主reactor将连接分配到子reactor中由子reactor负责后续IO事件。可以减少单点压力，并且可以动态调整子reactor数量以适应负载。但是复杂度高，适用于高并发系统。\nProreact模式 reactor是非阻塞同步网络模式，每次感知到由IO事件就需要应用主动调用read读取，因此是同步的，基于待完成的IO事件\nproreact是异步网络模式，发起异步读写请求时需要传入数据缓冲区的地址，内核会处理IO事件并将结果通知给应用。是异步的，基于已完成的IO事件。\nlinux不支持真正的异步io，windows支持。\n负载均衡问题 多个节点如何分配客户端请求。\n引入一个中间的负载均衡层，负责分配请求。\n对于每个节点存储数据相同的情况，可以使用加权轮询算法，即根据节点的硬件配置设置权值，将请求轮流转发给各节点。\n对于分布式系统，每个节点存储的数据不同。使用哈希算法则可以根据key定位到数据所在节点。但是不便于系统扩容和节点故障的情况，因为会破坏哈希映射至，导致大量数据迁移。\n一致性哈希算法将数据与节点映射到同一个哈希环上，数据根据哈希值定位到环上位置后，沿顺时针找到最近的节点作为归属，则增加节点只影响顺时针相邻的后继节点，从而减少数据迁移量。\n但是一致性哈希算法不能保证节点在哈希环上均匀分布。可以引入虚拟节点机制将物理节点映射成多个虚拟节点分散到哈希环的不同位置。因为物理节点直接计算出的哈希值可能是很集中的，但可以对每个物理节点计算多个虚拟哈希值。\n虚拟节点优点在于：1. 解决物理节点分布不均2. 高性能节点可以分配更多虚拟节点3.移除某个物理节点时数据均匀迁移到不同其他节点，避免单一节点承受过多压力，分散影响。\n进程管理 进程，线程，协程 进程是操作系统资源调度的基本单位，拥有自己的资源，包括内存地址空间，堆和栈。优点：拥有自己的地址空间，稳定性和安全性（无法访问其他进程的地址空间和堆栈资源）较高。缺点：切换进程的开销较大，需要保存上下文，进程之间通信比较复杂\n线程是CPU调度的基本单位，和同一进程中的其他线程共享资源。优点：创建线程资源开销小，线程之间通信方便（可以直接共享内存），切换线程方便（只需要保存线程级别的上下文，不需要保存进程状态）。缺点：线程之间存在数据竞争的问题，需要同步和互斥机制解决。\n协程是一种轻量级的用户态线程，调度由用户程序控制，不需要内核参与。因此没有上下文切换开销，适合大量并发任务。缺点就是实现比较复杂。\nLinux用task_struct管理进程 https://zhuanlan.zhihu.com/p/457403125\n每个进程由进程控制块管理，在Linux中进程控制块是task_struct结构体。\n其中有一个指针叫做stack，指向内核栈。\n与线程有关的资源在thread_struct中，比如寄存器状态等，具体和硬件架构有关。\n一个进程崩溃不会影响其他进程 进程之间具有隔离性，一个进程崩溃不会影响其他进程的地址空间、网络连接等资源。\n什么是分配给进程的资源 包括虚拟内存，比如代码段，全局变量静态变量区域，堆文件描述符，还有信号量，网络套接字等。\n为什么要在进程之下设计线程 当一个软件的功能比较复杂的时候，需要一种程序运行的实体，既能并发执行，又能够方便的相互通信。如果单独采用进程的话，一个进程只能同步执行一系列操作，没办法并发。采用多个进程的话，进程之间通信不方便，并且消耗系统资源多。因此在一个进程下面设计线程，能够满足并发执行和通信方便的需要。\n多线程和单线程的比较 多线程可以并发或者并行处理多个任务，提高执行效率，充分利用CPU计算能力。但是可能导致数据竞争和死锁，并且会占用更多CPU和内存资源。 如果线程过多，总体上占用的系统资源（栈，程序计数器，寄存器状态，线程控制块等）也会上升。并且C++中一个线程崩溃还会导致所属的进程崩溃。\nC++中线程崩溃导致进程崩溃 如果某个线程非法访问了地址，例如访问了只读内存、不存在的地址或者没有权限的地址（内核地址空间），那么会产生segmentation fault，导致进程崩溃。这是因为线程共享地址空间，出现非法访问可能会导致内存的不确定性，进而影响其他线程，因此会将进程崩溃。\n具体是发现非法访问内存后，内核会给进程发送kill信号\n进程切换和线程切换 进程切换的开销比线程切换要大。进程切换时需要保存虚拟地址空间里的信息，比如全局变量，堆，栈，文件描述符，等等用户空间的资源，还要保存内核态的栈，寄存器(CPU寄存器、程序计数器)。这些信息保存在进程控制块内。然后我们取出另一个进程的进程控制块，并且恢复这个进程的上下文。取出PCB的过程可能涉及缺页。\n如果两个线程是同一个进程的，那线程切换比进程切换简单，只需要保存上一个线程的栈，程序计数器还有其他寄存器。地址空间是共享的。如果属于不同进程那就和进程切换一样。\n进程的状态 运行态，就绪态，阻塞态，创建态和结束态。如果大量进程处于阻塞状态就会交换到外存，那么就是挂起态。挂起有阻塞挂起和就绪挂起。\n进程的控制 进程的控制信息保存在PCB结构里，包括进程的id，用户的id，进程的状态，进程的资源比如地址空间mmstruct，文件描述符等，还有CPU寄存器的值（切换上下文的时候保存）。\n相同状态的PCB组织成链表。形成各种队列，比如就绪队列，阻塞队列\n创建进程：首先创建一个PCB结构，并且填入进程相关信息，比如pid，用户和组。然后分配必须资源，如地址空间。最后插入到就绪队列等待调度\n阻塞进程：首先从运行队列中找到PCB，然后保存上下文，并且修改状态为阻塞，最后插入阻塞队列\n唤醒进程：首先从阻塞队列找到PCB，然后修改状态为就绪，最后插入就绪队列\n终止进程：首先从队列中找到PCB，如果正在运行就停止运行并调度其他的进程。然后检查是否有子进程并将子进程转移给1号进程收养，再释放进程的资源，最后删除PCB。\n进程的终止有自己正常终止（比如main函数返回，或者调用exit），也可能异常终止（如被kill掉。有可能是程序逻辑错误或者访问非法内存，也可能是被手动kill掉）。\n父进程终止：子进程变成孤儿进程，被1号进程收养。子进程终止：将资源归还给父进程。\n进程阻塞一般是去等待某些IO事件，或者等待锁和子进程终止。注意时间片耗尽不会导致阻塞，而是导致进入就绪队列。\n进程的用户态和内核态 用户态：是CPU运行用户程序的一种模式，权限较低，不能直接访问硬件资源。用户态负责运行用户应用程序。\n内核态：是CPU运行操作系统内核的一种模式，拥有最高权限，可以直接访问硬件资源。内核态负责管理系统的核心功能，如进程调度、内存管理、驱动等。\n之所以要区分内核态和用户态，是为了保证操作系统的隔离性。一方面，恶意的用户应用可能攻击操作系统，正常的应用也可能会故障。如果这些应用能够执行特权指令可能会导致整个系统的崩溃。另一方面，把内核态和应用态区分开也可以方便用户编程，不用考虑底层的实现和不同硬件的差异。只需要使用操作系统提供的抽象。\n用户态切换到内核态的方式有系统调用、中断、异常。只有系统调用是用户主动陷入内核的。\n系统调用：注意系统调用中不涉及进程切换，始终是同一个进程，只是状态变了。系统调用属于一种陷阱\n进程的上下文包括CPU中寄存器的值，进程的状态以及栈的内容。linux中进程的上下文保存在task struct结构体中。但是在特权模式切换的时候只需要保存寄存器级别的上下文，比如程序计数器，寄存器状态和栈指针，因为系统调用以后还是同一个进程，并没有切换进程。\n用户态进程调用某个系统调用函数 系统调用函数将参数和对应的系统调用号写入特定的寄存器，并执行syscall指令触发CPU状态切换，跳转到内核入口函数 把用户态的上下文压入内核栈中保存，比如pt regs结构体 设置内核栈指针，并从寄存器取得系统调用号并调用相应的系统调用处理函数，并将参数从寄存器传递给该函数 内核态任务执行完后将返回结果存入寄存器中并通过sysret指令回到用户态。这个过程中还会恢复用户态的上下文。 系统调用由应用程序主动触发，完成服务后返回原执行流。\n异常和中断由程序错误或外部事件强制触发，可能终止原执行流 异常：异常是程序运行中出现的非预期情况或错误。比如程序要执行一个非法操作（除以0），或者访问的页面不在内存中。异常发生后会被操作系统捕获，并执行异常处理函数。\n发生异常同样会切换到内核态。对于page fault会从外存中调入页面，对于除以0等错误，会直接杀死进程或重启进程。\n中断就是CPU停止执行当前任务，去处理其他的事情，处理完再返回当前任务。\n中断分为硬中断和软中断。硬中断是由于外部硬件设备的变化导致的中断，分为可屏蔽和不可屏蔽的。可屏蔽的中断通常是一些设备的IO事件，比如敲键盘，点鼠标，还有可能是打印机缺页等。这些中断不处理不会导致操作系统崩溃，所以可以屏蔽。不可屏蔽中断一般是一些致命错误，比如突然断电等。\n软中断是软件触发的中断机制，不直接由硬件产生。主要用于延迟处理，将不需要立即执行的任务安排到稍后执行。\n为了避免中断处理程序过长，将中断分为上半部和下半部。上半部一般是硬件触发的硬中断，用来快速处理和硬件紧密相关或者时间敏感的请求，下半部一般是内核触发的软中断，以内核线程的方式延迟处理上半部未完成的工作。除了下半部外，一些内核自定义事件也属于软中断，比如内核调度、RCU锁等\n硬中断是外部硬件设备触发的，是异步随机发生的，可以设置优先级，有些可以屏蔽，要求尽快响应完成，支持中断嵌套，在中断上下文中执行。\n软中断是内部软件指令触发的，是同步触发的，没有优先级，不可以屏蔽，可以延迟执行，可重入（允许被硬中断抢占），在进程上下文或内核线程中执行。\n软中断的调度时机一般是某个硬中断退出时，或在cpu负载较低时唤醒ksoftirqd线程处理软中断。\n中断通常是异步的，陷阱通常是同步的。意思是中断通常是CPU执行某些任务时被打断。而陷阱是CPU主动在已知的地方发生\n中断的过程首先是接收到中断信号，如果没有屏蔽该中断，就保存当前现场，包括寄存器和栈指针，程序计数器等。接着根据中断向量号找到对应的中断处理函数，并切换到内核态执行中断处理函数。中断处理结束后再恢复上下文，并切换回用户态接着执行之前的指令。\n正常状态下中断处理并不切换进程。\n用户线程和内核线程 https://www.cnblogs.com/FengZeng666/p/14219477.html\n用户线程就是有用户态应用程序或者函数库管理的线程，这类线程只运行在用户态，操作系统是看不见的。优点：不需要切换内核模式，节省开销；允许应用自行调度，更加灵活；缺点：操作系统只能看到进程，所以一个用户线程阻塞会导致进程阻塞；一个进程中的用户线程也没办法利用不同的CPU并行执行；一个线程不可能抢占另一个线程，因为没有这个特权；时间片是分配给进程的，所以用户线程需要共享时间片，一个线程得到的时间片较少。\n内核线程是操作系统直接支持管理的，优点：一个线程阻塞了，还可以调度别的线程运行；同一个进程的不同线程可以在不同CPU并行执行；得到的时间片比较多。缺点：线程调度需要切换内核态；\n线程的映射：用户态线程在系统调用等与内核有关的操作需要内核线程协助。用户线程和内核线程的映射有多对一，一对一，多对多。\n多对一就是用一个内核线程服务多个用户线程，那么其他想进入内核态的用户线程就被堵塞了。一对一是每个用户态线程都可以映射到一个内核线程。但是创建内核线程会有一定开销。Linux和windows都是一对一的。\n多对多就是用少量的内核线程服务大量的用户线程，可以减轻阻塞其他用户线程，也可以减少占用资源。但是管理复杂。\n进程间通信 进程通过虚拟地址空间实现了隔离性，因此进程之间的协作需要专门的通信机制。\n宏内核进程间通信有管道、消息队列、共享内存、信号量、信号和socket机制。 • 管道：字节流、两个进程、单向、匿名和命名 • 消息队列：消息体、多个进程、单向或双向 • 共享内存：内存区间、多进程、单向或双向 • 信号量：计数器、多进程、单向或双向、同步和互斥 • 信号：事件编号、多进程、单向 • 套接字：数据报文、两个进程、单向或双向、网络栈\n管道 管道就是一个单向队列，一端发送一端接收。\n有匿名管道和命名管道。 • 匿名管道就是bash命令中的竖线，只允许临时将前面的输出传递给后面的输入。 • 命名管道需要用mkfifo创建，将数据写入命名管道后终端会阻塞，直到另一端被读出。 • 管道通信效率低，不适合进程之间频繁交换数据\n匿名管道通过系统调用pipe，在内核中开辟了一块内存缓冲区，并返回两个文件描述符。文件描述符用于读和写缓冲区。\n匿名管道只能用于父子进程和子进程之间的通信，这是因为子进程会继承父进程的文件描述符表，从而找到内核缓冲区。\n命名管道通过函数mkfifo创建，可以用于不相关进程之间的通信。因为命名管道有一个具体的路径名，可以像文件一样打开。不过实际上并不存在于磁盘，而是一个内存缓冲区。\n由于匿名管道和命名管道文件都是先进先出的，所以不支持lseek操作。\n管道传递的是无格式的字节流。\n消息队列 消息队列是一个保存在内核的消息链表，链表的每一个节点都是一个消息体。消息的发送方和接收方约定好消息体的格式，发送方将数据放在消息队列中就返回，接收方需要时去队列中取得数据，因此是非阻塞的。\n消息队列的生命周期随内核，如果不释放消息队列或关闭操作系统，则消息队列一直存在。但是管道的生命周期随进程，引用管道的进程终止则管道消失。\n消息队列适合频繁交换数据，因为是非阻塞的。但缺点是不适合大量数据传输，因为消息体有大小限制，并且消息队列在内核，因此有用户态和内核态之间的数据拷贝开销。管道比消息队列更快，因为不需要消息体的封装与解封装。\n共享内存 管道和消息队列都要利用内核缓冲区，因此会有拷贝开销。\n共享内存机制是将不同进程的虚拟地址空间映射到相同的物理地址空间。优点是不需要用户态和内核态之间的拷贝，缺点是产生数据竞争。\n信号量 为了解决数据竞争引入信号量实现进程之间的互斥与同步。\n信号量就是一个整形计数器，因此并不能缓存数据，而是表示能进入临界区的进程数量。\n信号量有两个原子操作：P是信号量-1，V是信号量+1.P时信号量\u0026lt;0则P阻塞，V后信号量\u0026lt;=0则表示有进程需要唤醒。P和V必须成对使用。\n信号量初始化为1则为互斥锁。信号量初始化为0可以实现进程同步：另一个进程执行完后再V，这是当前进程就可以P然后执行。\n信号 信号用于单向的事件通知而不是数据传输。信号量也可以通知，但是需要进程主动查询信号量。信号则可以随时通知另一个进程，并且另一个进程不需要阻塞等待，内核会切换到处理函数。\nLinux内核为sigint等信号提供了默认处理函数，也可以自己定义信号处理函数。还可以屏蔽信号。但是sigkill等有些信号是不能屏蔽的，因为用于终止一个进程。\n信号处理并不是中断处理。因为处理信号的时机一般是进程从内核态返回用户态之前。信号处理函数一般在用户态执行，上下文会保存在用户栈上。如果信号处理函数中有系统调用则再次进入内核态。\n信号与中断的联系与区别： • 联系：1. 都是异步通信机制；2. 处理完毕时返回原来的断点；3. 有些中断或信号可以屏蔽 • 区别：1.中断有优先级，信号没有优先级；2.信号处理程序在用户态运行，中断处理程序在内核态运行；3. 中断响应是即时的，而信号响应有一定延迟。\nsocket 不同主机进程之间通信需要跨网络。\nsocket创建参数有协议、报文类型。可以实现TCP、UDP和本地进程间通信。\n• TCP通信过程 ◦ 服务端通过socket创建套接字，并使用bind将套接字绑定到特定的IP地址和端口上。接着调用listen监听客户端的连接请求。 ◦ 客户端通过socket创建套接字后，调用connect向服务端指定IP地址和端口发起连接请求 ◦ 服务端正在监听该端口并不超过最大连接数，则完成三次握手并建立连接。调用accept返回一个文件描述符代表与客户端的连接并开始数据交换。 ◦ 客户端断开连接则调用close，服务端读取数据时读到EOF，处理完数据后调用close表示连接关闭。 ◦ 注意：监听的socket和建立连接的socket并不一样。建立连接后双方通过read write或send recv向建立连接的socket读写数据。 • UDP通信过程 ◦ 服务端和客户端分别使用socket创建套接字并绑定到某个端口。 ◦ 通信时通过sendto和recvfrom。 ◦ 通信完成后调用close关闭套接字 • 本地socket ◦ 可以是有连接的，也可以是无连接的 ◦ 区别是在socket绑定时绑定一个本地文件系统中的路径作为地址。\n线程的互斥、同步和锁、信号量 多线程同时操作竞争变量时，非原子操作和并发会导致不可预期的结果。含有竞争变量的代码是临界区，我们希望同一时间只有一个线程进入临界区。这叫做线程的互斥。\n多个线程协作时，有些线程需要等待另一些线程的执行结果。因此需要保证线程之间的执行顺序，这叫做线程的同步。\n主要采用锁和信号量机制实现线程之间的互斥与同步。\n锁 任何进入临界区的线程必须先获得锁，然后进入。如果一个线程持有锁，其他请求锁的线程需要等待。\n• 自旋锁 ◦ 属于一种互斥锁，但被锁住的线程会一直请求锁而不会阻塞，避免线程切换开销 ◦ 只能适用于非常轻量的临界区 • 无等待锁 ◦ 当线程获取不到锁的时候就阻塞，进入锁的等待队列\n操作系统中常见的锁 • 互斥锁 ◦ 只有获得锁的线程能访问资源，其他线程必须等待锁的释放 ◦ 依赖于硬件的test and set原子操作实现。一个线程获取互斥锁时检查状态，如果空闲则设置上锁，如果已锁则被阻塞 • 读写锁 ◦ 支持并发读取，但只允许一个线程写入。 ◦ 有读锁时可以获取读锁，但是写者阻塞；有写锁时读写均阻塞 ◦ 两个计数器实现，一个记录读者数量，另一个记录是否有写者 • 条件变量 ◦ 多线程之间传递信号和状态信息 ◦ 用于实现阻塞互斥锁。允许线程在得不到锁的时候阻塞而不是自旋，当锁释放后可以通过条件变量唤醒一个或所有阻塞的线程。 • 信号量 ◦ 通用的同步互斥原语 ◦ 用于实现互斥：信号量代表可以同时访问临界区的线程数量，小于0时阻塞 ◦ 用于实现同步：上一个线程获取信号量并置为0，下一个线程需要等上一个线程V之后才能执行。 • 乐观锁 ◦ 并不实际加锁，认为数据冲突不常见。通过在执行临界区后检查数据版本号或者时间戳查看是否发生了数据竞争。如果发生了就回滚 ◦ 例如在线文档、git等。git就是允许用户各自修改，只有提交时冲突了才要求各自修改后提交。 ◦ 不适合经常冲突的场景，因为回滚的成本高 • 悲观锁 ◦ 通过严格的锁或信号量机制保证某一时刻只有一个线程访问临界区\n生产者-消费者问题 问题描述：生产者生成数据后放在一个缓冲区，消费者从缓冲区取出数据处理，任意时刻只能有一个生产者或消费者可以访问缓冲区。\n需要3个信号量：互斥信号量用于所有进程互斥进入临界区、资源信号量用于消费者进程等待生产者进程生产、空槽信号量用于生产者进程等待消费者产生空槽。\n生产者进程和消费者进程都是死循环，生产者先P空槽、再P互斥、进入临界区、V互斥、V资源。消费者P资源、P互斥、进入临界区、V互斥、V空槽。\n哲学家就餐问题 问题描述：5个哲学家围成圆圈，每人中间有一把叉子。哲学家必须拿起两把叉子吃饭，吃完后会放回两把叉子。不能死锁\n如果只有叉子信号量，则有可能每个哲学家都拿起左手的叉子。\n可以增加互斥信号量，认为拿叉子是临界操作，同一时刻只有一个哲学家拿叉子。但是并发性低。改进：为哲学家定义状态：饥饿、进餐、思考。只有两个邻居都没有进餐的时候才能进餐。因此在获取互斥量之前检查两边哲学家状态。如果不是都饥饿就阻塞。吃完后通知其他哲学家。实际上将这个互斥信号量用于同步。 还可以为哲学家编号，让偶数哲学家先拿左边叉子，奇数哲学家先拿右边叉子。\n读者写者问题 问题描述：允许同时读、没有写者才能读、没有读者才能写、没有其他写者才能写。\n读者优先策略：采用一个临界区互斥信号量、读者计数器和一个读者互斥信号量，临界区互斥信号量用于控制进入临界区、读者计数器用于记录当前读者数量，读者互斥信号量用于控制读者计数器的修改。 读计数代表了一个读者等待队列，当队列中有读进程时，就可以到达更多的读进程，而写进程被则阻塞在写等待信号量上，因此会导致写者饥饿。\n写者优先策略：对读写进程分别计数，并对计数变量分别用mutex保护。增加写互斥信号量用于写者之间互斥，增加读等待信号量用于读者等待写者。这种方案中写者来后不允许读者进入，读者来后却允许写者堵塞读者。因此对写者有利 读写计数分别代表了两个读写等待队列。当读队列中有读进程时，可以来更多的读进程。但是来了一个写进程就会导致更多的读进程被阻塞在读等待信号量上，因此读队列会慢慢减少。反之，写队列中有写进程，则允许更多的写进程进入写队列排队，但来读进程并不能阻塞更多的写进程进入写队列。所以会导致读进程饥饿。\n读写公平策略：在读者优先策略上增加一个初始为1的flag信号量。读者和写者必须先获取flag信号量再进入逻辑。这是为了写者来后可以阻塞后来的所有进程，读者来后也可以阻塞后来的写进程。 flag信号量可以让写进程阻止更多的读进程进入读队列，而读进程也可以将写进程阻塞在写等待信号量上。\n伪代码： • 读者优先\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // 读者优先采用临界区互斥、读计数器和读计数器互斥 int rCount = 0; semaphore mutex = 1; semaphore rCMutex = 1; void writer() { while (1) { P(mutex); write(); V(mutex); } } void reader() { while (1) { P(rCMutex); if (rCount == 0) { // 第一个读者，阻塞后来所有的写者 P(mutex); } ++rCount; V(rCMutex); read(); P(rCMutex); if (rCount == 1) { // 最后一个读者 V(mutex); } --rCount; V(rCMutex); } } • 写者优先\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 // 读计数器、写计数器、计数器信号量、读等待信号量、写等待信号量 int rCount; int wCount; semaphore rCMutex; semaphore wCMutex; semaphore rWMutex; semaphore wWMutex; void writer() { while (1) { P(wCMutex); if (wCount == 0) { // 第一个写者阻塞所有读者 P(rWMutex); } ++wCount; V(wCMutex); P(wWMutex); write(); V(wWMutex); P(wCMutex); if (wCount == 1) { V(rWMutex); } --wCount; V(wCMutex); } } void reader() { while (1) { P(rWMutex); P(rCMutex); if (rCount == 0) { P(wWMutex); } ++rCount; V(rCMutex); V(rWMutex); read(); P(rCMutex); if (rCount == 1) { V(wWMutex); } --rCount; V(rCMutex); } } • 读写公平\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // 增加一个flag信号量，使得读写双方公平竞争flag信号量 int rCount = 0; semaphore flag; semaphore mutex; semaphore rCMutex; void writer() { while (1) { P(flag); P(mutex); write(); V(mutex); V(flag); } } void reader() { while (1) { P(flag); P(rCount); if (rCount == 0) { P(mutex); } ++rCount; V(rCount); V(flag); read(); P(rCount); if (rCount == 1) { V(mutex); } --rCount; V(rCount); } } 避免死锁 死锁就是多个线程相互等待对方释放锁。\n死锁产生条件 互斥、持有等待、不可剥夺、环路等待。\n互斥：多个线程不能同时访问资源\n持有等待：线程在请求锁时不会释放自己的锁\n不可剥夺：线程的锁在主动释放之前不能被其他线程获取\n环路等待：多个线程获取资源的顺序构成环形图\n排查死锁问题 Linux下可以使用pstack和gdb定位死锁。pstack可以显示每个线程的栈跟踪信息：pstack pid。查看进程下线程的函数调用过程，对比结果查看哪几个线程一直没有变化，并且被锁住的线程一般会卡在某某lock wait函数处。还可以通过gdb工具查看线程等待的互斥锁对象信息，看看是不是被对方持有。\n避免发生死锁问题 破坏互斥、持有等待、不可剥夺或环路等待之一。\n资源有序分配法破坏环路等待。资源有序分配是给会产生竞争的资源编号，每个线程总是按序号从小到大申请资源，这样不会导致先申请大号资源再申请小号资源带来的环路问题。\n破坏互斥就是乐观锁或者将所有资源都拷贝，比如RCU机制，破坏持有等待就是要求线程一次性申请所有需要的资源，不太实际。破坏不可剥夺条件是抢占式分配，就是释放低优先级进程资源并分配给高优先级线程，要求低优先级线程稍后重新申请。\n银行家算法:一个进程申请使用资源时，先试探分配所需资源，检查分配后的系统是否处于安全状态。若是则分配，否则不分配。 其中，安全状态是指系统可以通过调整进程执行顺序，即存在安全序列，使得所有进程都能获得所需资源并顺序完成。 银行家算法的缺点在于，需要事先直到进程所需的最大资源总数，但是进程的需求是变化的。并且实现复杂，需要用到很多数据结构。算法倾向保持一定余量放置不安全状态，因此会产生浪费。安全性检查耗时影响性能。不适合实时系统，因为进程会被阻塞直到找到安全序列。\n进程可以创建多少个线程 进程在虚拟地址空间上创建线程，在32位系统下，因为用户态空间只有3G，所以线程的上限与线程的栈空间大小有关（线程的资源就是栈和一些寄存器）。理论上3G除以栈大小。 64位系统下，虚拟地址空间非常大，因此主要受操作系统参数或性能限制，有参数控制整个系统的最大线程个数。\n设备管理 CPU通过设备控制器操作设备 每个设备都有对应的设备控制器，其中有自己的芯片、寄存器等，可以自行执行逻辑。\n设备控制器有三类寄存器：数据寄存器、命令寄存器和状态寄存器。\n操作系统通过写入命令寄存器，向设备发送命令；写入数据寄存器向设备输出数据；设备完成命令后将状态写入状态寄存器，CPU读取状态寄存器了解命令执行情况；CPU读取数据寄存器输入数据。\nIO设备分为块设备和字符设备 块设备以数据块为数据处理单位，每个块有自己的地址。如硬盘、闪存等。\n字符设备以字符为单位发送或接收字节流，字符设备不可寻址。如鼠标等。\n块设备的数据传输量大，在控制器中存在数据缓冲区。读写数据时会累积读写提升性能。\nCPU通过端口和内存映射与设备控制器通信。端口IO（Port IO）：每个寄存器分配一个IO端口，通过汇编指令操作。内存映射IO（MMIO）：所有的寄存器映射到内存空间中，像读写内存一样控制数据缓冲区。\n端口IO：不占用内存地址空间，通信相对简单。但是需要专门指令访问。端口与处理器架构相关\nMMIO：可以利用内存管理机制，更通用。但消耗一部分内存地址空间，不适合资源受限的嵌入式环境。可能导致内存与IO访问之间的冲突。\nIO控制方式：轮询、中断、DMA CPU向设备控制器发送命令后，设备将完成状态放入状态寄存器。\n轮询：CPU一直查询状态寄存器，直到IO完成。简单，效率低\n中断：设备完成任务后触发中断控制器，中断控制器通知CPU。不适合大量数据传输，因为频繁打断CPU。\nDMA：DMA控制器在CPU不参与的情况下，自行将数据写入内存。CPU只负责一开始的指令。效率高、适合大量数据传输，但成本高，实现复杂。 ◦ CPU向DMA控制器发送指令，将某些数据读取到内存什么地方或写到某个设备的什么地方 ◦ DMA控制器将磁盘控制器发送指令，将数据从磁盘控制器的缓冲区或主机缓冲区读取到自己的缓冲区，再写入内存指定地址或设备缓冲区 ◦ DMA完成后，DMA控制器通过中断通知主机\n设备驱动程序屏蔽设备控制器差异 每种设备的控制器使用模式不同，需要厂商配套驱动程序向操作系统提供统一的接口\n通用块层管理不同的块设备 通用块层向上为文件系统提供访问块设备的标准接口，向下将不同的磁盘抽象成统一的块设备，提供一个管理驱动程序的框架。其次将发送给块设备的IO请求合并、重排、调度从而提高磁盘IO的效率。\n块层调度算法有： • 无调度：即不对IO做任何处理，主要用于虚拟机IO，将调度交给物理机做 • FIFO • 完全公平调度 • 优先级调度 • deadline调度\n缓存机制提高IO效率 为了提高文件访问效率，使用page cache、dentry缓存等机制减少访问磁盘。对于块设备还会使用数据缓冲区实现累积读写。\n从键盘输入一个字符 从键盘输入一个字符，则键盘控制器会产生扫描码数据，并将其缓冲在键盘控制器的数据缓冲区中，键盘控制器向CPU发送中断请求。CPU收到中断请求后保存进程上下文，然后调用键盘的中断处理程序。键盘的中断处理程序是在安装键盘驱动程序时注册的，中断处理函数的功能是从键盘控制器的缓冲区读取扫描码，并根据扫描码找到键盘输入的字符。\n如果输入的是显示字符，就将其翻译成ASCII码，并将其写入读缓冲区队列（存储从输入设备接收到的数据，因为输入可能快于处理），显示器的驱动程序定期从读缓冲区队列读取到写缓冲区队列（存储准备发送给输出设备的数据队列），将写缓冲区队列的数据写入显示器数据缓冲区，显示到屏幕上。最后恢复中断上下文，返回进程。\n调度算法 操作系统中的调度主要有三类：进程调度，页面置换和磁盘调度\n进程调度 作业类型\n进程调度的目的是让不同的作业都能及时快速得到响应。作业可以分为长作业和短作业，CPU密集型作业和IO密集型作业。\n长作业和短作业：需要CPU服务时间较长的和较短的作业。\nCPU密集型作业：需大量CPU计算的作业，等待IO的情况少。\nIO密集型作业：需要大量等待IO的作业，会频繁阻塞。\n进程调度算法 进程调度算法类型\n抢占式调度算法：可以在当前作业没有执行完的时候迫使其挂起，然后执行另一个作业\n非抢占式调度算法：必须等待当前作业执行完再调度下一个作业\n对于实时操作系统来说，为了保证有些高优先级任务及时得到响应，必须要抢占式调度。\n凡是基于优先级的调度算法，都可以实现程抢占式的。但是基于优先级的算法都有可能导致饥饿。\n常见进程调度算法 先来先服务：最简单的算法，按照先来后到的顺序将作业加入一个队列，每次从队列中取出一个作业执行。优点是实现简单，缺点是对长作业有利，对短作业不利。因为短作业会被堵在长作业之后，响应时间长。对CPU密集型的作业有利，对IO密集型的作业不利，因为IO密集型的作业堵塞后需要重新排队。\n短作业优先算法：每次调度时都选择需要时间最短的作业。优点：保证短作业及时得到响应；缺点：长作业可能饥饿。\n高响应比优先算法：定义一个响应比来平衡长短作业，响应比是服务时间加等待时间除以服务时间。对于长作业来说，等待时间越长会提升优先级，对于短作业来说，除了等待时间以外，服务时间越短也会提升优先级。优点：兼顾长短作业；缺点：实现较为复杂，计算有开销，需要准确预测进程的服务时间，这是有难度的。如果短作业不断到达，还是有可能导致长作业饥饿，没有考虑实时性。\n时间片轮转：给每个作业分配一定时间片，时间片用完就切换下一个作业。优点：实现简单，公平。缺点：没有考虑到不同作业的优先级，时间片大小难设置（太长退化成FCFS，太短频繁切换开销大），不利于IO密集，因为时间片可能没用完就睡眠了。\n高优先级作业优先：给每个作业静态分配优先级，或者基于其他指标动态分配优先级，每次调度优先级高的作业。优点：即时响应高优先级请求。缺点：低优先级请求饥饿\n多级反馈队列：设置多个不同优先级的服务队列，优先级越低的队列时间片越长。每次作业加入优先级最高的队列，如果时间片用完就加入低一级队列。高级队列执行完再调度低优先级。优点：兼顾高低优先级，低优先级的服务时间也更长。缺点：低优先级还是有可能饥饿，可以定期重新加回高优先级，不利于IO密集型，因为即使时间长也可能睡眠，缺乏实时性\ndeadline：给每个作业分配截止时间，优先调度接近截止时间的。缺点是有可能再多个截止时间接近的作业中频繁切换，那么都超时\nLinux中的进程调度器 普通任务：CFS完全公平调度器，为调度器队列中的每个进程设置一个虚拟时钟vruntime，表示进程使用了多少虚拟时间，虚拟时间与实际时间成比例，但是会根据进程的权重调整。优先级越高的进程vruntime增长越慢。每次选择vruntime最小的进程执行。使用红黑树存储队列，每次选择红黑树最左边节点（vruntime最小）\n实时任务：deadline调度器、RT实时调度器，deadline调度器选择当前待处理任务中离截止期限最近的任务，红黑树管理（有序），RT调度器由两种模式，一种是先入先出队列，但是支持高优先级随时抢占。另一种是时间片先入先出，优先级相同的任务按时间片轮转。\n页面替换 虚拟内存机制使得每个进程可以使用比真实内存更大的虚拟内存空间。当内存空间不够时需要将一些内存页替换到外存释放空间。\n选择哪些内存页调度？\n常见页面替换算法\n最优替换：最好的选择就是选择最远时间内不会使用的页面替换出去，但是这是不可能实现的，因为我们不知道内存页将来的访问情况。所以这是作为一个基准衡量其他算法的性能。\n先入先出：将所有内存页面按照创建时间加入队列，每次从队列尾部取出一个内存页替换出去。优点：实现简单，缺点：没有考虑到页面的最近使用情况，效果差。\nLRU：最近最少使用考虑将内存页面组织成一个双向链表，将最近访问过的页面提升到链表头部，这样链表尾部就是最近没有使用过的页面。基于局部性原理，认为最近访问过的页面将来也可能再次访问。优点：考虑使用情况，命中率更高。缺点：实现复杂（需要双向链表和映射），如果IO模式不符合局部性原理就会失效（1. 预读失效：对于随机IO来说，预读加载进链表的页面不会访问2. 缓存污染：对于大文件的顺序读来说，读上来的页面看起来都是最近使用过的，但是实际上不会再次访问，并且又把缓存填满了）优化：可以和FIFO配合使用，第一次访问某个页面的时候先加入FIFO队列，用来避免一次性页面污染缓存。第二次访问再加入LRU队列。同理可以设计更多级。\n时钟算法：将所有页面组织成一个环形链表，指针指向某个节点。每个节点都有一个标志位，标志最近是否访问过。需要替换的时候，指针就扫描链表中的节点，如果标志位是1就置为0，如果标志位是0就换出。优点：考虑到了最近使用情况，实现比LRU简单，因为不用双向链表只用单向，还不需要映射，并且不需要频繁的移动节点。缺点：还是有一定实现成本，并且只用一个位表示最近访问频率不精确。总体来说比较好\nLFU：记录每个页面最近被访问的频率，每次替换访问频率最低的页面。优点：考虑到最近访问的次数，不会被一次性数据污染。缺点：实现还是比较复杂，和LRU差不多，并且对工作集切换的情况不友好。因为上一个工作集之前经常访问，但是现在又不访问了，就占着缓存。当前工作集的页面因为访问次数不多所以频繁替换。改进：可以随着时间推移减少每个页面的访问次数，比如除以2.这样兼顾时间局部性。\n磁盘调度 目的是为了尽量减少磁头的移动，减少寻道时间\n常见磁盘调度算法 先来先服务：按照IO时间顺序服务。优点：实现简单，缺点：可能导致频繁移动，性能差\n最短寻道时间优先：下一次优先服务离当前磁道最近的IO。优点：性能比FCFS好，并且实现比较简单。缺点：导致远的IO饥饿\n电梯调度和循环电梯调度：电梯调度就是类似于汽车的雨刮器，每次从一个方向的起点扫描到终点再回头。优点：不会导致远处饥饿，缺点：中间的比较有利。循环电梯调度：到一个方向的终点以后回到起点，返程不服务。优点：更加公平，缺点：一个方向上可能后续没有IO，但是还是要扫描到终点。\nLOOK和CLOOK：就是对电梯和循环电梯的改进，一个方向没有了就立刻掉头。优点：减少扫描终点的开销。缺点：一个方向上一直来IO会导致背后的IO饥饿。\ndeadline：也可以deadline，但是可能导致频繁来回移动，多个请求都超时。\n缓存逐出算法\n一般有LRU，FIFO，LFU，TTL等。\n• LRU：最近最少使用。将所有的缓存项组织成一个双向链表，并且用一个映射管理链表节点，目的是让查找，更新，删除的时间复杂度都是O1.当缓存命中时，就将节点移动到表头。当缓存更新后，也移动到表头这样就可以保证表尾的数据是最近使用最少的。如果缓存没有命中，就将数据读出来并安放在表头，同时逐出表尾的节点。基于局部性原理假设\n• FIFO：将缓存项目按照时间顺序放入队列，每次逐出队列尾部\n• LFU：记录过去一段时间内缓存项目的使用情况，每次逐出最近访问次数最少的\n• TTL：给每个项目设置一个存活时间，超过存活时间就逐出\n优缺点：\n• LRU：实现复杂，预读失效，缓存污染 ◦ 随机IO不符合局部性原理，导致预读页面无法命中 ◦ 大文件的顺序读是最近访问过但之后又不会访问的，这种一次性数据会污染缓存 ◦ 解决：可以和FIFO结合使用，每个缓存项先进入FIFO，第二次访问再进入LRU链表，可以避免一次性数据的污染\n• FIFO：实现简单，性能差\n• LFU：实现复杂，存在工作集切换时的颠簸 ◦ 工作集突然切换的时候，旧工作集内容不会再访问，但占着缓存，导致新工作集被频繁逐出 ◦ 解决：可以每隔一段时间就将最近访问次数减少或者除以二，或者给每一项安排一个生存时间，从而将数据的访问时间纳入考量\n• TTL：优点，可以保证缓存的新鲜度，利用时间局部性，但是可能导致大量缓存同时失效，并且生存时间不易确定\n","date":"2025-04-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/caozuoxitong/","title":"操作系统理论知识"},{"content":"操作系统 比较管道，共享内存和消息队列 管道和共享内存是两种进程间通信方式。管道是通过在内核空间开辟一块缓冲区，写进程向缓冲区写入数据而读进程从缓冲区读出数据实现通信的，共享内存是开辟一块内核空间，让两个不同进程各自映射到用户空间实现进程通信的。\n管道是单向通信的，而共享内存是双向通信的。\n管道只适合少量数据传输，因为缓冲区较小。共享内存分配的空间较大，因此适合较多数据传输。\n管道的拷贝开销大，需要进程拷贝将待写的数据拷贝到内核管道，另一个进程从内核管道中拷贝出来。而共享内存通过映射的方式避免陷入内核态，因此只需要由写进程将写入共享内存区，读进程不需要复制就可以从共享内存区读出。\n共享内存需要锁机制避免数据竞争。\n消息队列是不同进程间约定好消息体格式，发送方将消息体写入内核的消息链表中就返回。接收方将消息体复制出来。由于管道是阻塞的，消息队列是非阻塞的，所以消息队列传输效率更高，适合频繁传输。\nmmap文件有哪些刷盘方式 mmap将文件映射到用户的文件映射区，当进程修改文件映射区内容后会产生脏页。一方面，内核会有线程定期写回脏页。另一方面可以主动调用msync刷回指定虚拟内存区域的脏页（可以是同步等待的，也可以是异步即只是通知内核刷盘，而不阻塞的），也可以调用fsync以文件为单位刷回文件所有脏数据和元数据。最后，使用munmap解除映射时内核会将未同步的脏页刷回。\nmmap的文件会马上放到物理内存吗，什么时候会放到物理内存 mmap仅分配虚拟空间，但是不会马上将文件复制进内存。只有访问虚拟地址时触发缺页中断才会将文件读入页缓存中，并将虚拟地址映射到页缓存的物理地址上。\n如果文件已经在page cache中，因为缺失PTE表项，所以mmap仍需要触发一次page fault。在异常处理流程中先检查page cache然后绑定相应物理地址。\nopen，read，write，unlink，rmdir，truncate函数执行过程 要打开或者创建一个文件，可以使用open系统调用。open系统调用有3个参数。第一个是文件的路径和文件名，第二个是文件的访问模式和操作，第三个是在创建文件时，定义文件的访问权限。\n首先，内核需要解析路径名。解析路径名的方式就是从根目录开始，逐级读取目录文件，并确定下一级目录文件的位置。如果目录较长，这个过程可能会很慢。另外，文件系统提供dentry缓存机制加快解析。如果最近访问过某个文件或目录，就会把该文件对应的目录项缓存在内存中，不会逐目录查找。\n其次，open第二个参数需要指定文件打开以后的访问模式，包括只读，只写和读写。如果文件不存在，可以用O_CREAT指定创建一个文件，还可以指定是否要追加写、截断等。O_EXCL和O_CREAT合用，表示如果文件本身存在就返回创建失败。O_TRUNC表示如果文件存在就把长度截断到0。O_APPEND表示文件打开后总是往尾部追加写。\n创建一个文件的流程包括：路径解析（已经提到），检查该目录是否有权限创建文件，为文件分配inode结构，将文件的名字和inode号更新到目录中。如果需要写入还要分配数据块并更新元数据索引。如果是direct io或者同步写还需要将数据块落盘。\nopen第三个参数代表权限。在创建文件的模式下，第三个参数指定了文件的访问权限，包括文件创建者，所在组和其他成员的读取，写入，执行权限。\nread和write系统调用负责文件的读写。均有三个参数。第一个参数是文件描述符，第二个是缓冲区指针，第三个是读或写的字节数。\nread和write都从文件的读写指针开始，可以通过lseek修改读写指针。lessk的三个参数是文件描述符，偏移量和偏移起始地址。偏移起始地址可以是文件开头，末尾或者当前读写指针。\nhttps://blog.csdn.net/weixin_44698673/article/details/125729055\n删除文件用unlink系统调用，参数是文件路径。之所以参数是文件路径，是因为不同的文件可能对应同一个inode，也就是硬链接。如果通过文件描述符找到文件，就不知道具体想要删除哪一个硬链接。当删除最后一个硬链接的时候，文件的空间才会真正释放。删除文件的过程包括从目录中删除该文件的表项，减少硬链接计数。如果是最后一个硬链接，就调用具体文件系统的删除操作。\n删除目录用rmdir系统调用，参数是目录路径，因为打开目录不会返回一个整数形式的描述符。只能删除空目录，目录不空就要递归删除。\n截断文件是指定一个长度，如果文件超过这个长度就把多余部分删除，如果不足这个长度就用\\0填充到这个长度。truncate和ftruncate系统调用可以截断。前者的参数是路径名和长度，后者的参数是文件描述符和长度。\n区别：truncate之前无需先打开文件，ftruncate需要在打开文件的情况下截断。truncate可能会收到符号链接攻击，意思是高权限的文件被链接到低权限的链接，如果文件系统不完善，那么低权限的用户可能可以操作高权限的文件。\ndentry详细介绍 dentry主要包括文件名和路径到inode的映射。同时dentry在内核中以哈希表和LRU链表的方式存放，从而快速定位到目录项。\npage cache加速原理 Linux上会将文件先读入内存，作为page cache映射到进程的虚拟地址空间。page cache就是struct page中的address_space结构体。每个文件会在内核中只有一份page cache。但是可以有多个进程中的struct file指向。\npage cache不仅是缓存基于文件的数据页，还用于元数据的缓存。不同的page有不同的操作函数，定义在address_space_operations操作集中。\npage cache的数据结构是基数树，用于快速定位某些状态，如脏的页。基数树是一种利用前缀索引快速检索的数据结构，检索复杂度是Ok，k是所有字符串最大长度。page cache的键是页偏移量，使用分层编码压缩的方式减少长度\n使用page cache和不使用page cache有什么区别 使用page cache的就是buffer io，不使用page cache的是direct io。他们之间的区别有几点。第一是buffer io的读写性能更好，因为可以在内存中命中，而direct io每次都要访存。第二是buffer io需要占用一定内存存放page cache，但是direct io不需要。第三个是buffer io 不用管理一致性问题和崩溃恢复问题，因为page cache已经实现了这个机制，但是direct io需要自己管理。buffer io适合大多数场景，但是数据库等需要精细控制IO或者是已经实现了自己的缓存的应用就可以跳过page cache。\n使用direct IO需要注意什么 首先需要手动管理数据的一致性，因为direct io只是保证数据落盘，但不会保证元数据都落盘，需要频繁调用fsync保证所有的元数据都落盘。其次还需要自己实现崩溃恢复，出错重发等机制，然后为了保证性能还需要自己实现批量写入和预读的策略，自己实现读写缓冲区。最后还需要把写入数据块的大小和文件系统数据块的大小对齐。 使用direct io需要在打开文件的时候设置O_DIRECT标志。\nPage Cache的一致性与可靠性 更新page cache上的页就成了脏页。一方面操作系统内核线程定期写回脏页，另一方面用户可以主动调用fsync和sync同步某个文件和整个文件系统的数据。最后内存压力大时会导致回收文件页（丢弃或回写）\n3个系统调用：fsync：将fd文件的所有脏数据和脏元数据写回磁盘；fdatasync：将fd文件所有的脏数据和必要的元数据写回磁盘。必要的元数据是对访问文件有关键作用的元数据，比如文件大小。但是文件修改时间就不写回。sync：将整个文件系统的脏数据和脏元数据写回磁盘。\npage fault 的过程 page fault主要包括缺页异常和权限异常。缺页异常是页表项不存在，权限异常是页表项存在但是没有操作权限。\n触发page fault时，首先这是一个中断，因此要保存用户态上下文，进入内核态，并根据中断向量表找到中断处理函数。然后检查虚拟内存地址属于哪一个vma，如果不属于任何一个vma说明还没有分配，因此终止进程。 根据vma的类型决定缺页处理逻辑：新匿名页缺页则分配零页或新物理页，文件映射缺页则从磁盘加载到page cache。swap页缺页则从swap分区换入被换出的物理页。\n对于匿名页的首次读访问，且表项为空，会映射到一个内容全为0的全局只读零页，后续发生写操作会再次触发page fault权限异常，进入写时拷贝流程。这是为了延迟分配物理页，等到要写入才真正分配。\n对于匿名页的首次写访问，会分配新物理页并更新表项。\n对于文件页的读访问，会从磁盘上读取文件页，并且会用预读机制优化\n对于文件页的写访问，如果是私有文件映射，会写时拷贝避免影响其他进程。如果是共享文件映射，会直接修改物理页。\npage fault 的预读机制 预读机制是触发文件页的缺页中断时，会从磁盘上读取该文件页以及后续若干文件页。预读窗口的大小与访问模式有关，检测到顺序访问会增加窗口，检测到随机访问会缩小窗口。\n一个进程open的文件，进程异常退出了，脏数据会落盘吗 会，因为文件的page cache在内核中独立于进程，进程崩溃了，page cache依然会回写脏数据。但是如果系统也崩溃，如掉电等，则不能保证。\n如果脏数据在用户态缓冲区还没有写入内存page cache，则也会丢失。\n一个进程在写文件，另一个进程删除该文件，删除会不会成功，为什么，写文件的进程能不能继续写 会成功，可以继续写。\n因为每个文件有硬链接计数和进程引用计数。删除文件只是减少硬链接计数，并且从目录中删除该文件的项。只要有进程打开文件，则进程引用计数不为零，数据块仍保留在磁盘中。\n写进程可以继续写，因为进程通过文件描述符操作文件，即使没有文件目录也可以写。并且可以正常落盘。只有最后一个进程关闭文件后才会删除。\n进程线程协程，协程的实现，优缺点 进程是操作系统资源分配的基本单位，线程是CPU调度的基本单位，协程是用户态的轻量级线程。\n进程拥有独立的资源，包括地址空间、文件描述符等。进程之间具有隔离性，不同进程需要进程间通信机制交换数据。进程切换的开销大。\n同一进程内的不同线程共享地址空间，不需要专门的通信机制，但是需要锁和信号量机制防止数据竞争。线程切换的开销比进程小。\n协程是用户态实现的更轻量的执行单位，由用户态的函数库管理，操作系统不知道协程的存在。协程栈只有KB级别，线程栈有MB级别，所以协程可以实现高并发，切换非常快速，而且不需要进入内核态。协程只在等待IO时主动让出CPU而不是被抢占，这是因为内核不能介入协程的执行过程，无法打断协程执行流，并且如果允许抢占则需在任意位置保存全部状态，开销大。协程在线程内串行执行不需要加锁。\n但是协程无法利用多核CPU，因为内核不知道协程的存在，所以无法调度。而且协程的使用和调试复杂，需要函数库支持。\n进程适合需要隔离多个任务的场景，线程适合需要共享数据、并发的场景，协程适合高并发的场景。\n创建进程线程的函数 创建进程： 使用fork可以创建一个进程，具体是根据父进程复制一个子进程，创建时不为子进程分配资源，而是采用写时拷贝技术，其页表项设置为只读，只有写入时触发page fault分配新页面并复制。\nexec系列函数用于替换当前进程的代码和数据，加载并执行新程序，覆盖原进程的代码段和数据段。经常和fork连用，因为fork的写时拷贝技术避免了复制父进程的无用数据。但是保留文件描述符等。\n创建线程： c语言有基于posix接口的pthread库，pthread_create创建线程，pthread_join等待线程结束并回收资源。线程函数调用pthread_exit主动退出。pthread库中还有互斥锁和条件变量。 C++11中有thread库，是基于pthread实现的，用法更简单，创建一个thread对象的时候就启动线程，主线程中调用线程对象的join方法等待线程返回或者调用detach使子线程脱离主线程。有互斥锁和条件变量，C++14提供读写锁。\n怎么排查死锁 用户态死锁：\npstack/gdb（C/C++）\n对进程执行 pstack 或 gdb -p ，查看各线程的调用栈和锁状态。\ntop/htop\n观察CPU占用率异常的进程，死锁线程可能处于高CPU或持续等待状态\nstrace\n跟踪进程系统调用，发现卡在 futex 或 pthread_mutex_lock 等锁操作\n内核态死锁\n内核日志：通过 dmesg 查看 WARNING 或 deadlock 关键字\ncoredump分析：利用 gdb 分析转储文件中的线程堆栈和锁状态\n使用Ftrace分析锁竞争路径\n操作系统如何保持进程隔离性 操作系统通过虚拟内存机制隔离不同内存，虚拟内存机制使得每个进程只能看到属于自己的连续独占虚拟地址空间，看不到其他进程的虚拟地址空间，所以保证了隔离性。\nmmap是否影响进程隔离性 不影响。mmap可以有共享和私有映射。私有映射仍然是隔离的，共享映射是进程间通信方式。\n进程线程之间哪些资源共享哪些不共享，线程独有的资源有哪些，为什么独有 线程可以共享地址空间，文件描述符，代码段，数据段、堆、环境变量和用户身份等。\n但是每个线程有自己独立的线程id，栈和寄存器。因为线程在并行或并发执行不同任务时用到的局部变量和函数不同，如果栈不独立会导致栈帧被覆盖。\n例如，某个线程在栈上分配了局部变量，另一个线程分配了另一个局部变量，假如第一个线程想要释放局部变量，就会导致另一个线程的也释放，因为栈时后进先出的。而堆不会，堆释放了不会立刻归还内存。\n不同的线程会嵌套执行多个函数，函数的返回地址需要按顺序压入栈，如果栈不独立会导致函数返回错乱。\nvfs四个关键结构体 要使用某种文件系统，必须先将其注册到 VFS 核心。注册文件系统就是将该文件系统类型插入一个 file_systems 单链表中。目的是向 VFS 提供 get_sb 和 kill_sb 回调函数，从而装载或卸载该类型的文件系统。 VFS中的对象都是仅存在于内存的。具体文件系统的对象会落盘，有盘上结构\n超级块super block存放了已挂载文件系统的元数据和控制信息，主要用来指向 fs 超级块（s_fs_info）以及 fs 提供的超级块操作表(s_op)。因为不同文件系统的操作时不同的，这个VFS超级块就是为了向上层提供统一的接口。文件系统层面的操作一般有分配、删除、写回inode、将超级块写入盘上、锁住文件系统和解锁文件系统（比如文件系统要sync）等。 inode唯一表示文件，通过inode编号管理，包括操作函数表等。VFS的inode只存在内存中，具体文件系统的inode会落盘。 目录项用来避免重复解析文件路径，加快路径解析速度。 file文件对象表示进程打开的文件示例，包括访问模式、偏移量和操作函数表。 比较LRU和FIFO LRU和FIFO是两种常见的缓存替换算法。\nLRU优先淘汰最久未被使用的数据，基于时间局部性原理，通常使用哈希表和双向链表实现，可以在O1内完成插入查找和删除。数据局部性较好的时候缓存命中率高。\nLRU的缺点第一是实现比较复杂，需要跟踪页面的访问情况，第二是对突然切换工作集的情况不适用，因为可能频繁逐出新工作集中的数据（这是LFU）。第三是不适合随机IO，因为会预读失效，第四是不适合顺序扫描大文件，因为这种模式缺乏时间局部性，每个块看起来都是最近使用的，会污染缓存，也就是缓存被新加载但是不会再被使用的数据填满。\nFIFO按照进入队列的顺序，淘汰最先进入队列的数据，使用队列实现，O1。实现简单，但是性能比较差，尤其是不符合先入先出假设的时候。并且还有belady异常，也就是队列增大，命中率反而下降。原因是队列深度可能不足以覆盖整个工作集（工作集就是一段时间内频繁访问的页面集合）\nLRU适合对性能要求较高的系统，FIFO适合对性能要求不高，资源有限的系统，或者数据访问本身就比较随机的场景。\n如何设计LRU 采用数据结构为双向链表和映射unordered_map。需要保存的信息有LRU队列的容量capacity，当前元素数量size和链表的头尾节点。实现的操作有，一是插入，如果待插入的节点已经存在于链表中就更新值并移动到头部。如果没有就在头部新增一个节点。同时如果size大于capacity就需要把尾部逐出。二是访问，如果数据在LRU中就读出来并且移动到头部，如果不在可能就要从别处读取，比如从磁盘中读取。\n如何改善LRU LRU只记录最近一次访问的时间，可以改成记录最近K次访问情况，比如最近k次访问中至少访问一次。 双队列LRU：将缓存分为两个部分，一个是短期的FIFO队列，另一个是长期的LRU。数据第一次进入短期队列，第二次再进入LRU。可以解决一次性数据带来的预读失效和缓存污染 多级缓冲队列：设置多个优先级不同的队列，每个队列都是LRU的，数据被访问多次后可提升到优先级更高的队列。 操作系统有哪些进程和线程的调度算法 先来先服务、短作业优先、高响应比优先、时间片轮转、高优先级、多级反馈队列\n先来先服务：先来先服务是最简单的调度算法，原理就是将作业按照提交的顺序假如队列，每次从队列的头部取出作业执行。优点：实现简单；缺点：对长作业有利，导致短作业被长作业卡住，响应慢。适合CPU密集型的任务，因为可以不被干扰一直运行，不适合IO密集型的任务，因为IO经常要等待，就要重新排队。\n短作业优先算法：短作业优先是指每次尽量选择执行时间较短的作业。优点：对短作业友好，不会被长作业卡住；缺点：可能导致长作业饥饿\n高响应比优先算法：根据服务时间和等待时间综合计算优先级，服务时间加等待时间除以服务时间。优点：兼顾长短作业，对于两种作业来说，都是等待时间越长优先级越高，对于短作业来说服务时间短，所以优先级略高于长作业。这种基于优先级的算法都可以是抢占式也可以是非抢占式。随着运行可能出现优先级更高的其他作业。\n时间片轮转算法：给每个作业分配时间片，每个作业只能运行一定时间片，然后要重新排队。优点：实现简单，公平；缺点：时间片不好选择，时间片太短导致频繁切换，时间片太长退化成先来先服务。\n高优先级算法：给每个作业分配优先级，每次执行高优先级的作业。优先级可以是静态的也可以是动态的。优点：保证高优先级的作业及时响应；缺点：导致低优先级作业饥饿。\n多级反馈队列算法：设置多个优先级队列，优先级越低的队列时间片越长。将第一次调度的作业放到最高优先级的尾部，如果时间片用完了，就移动到下一级队列。每次只有上一个优先级的队列空才会执行下一个队列的作业。优点：兼顾长短作业，长作业虽然优先级低，但是时间片也长；缺点：仍然可能导致长作业饥饿。并且实现复杂。改进：定期将低优先级队列的内容加回高优先级队列。\nLinux块层如何调度IO请求 上层应用调用read或者write发送IO请求，在文件系统中会封装成bio的形式发给块层。块层会将bio合并转化为一个或多个request，并将request插入对应块设备的请求队列中。每个块设备都有自己的请求队列。请求队列会采用一定调度算法进行排序。比如可以先来先服务，也就是不做调度，还可以用deadline机制，也就是排队时间越长优先级越高。\n多队列机制：暂时说不上来\nLinux的IO栈 首先是一个用户态的程序调用read write等涉及IO的系统调用，然后是进入内核态，由虚拟文件系统VFS将请求交给具体的文件系统处理。文件系统将请求封装成bio发给块层，块层经过调度之后发给设备驱动，设备驱动 再发给设备。\n不同的线程在不同的时间通过page cache去写文件，而这两个IO离得比较近，怎样去做调度\n没看懂，应该是要做磁盘调度算法\n磁盘调度算法 磁盘调度算法的目的是尽量减少磁头移动的距离，减少寻道时间。\n先来先服务：这种算法按照IO请求的到来顺序访问磁盘块。当大量进程IO到来的时候可能会导致访问的磁道很分散，寻道时间长。\n最短寻道时间优先：先选择离当前磁头最近的位置。问题在于可能导致位置较远的IO饥饿。\n扫描算法或者电梯算法：磁头在一个方向上移动，直到到达这个方向上磁道的末尾，然后再换方向。优点是性能较好，不会饥饿，但是缺点是中间的磁道被扫描的频率比两边高。\n循环扫描算法：就是单向的电梯算法，只有一个方向是扫描方向，当一次扫描到头以后就回到起点重新扫描。回来的过程不处理IO。优点：每个磁道被访问的机会比较平均；缺点：请求扫描完不会立刻停止，而是扫描到最后一个磁道。\nLOOK算法和CLOOK算法：就是针对扫描算法和循环扫描算法的优化，扫描到一个方向的最远请求就回头。CLOOK的缺点：如果一个方向一直到达请求，那么就一直无法回头，导致另一个方向饥饿\ndeadline算法：为每个请求设置截止时间，优先服务快到截止时间的请求。优点：可以防止饥饿。缺点：需要额外的资源管理请求的截止时间和更新状态。\n对于SSD来说没必要做磁盘调度优化，因为SSD不需要寻道。\n在文件系统中创建一个文件的流程，如果创建的目录比较长会有什么问题 首先是解析路径，内核需要逐层解析路径的每一个目录，直到找到目标目录。\n然后是检查是否有在该目录下创建文件的权限\n再分配文件的inode\n再将文件的名称和inode编号更新到目录文件中\n如果文件需要写入，还需要为文件分配实际的数据块。把数据块先放在page cache中，然后异步刷新到磁盘。\n如果目录比较大，可能路径解析的过程比较长。因为需要遍历更多的目录条目。我们可以用符号链接只想较深层次的目录，方便快速访问\n怎样保证文件系统写入数据的一致性和原子性 文件系统写入的一致性是指文件系统在任何时候都应该保证逻辑上的一致状态，比如文件的创建时间应该早于修改时间，文件的索引应该是正确的。我们一般讨论的是文件系统的崩溃一致性，比如断电的情况下，有可能导致文件系统的不一致。\n为了保证文件系统的一致性，我们可以采用日志的方式。比如在ext4文件系统中有独立的日志区域。文件系统在执行操作之前，会先将要执行的操作提交到日志去，然后再去完成这些操作，最后清除日志。如果文件系统崩溃，那么在重新挂载以后就扫描日志区域，看有没有未完成的事务。\n如果文件系统在写日志的时候崩溃，那么数据是否能恢复取决于日志的状态，如果日志中的某个事务没有提交，那么这个事务就不重新应用。只有提交的事务才能重新应用。\next4提供了几种日志机制，可以保证不同的一致性等级。最差的是writeback机制，这种机制只把元数据的更新写到日志里。那么在上电之后可能会导致元数据指向的数据还没来得及落盘。第二个是ordered机制，他也只把元数据的更新写到日志里，但是必须保证先把数据落盘，然后再写元数据的日志。这样就不会导致元数据指向不存在的数据。第三种是journal机制，就是把数据和元数据都写入journal。这种机制会造成严重的写放大。\n除了日志机制以外，还有写时拷贝技术。在文件系统中就是在修改数据的时候，不在原始的地方修改，而是把数据复制到一个新的区域修改，修改完成后再将元数据指向新的位置。比如btrfs和zfs都支持COW。优点是不用重放日志，上电以后就是最新的状态。缺点是导致写放大，因为复制会有开销，并且带来了额外的元数据更新。还会导致磁盘碎片化。\n然后还有日志结构文件系统的机制。日志结构文件系统是把整个盘当作一个大日志，在写入的时候也是先写入数据再写入元数据。上电之后再扫描写过的区域重建文件。这种文件系统会定期做检查点来保证一致性。\n基于HDD的文件系统和基于SSD的文件系统的区别 基于HDD的文件系统可能采用连续分配来优化大文件的顺序读取。因为HDD更适合顺序IO。连续分配就是尽量给数据块分配连续的地址，这样可以减少寻道时间。但是容易产生外碎片，需要定期做磁盘整理。还可以通过设置预读来减少磁头移动。\n然而SSD不用寻道，所以顺序和随机IO都擅长。但是SSD的闪存有读写寿命，所以针对SSD的文件系统需要减少写放大。比如采用日志结构的F2FS文件系统。此外TRIM命令对SSD非常重要，可以告诉SSD哪些数据块不再使用，可以擦除，这样就不用等到写入之前再做GC。\nCPU密集型和IO密集型应用特点 CPU密集型应用是指执行流程中大部分时间用于CPU运算（如大规模矩阵计算），IO密集型应用是指执行流程大部分时间用于IO等待（如磁盘寻道或网络传输）\nCPU密集型应用的优化可以：采用多核CPU或者GPU并行计算\nIO密集型的应用可以采用IO多路复用机制减少IO的监控开销，还可以采用缓存机制减少IO次数。还可以通过协程加速。因为协程的切换开销小，并发性高。\n什么是vfork vfork也是创建一个子进程，但是允许子进程和父进程共享虚拟地址空间，所以可能会导致数据竞争问题。vfork的初衷是避免创建进程时复制地址空间以减少开销，但是现在fork采用写时拷贝技术，不会弄直接复制。因此vfork很少用到。\n进程间通信方式 进程通过虚拟地址空间实现了隔离性，因此进程之间的协作需要专门的通信机制。\n宏内核进程间通信有管道、消息队列、共享内存、信号量、信号和socket机制。 • 管道：字节流、两个进程、单向、匿名和命名 • 消息队列：消息体、多个进程、单向或双向 • 共享内存：内存区间、多进程、单向或双向 • 信号量：计数器、多进程、单向或双向、同步和互斥 • 信号：事件编号、多进程、单向 • 套接字：数据报文、两个进程、单向或双向、网络栈\n管道 管道就是一个单向队列，一端发送一端接收。\n有匿名管道和命名管道。 • 匿名管道就是bash命令中的竖线，只允许临时将前面的输出传递给后面的输入。 • 命名管道需要用mkfifo创建，将数据写入命名管道后终端会阻塞，直到另一端被读出。 • 管道通信效率低，不适合进程之间频繁交换数据\n匿名管道通过系统调用pipe，在内核中开辟了一块内存缓冲区，并返回两个文件描述符。文件描述符用于读和写缓冲区。\n匿名管道只能用于父子进程和子进程之间的通信，这是因为子进程会继承父进程的文件描述符表，从而找到内核缓冲区。\n命名管道通过函数mkfifo创建，可以用于不相关进程之间的通信。因为命名管道有一个具体的路径名，可以像文件一样打开。不过实际上并不存在于磁盘，而是一个内存缓冲区。\n由于匿名管道和命名管道文件都是先进先出的，所以不支持lseek操作。\n管道传递的是无格式的字节流。\n消息队列 消息队列是一个保存在内核的消息链表，链表的每一个节点都是一个消息体。消息的发送方和接收方约定好消息体的格式，发送方将数据放在消息队列中就返回，接收方需要时去队列中取得数据，因此是非阻塞的。\n消息队列的生命周期随内核，如果不释放消息队列或关闭操作系统，则消息队列一直存在。但是管道的生命周期随进程，引用管道的进程终止则管道消失。\n消息队列适合频繁交换数据，因为是非阻塞的。但缺点是不适合大量数据传输，因为消息体有大小限制，并且消息队列在内核，因此有用户态和内核态之间的数据拷贝开销。管道比消息队列更快，因为不需要消息体的封装与解封装。\n共享内存 管道和消息队列都要利用内核缓冲区，因此会有拷贝开销。\n共享内存机制是将不同进程的虚拟地址空间映射到相同的物理地址空间。优点是不需要用户态和内核态之间的拷贝，缺点是产生数据竞争。\n信号量 为了解决数据竞争引入信号量实现进程之间的互斥与同步。\n信号量就是一个整形计数器，因此并不能缓存数据，而是表示能进入临界区的进程数量。\n信号量有两个原子操作：P是信号量-1，V是信号量+1.P时信号量\u0026lt;0则P阻塞，V后信号量\u0026lt;=0则表示有进程需要唤醒。P和V必须成对使用。\n信号量初始化为1则为互斥锁。信号量初始化为0可以实现进程同步：另一个进程执行完后再V，这是当前进程就可以P然后执行。\n信号 信号用于单向的事件通知而不是数据传输。信号量也可以通知，但是需要进程主动查询信号量。信号则可以随时通知另一个进程，并且另一个进程不需要阻塞等待，内核会切换到处理函数。\nLinux内核为sigint等信号提供了默认处理函数，也可以自己定义信号处理函数。还可以屏蔽信号。但是sigkill等有些信号是不能屏蔽的，因为用于终止一个进程。\n信号处理并不是中断处理。因为处理信号的时机一般是进程从内核态返回用户态之前。信号处理函数一般在用户态执行，上下文会保存在用户栈上。如果信号处理函数中有系统调用则再次进入内核态。\n信号与中断的联系与区别： • 联系：1. 都是异步通信机制；2. 处理完毕时返回原来的断点；3. 有些中断或信号可以屏蔽 • 区别：1.中断有优先级，信号没有优先级；2.信号处理程序在用户态运行，中断处理程序在内核态运行；3. 中断响应是即时的，而信号响应有一定延迟。\nsocket 不同主机进程之间通信需要跨网络。\nsocket创建参数有协议、报文类型。可以实现TCP、UDP和本地进程间通信。 • TCP通信过程 ◦ 服务端通过socket创建套接字，并使用bind将套接字绑定到特定的IP地址和端口上。接着调用listen监听客户端的连接请求。 ◦ 客户端通过socket创建套接字后，调用connect向服务端指定IP地址和端口发起连接请求 ◦ 服务端正在监听该端口并不超过最大连接数，则完成三次握手并建立连接。调用accept返回一个文件描述符代表与客户端的连接并开始数据交换。 ◦ 客户端断开连接则调用close，服务端读取数据时读到EOF，处理完数据后调用close表示连接关闭。 ◦ 注意：监听的socket和建立连接的socket并不一样。建立连接后双方通过read write或send recv向建立连接的socket读写数据。 • UDP通信过程 ◦ 服务端和客户端分别使用socket创建套接字并绑定到某个端口。 ◦ 通信时通过sendto和recvfrom。 ◦ 通信完成后调用close关闭套接字 • 本地socket ◦ 可以是有连接的，也可以是无连接的 ◦ 区别是在socket绑定时绑定一个本地文件系统中的路径作为地址。\naio与io_uring https://blog.csdn.net/youzhangjing_/article/details/127848418\nhttps://arthurchiao.art/blog/intro-to-io-uring-zh/\nhttps://kdocs.cn/l/chW272CU3dmZ\naio和io_uring是linux中实现异步io的两种框架。io_uring改善了aio存在的一些问题。\n在aio中，提交io操作和获取io操作结果都需要通过系统调用完成，但是系统调用有切换内核态的开销。 aio只支持dio，不支持buffer io aio中提交和获取io操作接口存在用户态和内核态之间的拷贝 io_uring中在用户态映射了一块共享内存，从而消除了内核态切换的开销。用户进程向共享内存提交需要发起的IO操作，内核线程从共享内存中读取IO操作，并且写入返回结果。\nio_uring创建了3块共享内存，分别是提交队列、完成队列和提交队列项数组。这三个队列都是环形的，类似与NVMe协议中的环形命令队列。提交队列存储待执行的IO操作索引，具体的IO操作参数放在提交队列项数组中。完成队列存储操作结果。\n操作流程是用户提交SQE，内核处理，结果写入CQE，用户读取CQE。整个过程不需要用户态和内核态之间的转换和拷贝。\n如何优化malloc 如果所有的线程都从同一个地方分配内存，会使得竞争激烈，所以可以划分不同的分配区。优化malloc有预分配和线程本地缓存的思路。比如tcmalloc就是每个线程独立维护一个本地缓存，用于快速分配小对象。优点在于线程的本地缓存不需要获取全局锁，线程之间没有竞争，所以速度快。中对象的缓存全局共享，大对象用mmap分配。中和大对象使用自旋锁。\n还有jemalloc，将分配区划分更细粒度，每个线程绑定一块区域从而减少全局锁争用。释放空间时还会合并相邻空闲块减少碎片。\ntcmalloc小对象分配更快，jemalloc可以控制碎片化，但是内存占用更高\n注意：ptmalloc分配时，brk在堆上分配需要加锁，因为线程共用一个堆。mmap在映射区分配空间时，每个线程可以绑定自己的分配区（arena），不需要全局加锁，只需要局部锁。\nlinux中fsync过程 用户通过fsync(fd)系统调用发起请求，内核检查fd的有效性，获取对应的struct file对象，根据文件系统调用注册的fsync实现函数。\nfsync会将文件的脏页从page cache写回磁盘，还会将元数据同步回磁盘\n信号量是如何唤醒和阻塞线程的 信号量由一个计数器和等待队列组成。计数器代表当前可用资源的数量，当计数器为正数时，线程可以进入临界区，当计数器为0或负数时，线程需要进入等待队列。等待队列存储因资源不足而被阻塞的线程，调度策略可以是FIFO或按优先级排序的\n网络 tcp如何保证可靠 TCP通过握手挥手建立连接、校验和、序列号确认、滑动窗口、动态重传及拥塞控制等机制，在不可靠的IP层上实现了端到端的可靠传输。其设计平衡了效率与安全性，例如通过SACK减少重传冗余、通过拥塞控制适应网络波动。实际应用中需注意半包/粘包问题（需应用层定义消息边界）及校验和的局限性（无法完全检测多比特错误）\n如何用UDP实现可靠传输/QUIC 在应用层实现序列号和确认应答机制，就是为每个数据包添加递增序列号，接收方通过ack报文确认已接受的数据包。并且超时未确认则重传。通过滑动窗口机制避免乱序。这相当于把tcp在应用层又实现了一遍。\n但是也可以采用类似QUIC的设计。quic的头部分为长头部和短头部，只有在建立连接的时候使用长头部，连接建立后使用短头部。从而降低连接开销。\nquic采用动态协商的连接ID标识连接，而不是五元组（源、目的IP；源目的端口；协议）表示。这样能够支持无缝切换网络，并且不易被追踪。\nquic采用严格递增的序列号，因此重传的序列号也不一样。避免出现TCP中的ACK歧义问题（第一次响请求的ACK在网络中堵塞，重传后又发了ACK，无法知道这两个ACK哪一个是重传的，从而不能准确计算RTT）。并且可以支持乱序确认，因为TCP丢包会导致窗口不滑动，而quic中的序列号是严格递增的，可以通过更大的序列号确认后续重发的包。\n之所以支持乱序确认，是因为数据包内容的顺序不是依赖packet number，而是依赖stream id和stream offset表示的。数据接收方根据stream id将数据归类，再根据stream offset重新排序。\n流量控制方面，quic对每个流限制了最大偏移量，防止一个流数据量过大耗尽缓冲区。quic对所有的流的总数据量也有限制，避免整体资源超出限制。根据RTT时间和缓冲区的处理速率动态调整每个流的最大偏移量和所有流的总数据量。\n拥塞控制和TCP差不多，但是可以根据不同应用选择不同拥塞控制算法，而TCP只能用同一套拥塞控制算法。\n队头阻塞问题 有哪些拥塞控制算法\n经典的有慢启动、拥塞避免、拥塞发生、快恢复。这是reno算法\n还有更多的，但是这里看不完了。\n三次握手的过程 三次握手的过程是1. 服务器监听某个端口2.客户端发送第一次握手报文，包括随机初始化序列号、SYN标志并进入syn sent状态3.客户端返回第二次握手syn ack，是对第一次握手的确认，包括确认应答号是第一次握手的序列号+1，随机初始化序列号，SYN和ACK标志并进入syn rcvd状态3. 客户端返回第三次握手报文，包括确认应答号是序列号+1，而且可以携带数据包。进入established状态，4. 服务器收到第三次回收进入established状态\n注意序列号代表报文第一个字节的编号，下一个序列号是上一个序列号+上一个报文长度。确认应答号是期望收到的字节编号。例如上一个请求序列号是1，长度1000，则成功收到的确认应答号是1001，下一个报文序列号是1001。\n每一次握手丢失会怎么样 首先，tcp有超时重传机制，如果迟迟收不到应答会重发请求或者响应。重发次数可以设置，每一次重发间隔是指数增长的。不过，第一次握手、第二次握手、普通报文和挥手阶段重传的参数是不一样的。\n如果第一次握手丢失，会导致一直收不到ack应答，导致触发重传。一直收不到应答则最终建立连接失败。\n如果第二次握手丢失，会导致服务端收不到第三次握手，服务端会重传ACK。此外客户端会认为是第一次握手丢失，所以会各自重传第一次握手。注意，如果客户端重传第一次握手后收到迟到的ACK响应，会与当前期望的ack不匹配，认为是旧连接，从而丢弃迟到的ack报文。如果重传次数耗尽，就会关闭连接。\n如果第三次握手丢失，服务器会以为第二次握手丢失从而重传第二次握手，客户端根据重传的第二次握手重新发送第三次握手，但是客户端不会主动重传第三次握手。\n什么是syn flood 攻击，有什么危害，如何解决 如果攻击者伪造不同ip大量发起第一次握手而不发送第三次握手，就会导致服务器处于大量syn rcvd状态，从而占用服务器的系统资源。\n处于syn rcvd的连接放在服务器的半连接队列中，半连接队列过多会导致无法为新连接分配资源，导致服务器无法响应正常请求。\n可以增大半连接队列、减少第二次握手的重传次数以快速释放无效连接，或者启用syn cookie技术，就是服务器收到第一次握手后不为连接分配资源，而是再响应报文中嵌入一个cookie值，客户端返回携带正确cookie的第三次握手才会为连接分配资源。\n什么是半连接队列和全连接队列 半连接队列是服务端收到第一次握手后，处于syn rcvd状态的连接队列，全连接队列是服务端收到第三次握手后处于established状态的连接队列。\n半连接队列用于临时存放未完成握手的连接请求，全连接队列用于缓存已经建立但是来不及被处理的连接。\n半连接队列采用哈希表存储，为了收到第三次握手后快速根据tcp四元组定位特定连接。全连接队列使用链表存储，只需要先进先出取出队列头部即可。\n四次挥手的过程 连接双方都可以主动关闭连接，1.主动关闭放发送第一次挥手，就是设置FIN位的报文，表示主动关闭方不再发送数据但是可以接收数据，进入fin wait 1状态2.被动关闭方发送第二次握手，就是一个ack报文，表示知道关闭请求，但是自己可能还有数据要发送，可以接着发送数据。如果没有数据发送也可以将第三次挥手合并到第二次挥手一起发送。第二次挥手发送后被动关闭放进入close wait状态。3.被动关闭方没有数据要发送了，则发送携带FIN标志位的第三次挥手并进入last ack状态4.主动关闭方收到第三次挥手并发送ack报文，进入time wait状态5. 被动关闭方收到第四次挥手进入closed状态。\n每一次挥手丢失会怎么样 第一次挥手丢失，会导致主动关闭方收不到ack响应，重发第一次挥手。超过最大重传次数后就强制断开连接\n第二次挥手丢失，主动关闭方会认为第一次挥手丢失，从而重传第一次挥手，被动关闭方收到重发的第一次挥手，就再发第二次挥手。\n主动关闭方收到第二次挥手后进入fin wait2状态，如果主动关闭方用close关闭，则代表不再收发数据，因此close wait会有超时时间，如果超时时间内未收到第三次挥手则直接关闭连接。如果是shutdown关闭连接则是不发，但是可以收，因此主动关闭方可以一直处于finwait2.\n第三次挥手丢失则收不到ack，被动关闭方会重发第三次挥手。和第一次挥手相似。\n第四次挥手丢失，则被动发送方认为第三次挥手丢失，则重发第三次挥手。\ntime wait和close wait过多的原因，怎么解决 time wait是四次挥手结束后，主动发起关闭的一方进入的状态。作用是1. 防止旧连接干扰，吸收网络中残留的数据包，避免影响后续新建的同端口连接（序列号虽然随机但是可以回绕，如果序列号刚好符合下一个同端口连接则会导致数据错乱）2.如果最后一次ACK丢失则另一方会重发FIN，这时可以重发ACK避免异常关闭连接，持续2MSL，认为数据包全部死亡。\ntime wait过多说明连接频繁断开，可能是因为频繁使用短连接、或者大量客户端建立连接后不发送数据导致长连接超时、或者http长连接请求的数量达到上限（每个长连接只能处理上限次请求就要关闭）。\ntimewait过多会导致占用系统资源和端口资源。占用系统资源指会占用文件描述符、内存和CPU。占用端口资源是指timewait时无法对相同IP相同端口发起连接。对于客户端来说，端口资源有限，占满会导致新连接报错。对于服务端，虽然监听单一端口，但是可以处理不同客户端，只是不能和刚才这个客户端马上建立连接。但是都会占用系统资源。\n解决timewait过多可以开启长连接代替短连接、排查是否有网络问题导致大量连接超时、允许复用timewait窗口（开启tcp_te_reuse和tcp_timestamp，通过引入时间戳区分新旧数据包）、调高长连接请求上限。\nclose wait是被动关闭方发送第二次挥手ACK后进入的状态，该状态允许被动关闭方继续发送数据。\nclose wait过多的原因是被动关闭方可能由于代码缺陷没有正确关闭连接。\nclose wait过多会导致占用系统资源（文件描述符、内存等）、占用端口。\n如果暂时close wait过多可以重启服务强制释放连接。但是根本在于排查代码的缺陷，需要正确关闭连接。\nclose和shutdown关闭 close是关闭socket的文件描述符，并减少引用计数，当引用计数减为0就会真正释放资源并出发4次挥手。close会完全关闭双向通信，无法会通过套接字进行读或写操作\nshutdown是关闭指定方向的通信，而不是减少引用计数。\nclose会立即发送fin包，可能丢弃缓冲区的未发送数据，导致数据丢失。如果接收缓冲区有未读数据则会发rst而不是fin。shutdown会等待缓冲区处理完毕再发fin。\n主动调用close后，直接进入timewait，只有3次挥手，调用shutdown有4次\n什么时候会用rst报文 目标端口未开放，则服务器直接回复rst拒绝连接 重复syn报文，例如一个syn报文在网络阻塞，然后重传新的syn并且建立了连接，就会返回rst终止无效请求 连接一方因为异常终止连接，又重启，收到对方继续发的数据就会返回rst 接收到数据包序列号不在滑动窗口范围内，或者数据包校验错误，会发rst 尝试连接timewait状态的服务器会rst 接收方缓冲区溢出，处理不过来 tcp保活机制检测长时间无活动 Linux查看已连接socket、端口的命令 ss命令可以看，参数-a是所有，-t是tcp，-u是udp，-l是监听端口，-p是展示进程信息\nnetstat可以看，参数一样，但是性能没有ss好\nTCP和UDP区别 tcp是面向连接的有序可靠传输，udp是无连接的不可靠传输。\ntcp需要复杂的重传和建立释放连接开销，头部大。udp传输效率高但是可能丢失，头部小。\ntcp适合文件传输、网页浏览等需要可靠性的场景，udp适合实时性高，如视频通话等场景。\n在电梯间内网络不稳定，如何解决音视频信号传输卡顿 可以动态调整码率，减少传输数据量。\n采用quic协议，首次握手时间仅需1rtt，后续可以0rtt握手。并且支持多路复用，允许并行传输数据流，支持前向纠错，发送冗余数据包，可以减少重传次数。并且切换网络可以保持连接不中断，避免重建连接。最后内置tls加密，不需要专门加密握手。\n因为弱网条件丢包率高容易卡顿，需要通过quic改善可靠性。\nTCP拥塞控制过程 TCP拥塞控制的主要过程可以简化为以下几个阶段：\n​慢启动（Slow Start）​ TCP连接建立后，拥塞窗口（cwnd）初始化为1个MSS（最大报文段长度）。 每收到一个ACK确认，cwnd增加1个MSS，因此cwnd呈指数增长。 当cwnd达到慢启动阈值（ssthresh）时，进入拥塞避免阶段\n拥塞避免（Congestion Avoidance）​ 在拥塞避免阶段，cwnd每经过一个RTT（往返时间）增加1个MSS，呈线性增长。 目的是避免cwnd增长过快导致网络拥塞\n​拥塞发生（Congestion Detection）​ 如果发生超时，TCP认为网络拥塞严重，将ssthresh设置为cwnd的一半，cwnd重置为1，重新进入慢启动阶段。 如果收到3个重复ACK（快速重传），TCP认为发生部分丢包，进入快速恢复阶段\n​快速恢复（Fast Recovery）​ 在快速恢复阶段，ssthresh设置为cwnd的一半，cwnd减半并加上3个MSS，然后进入拥塞避免阶段。 目的是在部分丢包的情况下快速恢复传输效率\n总结：TCP拥塞控制通过慢启动、拥塞避免、拥塞发生和快速恢复四个阶段动态调整发送速率，避免网络拥塞并提高传输效率\n两个进程可以用同一个端口吗 如果是不同协议，如一个是TCP，一个是UDP，则各自端口空间独立，可以重用。\n如果是同一协议，但是不是同一IP，也可以重用。因为可以配置多个网卡，转发到不同应用。\n内核通过四元组表示连接，如果四元组相同，默认不能显式绑定同一IP地址的同一端口。\n如果开启SO_REUSEADDR选项，则可以复用timewait状态的端口。或者开启tcp_tw_reuse和tcp时间戳，也可以重用time wait状态的端口\n如果开启SO_REUSEPORT选项，则可以多个进程绑定同一个IP和端口。绑定同样IP和端口的连接放到一条哈希冲突链上，\nsocket怎么重用连接 为了避免频繁创建和销毁socket，可以使用socket连接池。原理是预创建多个socket，需要用到连接时取出，用完归还。\n初始化时创建一组socket放入池中，避免后面请求时同步的三次握手建立连接。需要使用连接时从池中获取空闲连接，用完后归还到池中而不是关闭。从而允许下次再用。\n需要定期检查连接的有效性，发送心跳包探测连接是否断开。\n连接池可以动态扩展和收缩，根据当前负载和资源消耗。\n空闲连接超时后需要释放，避免资源泄漏。\n一条TCP连接上可以发多少个http请求 短连接模式下，每个http请求结束后会立即释放tcp连接。\n长连接模式下，多个http请求可以复用一条tcp连接。理论上只要tcp存在，则可以发的http请求没有上限，但是服务端会限制单个tcp连接处理的http请求次数，这个参数可以调整。\nsocket连接中，如果客户端在read()的时候服务器挂掉，会怎么做？客户端有感知吗\n如果是服务器进程崩溃了，则内核会回收其socket资源，并发送FIN报文，触发四次挥手。客户端的read会立刻返回0，表示对方已经关闭连接，所以客户端有感知 如果服务器宕机， a. 如果开启了tcp保活机制，则一段时间后会发送探测报文，探测报文多次无响应会断开连接 b. 如果没有开启则会一直阻塞，直到客户端下一次给服务器发送报文得不到响应，并且超时重传也得不到响应才会断开 数据库 事务的特性，如何保证 事务的特性是ACID，即原子性、一致性、隔离性、持久性。\n原子性是指一个事务中的所有操作，要么全部完成，要么全都不完成。\n一致性是指事务操作前后，数据满足完整性约束，不能出现我账户的钱减少而对方账户钱不增加的情况。\n隔离性是指，数据库允许多个并发事务读写数据，需要隔离多个事务读写数据时的相互干扰。\n持久性是指，数据库事务提交后，对数据的修改是永久的，即使系统故障也不会丢失。\n原子性通过undo log回滚保证，在事务执行之前，数据库会记录操作前的数据状态到undo log，如果事务失败则根据undo log逆向操作，将数据恢复事务之前的状态。\n一致性是由原子性隔离性和持久性保证的\n隔离性可以通过锁机制和多版本并发控制保证。\n持久性可以通过redo log保证。\nundo log和redo log什么时候起作用 undo log用来保证事务的原子性，保证事务失败时可以通过undo log回滚到事务发生之前的状态。redo log用来保证事务的持久性，保证系统故障时可以根据redo log重放操作。 事务在commit之前不会落盘，因此这个阶段主要是根据undo log保证能够回滚。事务在commit之后保证落盘，这个阶段如果出问题就根据redo log重放操作。\n事务的隔离级别 四个级别：读未提交、读已提交、可重复读、串行化\n读未提交是一个事务的修改在提交之前就可以被其他事务看到。\n读已提交是一个事务的修改只有在提交之后才能被其他事务看到。\n可重复读是一个事务执行过程中看到的数据一直和事务启动时看到的数据是一致的。是innodb引擎的默认隔离级别\n串行化是对记录加读写锁，多个事务通过读写锁访问数据。\n脏读是指读取到其他事务还未提交的数据，这些数据是有可能回滚的。\n不可重复读是一个事务多次读取同一个数据，但是前后读到的不一样。比如数据被其他事务提交，但是这个数据和本事务开始时看到的不一样\n幻读是一个事务中满足查询记录的条目数量会因为其他事务提交数据而变化。\n读未提交可能发生脏读、幻读、不可重复读\n读已提交可能发生不可重复读和幻读\n可重复读可能发生幻读\n串行化不会发生这些现象。\n可重复读的实现原理 在可重复读级别下，每个事务在第一次查询时会生成一个readview，后续的查询语句会利用read view找到开始查询时的数据，所以每次查询都是一样的。\n可重复读不能解决幻读 事务的读取有快照读和当前读。快照读是普通的select语句，基于事务启动时的read view。当前读是select dor update、update或insert等显式要求读取最新数据或者可能修改数据的操作。\n事务如果更新了其他事务插入的数据（虽然看不到），会导致更新快照，后续查询会包含新数据。\n可以在开启事务后，马上执行当前读语句，这样会给记录加next key锁，避免其他事务插入新记录。\nMVCC版本链实现、记录的可见性 读提交在每个select执行之前重新生成read view\n可重复读在执行第一个select时生成一个read view，然后该事务都用这个read view\nread view有4个字段，包括创建read view 的事务id，创建read view时当前数据库中未提交的事务id列表，创建readview时当前数据库中未提交的事务中最小事务的id，创建read view时当前数据库预留的下一个事务的id\ninnodb存储引擎的数据表记录中，每条记录会有隐藏列，包括事务id和指向旧版本记录的指针。指针的意思是说，每次改动记录时会将旧版本的记录写到undo日志中，这个指针就可以找到旧版本记录。\n如果记录的事务id小于read view中未提交事务的最小id，就说明记录在创建read view之前已经提交，所以可见。\n如果记录的事务id大于read view预留的下一个id，说明事务是在创建read view 之后创建，所以不可见。\n如果记录的事务id在两者之间，则检查read view的未提交事务列表，如果在列表则说明未提交，所以不可见，如果不在则已经提交，所以（对读提交来说）可见\n事务没有commit之前会不会持久化，为什么需要undo log 事务在commit之前，数据的持久化程度取决于redo log的刷盘策略。\n在事务的运行过程中，mysql会把日志写到redo log buffer中，等到事务真正提交的时候，再将buffer中的内容写到redo log文件中。但是这个过程没有绕过page cache，也就是说提交的时候只是写到了pagecache，还需调用fsync保证落盘。\n但是，在事务还未提交的时候，redolog也可能落盘，因为innodb有一个后台线程，会定期将redolog buffer中的日志写到pagecache，然后调用fsync持久化，因此事务没有commit也会部分落盘。并且其他事务提交时，可以设置将redo log buffer全部落盘，因此可能被带着持久化。第三种情况是事务的redo log buffer超过指定大小，这时候也需要写盘。\n因为事务在未提交时候也可能部分落盘，所以需要undo log。\n一条update是不是原子性的 update是原子性的，主要通过锁和undolog日志保证。执行update时会加行级锁，保证一个事务更新一条记录时不会被其他事务打扰。事务执行过程中会生成undolog，如果执行失败可以根据undolog回滚。\n一个事务的语句特别多会怎样 为了保证事务的原子性和一致性需要加锁，如果语句过多会导致锁住太多数据，使得数据库性能下降，并可能会锁超时。其次需要保存的回滚记录太多占用空间。最后这个事务的执行时间长，会导致主从延迟（？）\nmysql的锁 mysql中有全局锁、表级锁和行级锁。\n全局锁会将整个数据库锁住，用于全库备份。\n表级锁： • 表锁：lock tables 表名 read/write，read就是读锁（共享锁，其他线程只读），write就是写锁（独占锁，其他线程不能访问）。注意加锁之后在解锁之前本线程不允许访问其他表。表锁的颗粒度大，影响并发性能 • 元数据锁：这里的元数据是指的表结构，操作数据表时会自动加元数据锁。CRUD加读锁，更改表结构加写锁。注意这里写锁申请不到，会导致所有事务不能申请读锁。阻塞住 • 意向锁：当事务需要对某行加锁时，先申请表级的意向共享锁或者意向排他锁，这样其他事务可以快速判断该表中是否有行级锁，避免逐行扫描。比如一张表获取了行级独占锁，那么另一个事务就不能对这张表加锁。 • auto inc锁用于管理表内的自增列，保证自增字段的唯一性和连续性。auto inc锁是插入语句执行完就释放，而不是等待事务结束再释放。还有一种更轻量的自增锁，将自增字段赋值之后就释放了，而不是等到插入语句完成。\n行级锁：innodb引擎支持，myisam不支持，需要先加表级意向锁避免逐行扫描 • 记录锁： ◦ 锁住的是记录，可以有共享和独占两种。一个事务对一行加共享锁后，其他事务还可以对这一行加共享锁，但是不能加独占锁。如果加了独占锁，其他事务不能对这行加锁。 • 间隙锁 ◦ 间隙锁是在可重复读级别下引入的一种行级锁，锁定的是一个区间而不是具体的记录。加锁之后不能对区间中做插入操作，从而避免幻读。 • next key锁 ◦ 结合了记录锁和间隙锁，既能锁定记录本身不被修改，又不允许在范围内插入记录。\nLevel DB join table 实现方式 数据库如何处理null 数据库如何判断事务的可见性 • 数据库存储引擎格式 • B+树上数据记录1 1 1，一个操作将其更新为 1 2 2，如何操作？ ◦ 记录日志 • mysql中事务A读 1 1 1，另一个事务B将其update为1 2 2，但没有提交，那么当前处理器，B+树中存了哪些数据，页面的值是什么，哪些信息发生了变化 • 版本号存在哪里，如何组织 • RocksDB • 主键索引和联合索引的区别 • 主键索引在索引表上只存有索引数据吗 分布式 RDMA\nraft协议\nGFS如何保证高可用\n存储系统如何做容灾备份\nLinux命令 • 查看磁盘使用量的命令 ◦ df命令查看文件系统的整体磁盘空间占用情况，-h转换成MBGB显示，-i显示inode使用情况，-T显示文件系统类型 ◦ du命令查看目录或文件的磁盘占用 ◦ 有隐藏文件怎么办 ▪ 隐藏文件以一个点开头，可以指定通配符：.[!.]* • .*表示以.开头的所有文件 • [!.]表示第二个不为. 即不包括父目录 ◦ df统计文件系统整体空间，包括已删除但被进程占用的文件，du统计现存文件的占用。两者差异过大可以用lsof列出系统打开文件进行排查 • 查看某个端口被哪个服务占用的命令 ◦ ss命令 ◦ netstat命令 查看dd命令进度的方法 在较新版本的dd中，可直接通过status=progress参数显示实时进度\ngit场景用法 数据结构与算法 B+树、基数树、红黑树、还有哪些树结构、比较\n口述一下用两个栈模拟队列\n用两个栈模拟队列就是需要两次翻转。两个栈分别是入队栈和出队栈。比如将12345加入入队栈，在出栈的时候先将其压入出队栈变成54321，最后弹出出队栈就变成了12345.\n八个常见排序算法 排序算法分为基于比较的和非比较的。非比较的排序算法中一个数的值就决定了应该放在哪个位置 基于比较的排序算法有 选择排序、冒泡排序、插入排序、希尔排序、归并排序、快速排序、堆排序 非比较的排序算法有计数排序、基数排序、桶排序。 非比较的排序算法很有局限性，对数据有很多限定。例如计数排序需要小范围整数、基数排序需要数据是相同位数、桶排序需要数据均匀分布。并且非比较算法通常需要较多辅助空间。\n选择排序：每次扫描数组可以选出本轮最小的元素，交换到数组的开头。因此需要On2。选择排序的执行时间与数据无关。 冒泡排序：每一轮扫描将较大的数据和后面相邻元素交换。On2如果某一轮没有数据交换，则说明已经有序可以退出。因此顺序数组可以实现On的复杂度 插入排序：类似扑克牌方式，数组的前端是有序数组，每次将当前的元素插入有序部分，也就是逐个交换，直到前面的元素比该元素小。如果本身有序则On。插入排序比选择排序好，因为对于有序数组可以降为On，而且内层循环很有可能提前终止，而冒泡排序提前终止比较少见。插入排序很适合接近有序或规模较小的数组，因为这些数组很容易提前终止内层循环。归并排序和快速排序拆分到较小子区间的时候可以转为使用插入排序，更快。 希尔排序：比快排差但是不需要递归，可以用在不支持递归的环境。希尔排序是分组插入排序，将数组按间隔分为多个组（比如0，4，8为1组，1，5，9为一组等），对每组执行插入排序。接下来将间隔缩减为一半，再执行插入。因此数据会越来越有序，适合插入排序。最后间隔为1.\n快速排序：随机选择区间内某一个元素作为基准，将区间内所有小于基准的交换到基准之前，将所有大于基准的交换到基准之后。因此基准就在应该在的地方。分别再排序pivot左边和pivot右边区间 之所以随机选择是因为对于有序数组来说会使得左边的子区间特别小、右边子区间特别大，使得递归树倾斜 双路快排是pivot相等元素过多时，避免重复元素堆积到单侧，导致容易出现有序情况，从而递归树倾斜。每次将等于pivot的元素平均分到pivot两侧：\n选择基准值（通常为左端元素，但推荐随机化选择 i指针向右移动，直到遇到大于基准值的元素；j指针向左移动，直到遇到小于基准值的元素。 交换i和j的元素，继续扫描直至两指针相遇。 将基准值与j指针位置交换，完成分区。 优势：通过双向扫描，减少单侧元素堆积，使分区更均衡，降低递归深度 三路快排是每次将等于pivot的元素都排到一起，可以避免接下来的区间中再出现pivot，从而减少了左右递归区间的长度 • 指针定义与初始化 ◦ lt（左边界）：初始指向l，左侧区域的上界。 ◦ gt（右边界）：初始指向r+1，右侧区域的下界。 ◦ i（遍历指针）：初始指向l+1，从左向右扫描元素。 • 遍历与交换逻辑 ◦ 当前元素 \u0026lt; 基准值：与lt+1位置的元素交换，lt和i均右移。 ◦ 当前元素 \u0026gt; 基准值：与gt-1位置的元素交换，gt左移（i不动，需重新检查交换后的元素）。 ◦ 当前元素 = 基准值：仅i右移。 • 终止条件：当i与gt相遇时，遍历结束。 • 基准值归位 ◦ 将基准值（原左端元素）与lt位置的元素交换，此时基准值位于中间区域的左边界。 ◦ 递归处理左区域（l至lt-1）和右区域（gt至r），中间区域无需处理 B+树 增、删、查Olog_m^n n为总数据量，m为每个节点的最大子节点树 B+树是一种多路平衡查找树，所有数据记录在叶子节点，非叶子节点只存键和子结点指针。叶子节点形成一个链表，便于范围查找和遍历。由于多路，节点存储密度高，适合磁盘，能够减少磁盘IO次数，例如EXT4中的extent。传统inode使用多级间接索引，而B+树可以减少查询次数，提高效率\nB树 增、删、查Olog_m^n n为总数据量，m为每个节点的最大子节点树\nB树也是一种多路平衡查找树，数据放在叶子节点和内部节点上，叶子节点既存储键值又存储数据本身或者数据的指针。并且叶子节点没有链式连接，遍历不方便（这是因为数据在非叶子节点上，有链表也没用）\n与B+树相比，B树不适合范围查询，因为需要回溯遍历。B树不如B+树扁平，因为B+树的非叶子节点不存储数据，可以放更多子节点指针。B树的删除插入节点可能在任意层分裂和调整。而B+树只影响叶子节点，复杂度较小。\nB树的优点在于键值唯一，因为B+树只有叶子节点才放值，可能存储重复的键\n红黑树 红黑树是一种自平衡的二叉查找树，最长路径不超过最短路径的两倍，特点是插入删除的时候，旋转次数少，保证增删改查的时间复杂度是Ologn，适合内存中的数据结构，比如虚拟内存管理中的VMA 通过颜色翻转和很少的旋转可以保持平衡，常数时间\nB+树如果插入导致叶子节点分裂需要递归调整父节点的索引键\nB+树插入流程 插入步骤\n• 定位叶子节点\n从根节点开始，根据关键字大小向下查找，直到找到对应的叶子节点（插入操作仅在叶子层进行）\n• 直接插入（未溢出）\n若叶子节点当前关键字数 k\u0026lt;m−1，直接按序插入新关键字并更新指针，操作结束\n• 分裂叶子节点（溢出处理）\n若插入后关键字数 k=m，则分裂为两个节点：\n• 左节点：保留前 ⌈m/2⌉ 个关键字。\n• 右节点：包含剩余 ⌊m/2⌋ 个关键字。\n• 中间关键字上浮：将第 ⌈m/2⌉ 个关键字（即左节点的最大值）复制到父节点作为索引\n• 递归调整父节点 ◦ 若父节点因接收上浮关键字导致 k=m，则继续分裂父节点，直至根节点。 ◦ 若根节点分裂，则生成新根（树高增加1），新根包含1个关键字和2个子指针\n示例（4阶B+树）\n插入43：根节点（也是叶子节点）变为 [43]。\n插入48, 36：叶子节点变为 [36, 43, 48]。\n插入32：节点分裂为 [32, 36] 和 [43, 48]，关键字36上浮到父节点\n关键点\n• 分裂策略：偶数阶（如4阶）取 ⌈m/2⌉（即第2个关键字）上浮；奇数阶（如3阶）取中间关键字\n• 索引更新：内部节点仅存储关键字副本（或最大值）和子指针，不存储实际数据\nAVL树 AVL树也是一种自平衡二叉查找树，可以保证时间复杂度为logn。AVL树要求每个节点的左右子树高度差不超过1，否则需要旋转调整。树的高度比红黑树小，因为红黑树只是保证最长路径不超过最短路径的两倍。但是旋转调整的开销大。红黑树由于放宽了平衡限制，可以保证旋转操作不超过3次，所以适合频繁修改的场景。\n基数树 Ok 基数树是一种前缀树，结点存储字符或部分字符串，路径从根到叶子标识完整键，适合需要按前缀快速检索字符串的场景，通过合并公共前缀减少存储冗余，例如page cache就是基数树\n三种树比较 B+树和红黑树查询都是Ologn，但是红黑树插入删除的开销小。\nB+树是为磁盘优化的，一是树的高度小，便于减少访存，适合大数据量场景，二是结点与磁盘页面对齐，可以预读其他结点，三是双向链表便于范围查找和遍历。\n红黑树的高度高于B+树，但是插入删除结点的旋转的次数少，开销小。而B+树需要合并和分裂结点，操作复杂度为Ologn\nB+树用于文件系统和数据库索引，大规模数据树高可控。\n红黑树适合内存中经常修改删除的场景，能够减少更新开销。比如stl中的map和set。\n基数树能够通过共享前缀减少存储量。例如两个字符串共享前缀，则前缀放在父节点中，后面不同的部分放在两个子节点。通过前缀的匹配能够快速查询字符串，Ok，k是最长字符串长度。比如在page cache中将文件偏移量映射到物理页，偏移量作为键，4K页的地址作为值。\n完全二叉树 只允许最后一排不满， 中间不可以有空缺，最后一排必须从左往右排序。\n堆 堆是一种完全二叉树。\n大根堆是指堆中每个节点都要大于等于子节点，小根堆是堆中每个节点都要小于等于子节点。\n堆通常用数组的形式实现，按照层序遍历存放在数组中\n建堆：1. 自下而上：将新元素插入叶子节点并且向上过滤；2.自上而下： 将新元素插入根节点并向下过滤\n向上过滤：将某个叶子节点与父节点比较，如果大于/小于父节点就交换，直到根节点。\n向下过滤：将父节点与子节点比较，若小于子节点就和最大/最小的子节点交换，直到叶子节点。\n堆排序\n从小到大排序就是小顶堆（每次弹出最小的根节点），从大到小排序就是大顶堆。\n小顶堆/大顶堆建堆：第一步将数组调整为堆，On\n• 从最后一个非叶子节点开始逆序下沉/上浮\n排序：每次将堆顶元素交换到末尾，并缩小堆范围重新建堆，Onlogn\n其他 实时系统优化 可以从任务调度、高可靠性和资源利用方面展开。比如调度进程的时候可以分配优先级，并且采用抢占式调度，让更紧急的任务提前执行。\n内存优化 • 预分配内存池：避免动态内存分配导致的碎片和延迟。例如，在启动阶段为实时任务预留固定内存区域 • 大页内存（HugePage）：减少TLB缺失率，提升内存访问效率（适用于高频访问的数据缓冲区）\n将ZNS用在云存储上 https://mp.weixin.qq.com/s?__biz=Mzg3MjY5MTc0Ng==\u0026mid=2247485054\u0026idx=1\u0026sn=3606a61e043dee464899134109496a9d\u0026chksm=cf0c4dc9e45288826fc3e866dd3a67f92afebb709f0e3c6141842d9eb2f891da28327af6f6cf#rd\n字节跳动做过相关的研究，实际上效果并不好。一方面，使用ZNS需要对软件栈做较大的修改，比如这个顺序写限制。那么对于现有的生态适配是一个挑战。第二个，就是ZNS本身的优化也有点虚。比如，他最原始的那篇文章中是将文件系统层次的GC也当作正常的IO带宽来算，因为盘并不知道文件系统的GC操作，而GC也就是把数据从盘上读出来再写到另一个地方去，那你带宽当然是大的。但是垃圾回收的带宽并不是用户真是想要的，所以ZNS的性能优化本身就是存疑的。第三点，ZNS将盘内管理的工作交给主机去控制，从某种程度来说是给主机增加了负担，因为盘上本身是有一个主控的。对于云服务来说更是这样，本身存储节点也有一定计算能力。那现在将这些结点上的计算能力浪费了。因此不是特别合适。但是我手机上没这个问题，因为手机上一方面用的本身是F2FS，他的管理方式本来就适合ZNS，不会因为使用ZNS额外增加开销。另一方面手机的UFS本身没有计算能力，确实比较适合ZNS。但是对于云服务来说，我看不是特别适合。\n怎么解决多 CPU 下同时访问自旋锁的性能问题 无锁数据结构替代：对高频访问场景（如计数器），优先使用原子变量或无锁队列\n混合锁策略：对长临界区代码切换为互斥锁（如pthread_mutex），避免CPU资源浪费\n读写锁分离（Read-Write Lock）\n对读多写少场景，使用读写自旋锁（如rwlock），允许多个读线程并发访问，减少写线程阻塞时间\n缓存行填充（Cache Line Padding）\n将锁变量独占一个缓存行（如64字节对齐），避免与其他变量共享缓存行。例如Linux内核的spinlock_t结构体通过__attribute__((aligned(64)))强制对齐\n分片锁（Sharded Lock）\n将全局锁拆分为多个子锁，细化锁的粒度，通过哈希函数将资源映射到不同子锁，降低单个锁的争用率。例如，天翼云在高并发CDN场景中采用外层锁+内层锁的分级架构，通过哈希算法将80个Worker进程的竞争分散到多个外层锁，最终仅少数进程进入内层锁竞争\n缓存行乒乓效应 缓存行乒乓效应（Cache Line Ping-Pong Effect）是多核系统中因多个处理器频繁读写同一缓存行导致的高性能损耗现象，其本质是缓存一致性协议（如MESI）的频繁触发。以下从原理、影响及优化策略展开分析： 共享缓存行的争用\n当多个CPU核心同时访问同一缓存行内的数据时（即使数据不同），会触发缓存一致性协议（如MESI）的广播机制。例如：\n假共享（False Sharing）：不同线程访问同一缓存行中的独立数据（如数组相邻元素），导致缓存行被频繁无效化\n硬件机制的限制\n缓存行粒度：缓存以固定大小（通常64字节）的缓存行为单位管理，无法区分同一缓存行内的不同数据 MESI协议开销：每次缓存行状态变更（如从Modified到Invalid）需跨核心同步，导致总线带宽和CPU周期浪费\n二、性能影响\n吞吐量下降\n单核场景下操作耗时若为1单位，双核可能因乒乓效应增至2-3单位\n例如，全局计数器在多核下的吞吐量可能随线程数增加而指数级下降\n资源浪费\n缓存带宽占用：缓存行频繁传递占用内存控制器带宽；\nCPU空闲等待：核心因等待缓存行同步而无法执行有效指令\n三、优化策略\n将高频访问的变量独占一个缓存行，避免假共享。例如：\n","date":"2025-04-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/mianjingzongjie/","title":"面经总结"},{"content":"（AI生成）\n题目 有 2 个 api aligned read 和 write，调用他们实现其中的 read 和 write 调用接口\n请使用C语言帮我实现程序，并给出思路和代码的详细注释：\naligned size = 4kb\nint aligned read(int fd, void *buf, u64 pos u64 len)\nint read(int fd, void *buf，u64 pos, u64 len)\nint aligned write(int fd, void *buf, u64 pos, u64 len)\nint write(int fd, void *buf，u64 pos, u64 len)\n思路 为了实现 read 和 write 接口，我们需要调用 aligned_read 和 aligned_write API。这两个 API 要求缓冲区和位置都是对齐的（aligned size = 4KB）。因此，我们需要确保在调用 aligned_read 和 aligned_write 时，缓冲区和位置都是 4KB 对齐的。\n对齐检查:\n确保传递给 aligned_read 和 aligned_write 的缓冲区和位置都是 4KB 对齐的。\n分段处理:\n如果读取或写入的范围不对齐，我们需要分段处理对齐的部分和非对齐的部分。\n缓冲区管理:\n使用临时缓冲区来处理非对齐的部分。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdint.h\u0026gt; #define ALIGN_SIZE 4096 // 模拟 aligned_read 和 aligned_write API int aligned_read(int fd, void *buf, uint64_t pos, uint64_t len) { // 模拟读取操作 lseek(fd, pos, SEEK_SET); return read(fd, buf, len); } int aligned_write(int fd, void *buf, uint64_t pos, uint64_t len) { // 模拟写入操作 lseek(fd, pos, SEEK_SET); return write(fd, buf, len); } // 实现 read 接口 int read(int fd, void *buf, uint64_t pos, uint64_t len) { uint64_t aligned_pos = pos \u0026amp; ~(ALIGN_SIZE - 1); uint64_t end_pos = pos + len; uint64_t aligned_end_pos = (end_pos + ALIGN_SIZE - 1) \u0026amp; ~(ALIGN_SIZE - 1); uint64_t aligned_len = aligned_end_pos - aligned_pos; void *aligned_buf = aligned_alloc(ALIGN_SIZE, aligned_len); if (!aligned_buf) { perror(\u0026#34;aligned_alloc\u0026#34;); return -1; } // 读取对齐的范围 int ret = aligned_read(fd, aligned_buf, aligned_pos, aligned_len); if (ret \u0026lt; 0) { free(aligned_buf); return ret; } // 复制所需的数据到目标缓冲区 memcpy(buf, (char *)aligned_buf + (pos - aligned_pos), len); free(aligned_buf); return len; } // 实现 write 接口 int write(int fd, void *buf, uint64_t pos, uint64_t len) { uint64_t aligned_pos = pos \u0026amp; ~(ALIGN_SIZE - 1); uint64_t end_pos = pos + len; uint64_t aligned_end_pos = (end_pos + ALIGN_SIZE - 1) \u0026amp; ~(ALIGN_SIZE - 1); uint64_t aligned_len = aligned_end_pos - aligned_pos; void *aligned_buf = aligned_alloc(ALIGN_SIZE, aligned_len); if (!aligned_buf) { perror(\u0026#34;aligned_alloc\u0026#34;); return -1; } // 读取对齐的范围 int ret = aligned_read(fd, aligned_buf, aligned_pos, aligned_len); if (ret \u0026lt; 0) { free(aligned_buf); return ret; } // 复制新的数据到对齐缓冲区 memcpy((char *)aligned_buf + (pos - aligned_pos), buf, len); // 写入对齐的范围 ret = aligned_write(fd, aligned_buf, aligned_pos, aligned_len); free(aligned_buf); return ret \u0026lt; 0 ? ret : len; } // 测试函数 int main() { // 示例文件操作 int fd = open(\u0026#34;testfile.txt\u0026#34;, O_RDWR | O_CREAT, 0644); if (fd \u0026lt; 0) { perror(\u0026#34;open\u0026#34;); return 1; } char write_buf[] = \u0026#34;Hello, this is a test for aligned write and read.\u0026#34;; char read_buf[sizeof(write_buf)]; // 写入数据 if (write(fd, write_buf, 0, sizeof(write_buf)) \u0026lt; 0) { perror(\u0026#34;write\u0026#34;); close(fd); return 1; } // 读取数据 if (read(fd, read_buf, 0, sizeof(read_buf)) \u0026lt; 0) { perror(\u0026#34;read\u0026#34;); close(fd); return 1; } printf(\u0026#34;读取的数据: %s\\n\u0026#34;, read_buf); close(fd); return 0; } 详细注释 aligned_read 和 aligned_write 模拟: 这两个函数模拟了对齐的读取和写入操作，实际实现中可能需要根据具体需求进行调整。 read 接口: 对齐计算: 计算对齐的起始位置 aligned_pos 和结束位置 aligned_end_pos。 分配对齐缓冲区: 使用 aligned_alloc 分配对齐的缓冲区。 读取对齐的范围: 调用 aligned_read 读取对齐范围的数据。 复制数据: 从对齐缓冲区复制所需的数据到目标缓冲区。 释放缓冲区: 释放分配的对齐缓冲区。 write 接口: 对齐计算: 计算对齐的起始位置 aligned_pos 和结束位置 aligned_end_pos。 分配对齐缓冲区: 使用 aligned_alloc 分配对齐的缓冲区。 读取对齐的范围: 调用 aligned_read 读取对齐范围的数据。 复制数据: 将新的数据复制到对齐缓冲区的相应位置。 写入对齐的范围: 调用 aligned_write 写入对齐范围的数据。 释放缓冲区: 释放分配的对齐缓冲区。 通过上述方法，我们可以确保在调用 aligned_read 和 aligned_write 时，缓冲区和位置都是 4KB 对齐的，从而实现高效的读取和写入操作。\n追加问题 如果系统内存是有限的，读和写可以做哪些优化？\n在内存有限的情况下，我们可以对读和写操作进行一些优化，以减少内存使用和提高性能。以下是一些优化策略：\n1. 分块处理 对于大文件的读写操作，可以将文件分成多个小块进行处理，每次只处理一个小块，从而减少内存消耗。\n2. 避免不必要的内存分配 尽量避免频繁的内存分配和释放操作，可以复用已经分配的内存缓冲区。\n3. 使用内存映射文件（mmap） 内存映射文件可以将文件的一部分映射到内存中，从而减少内存拷贝的开销，提高读写性能。\n优化后的代码实现 以下是优化后的代码实现，使用分块处理来减少内存消耗：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;stdint.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #define ALIGN_SIZE 4096 // 模拟 aligned_read 和 aligned_write API int aligned_read(int fd, void *buf, uint64_t pos, uint64_t len) { // 模拟读取操作 lseek(fd, pos, SEEK_SET); return read(fd, buf, len); } int aligned_write(int fd, void *buf, uint64_t pos, uint64_t len) { // 模拟写入操作 lseek(fd, pos, SEEK_SET); return write(fd, buf, len); } // 实现 read 接口 int read(int fd, void *buf, uint64_t pos, uint64_t len) { uint64_t aligned_pos = pos \u0026amp; ~(ALIGN_SIZE - 1); uint64_t end_pos = pos + len; uint64_t aligned_end_pos = (end_pos + ALIGN_SIZE - 1) \u0026amp; ~(ALIGN_SIZE - 1); uint64_t aligned_len = aligned_end_pos - aligned_pos; void *aligned_buf = aligned_alloc(ALIGN_SIZE, ALIGN_SIZE); if (!aligned_buf) { perror(\u0026#34;aligned_alloc\u0026#34;); return -1; } uint64_t bytes_read = 0; while (bytes_read \u0026lt; len) { uint64_t chunk_pos = aligned_pos + bytes_read; uint64_t chunk_len = ALIGN_SIZE; if (chunk_pos + chunk_len \u0026gt; end_pos) { chunk_len = end_pos - chunk_pos; } // 读取对齐的范围 int ret = aligned_read(fd, aligned_buf, chunk_pos, chunk_len); if (ret \u0026lt; 0) { free(aligned_buf); return ret; } // 复制所需的数据到目标缓冲区 uint64_t copy_start = (chunk_pos \u0026lt; pos) ? (pos - chunk_pos) : 0; uint64_t copy_len = (chunk_len - copy_start \u0026gt; len - bytes_read) ? (len - bytes_read) : (chunk_len - copy_start); memcpy((char *)buf + bytes_read, (char *)aligned_buf + copy_start, copy_len); bytes_read += copy_len; } free(aligned_buf); return bytes_read; } // 实现 write 接口 int write(int fd, void *buf, uint64_t pos, uint64_t len) { uint64_t aligned_pos = pos \u0026amp; ~(ALIGN_SIZE - 1); uint64_t end_pos = pos + len; uint64_t aligned_end_pos = (end_pos + ALIGN_SIZE - 1) \u0026amp; ~(ALIGN_SIZE - 1); uint64_t aligned_len = aligned_end_pos - aligned_pos; void *aligned_buf = aligned_alloc(ALIGN_SIZE, ALIGN_SIZE); if (!aligned_buf) { perror(\u0026#34;aligned_alloc\u0026#34;); return -1; } uint64_t bytes_written = 0; while (bytes_written \u0026lt; len) { uint64_t chunk_pos = aligned_pos + bytes_written; uint64_t chunk_len = ALIGN_SIZE; if (chunk_pos + chunk_len \u0026gt; end_pos) { chunk_len = end_pos - chunk_pos; } // 读取对齐的范围 int ret = aligned_read(fd, aligned_buf, chunk_pos, chunk_len); if (ret \u0026lt; 0) { free(aligned_buf); return ret; } // 复制新的数据到对齐缓冲区 uint64_t copy_start = (chunk_pos \u0026lt; pos) ? (pos - chunk_pos) : 0; uint64_t copy_len = (chunk_len - copy_start \u0026gt; len - bytes_written) ? (len - bytes_written) : (chunk_len - copy_start); memcpy((char *)aligned_buf + copy_start, (char *)buf + bytes_written, copy_len); // 写入对齐的范围 ret = aligned_write(fd, aligned_buf, chunk_pos, chunk_len); if (ret \u0026lt; 0) { free(aligned_buf); return ret; } bytes_written += copy_len; } free(aligned_buf); return bytes_written; } // 测试函数 int main() { // 示例文件操作 int fd = open(\u0026#34;testfile.txt\u0026#34;, O_RDWR | O_CREAT, 0644); if (fd \u0026lt; 0) { perror(\u0026#34;open\u0026#34;); return 1; } char write_buf[] = \u0026#34;Hello, this is a test for aligned write and read.\u0026#34;; char read_buf[sizeof(write_buf)]; // 写入数据 if (write(fd, write_buf, 0, sizeof(write_buf)) \u0026lt; 0) { perror(\u0026#34;write\u0026#34;); close(fd); return 1; } // 读取数据 if (read(fd, read_buf, 0, sizeof(read_buf)) \u0026lt; 0) { perror(\u0026#34;read\u0026#34;); close(fd); return 1; } printf(\u0026#34;读取的数据: %s\\n\u0026#34;, read_buf); close(fd); return 0; } 详细注释 aligned_read 和 aligned_write 模拟: 这两个函数模拟了对齐的读取和写入操作，实际实现中可能需要根据具体需求进行调整。 read 接口: 对齐计算: 计算对齐的起始位置 aligned_pos 和结束位置 aligned_end_pos。 分配对齐缓冲区: 使用 aligned_alloc 分配对齐的缓冲区，每次只分配 4KB。 分块读取: 使用 while 循环分块读取数据，每次读取 4KB。 调用 aligned_read 读取对齐范围的数据。 复制所需的数据到目标缓冲区。 释放缓冲区: 释放分配的对齐缓冲区。 write 接口: 对齐计算: 计算对齐的起始位置 aligned_pos 和结束位置 aligned_end_pos。 分配对齐缓冲区: 使用 aligned_alloc 分配对齐的缓冲区，每次只分配 4KB。 分块写入: 使用 while 循环分块写入数据，每次写入 4KB。 调用 aligned_read 读取对齐范围的数据。 将新的数据复制到对齐缓冲区的相应位置。 调用 aligned_write 写入对齐范围的数据。 释放缓冲区: 释放分配的对齐缓冲区。 通过这些优化，我们可以在内存有限的情况下高效地进行读取和写入操作。分块处理减少了内存消耗，避免了频繁的内存分配和释放操作，从而提高了性能。\n","date":"2025-02-26T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/moushoujigongsimianshitirw/","title":"某手机公司面试题：实现 read/write 接口"},{"content":"题目链接： https://leetcode.cn/problems/group-anagrams/solutions/520469/zi-mu-yi-wei-ci-fen-zu-by-leetcode-solut-gyoc/\n题解中定义了array\u0026lt;int, 26\u0026gt;到 vector\u0026lt;string\u0026gt;的映射，其中前者为一个大小为26的int数组。C++中，除了vector这种变长数组，还提供了固定大小数组的std::array这个容器类模板。用意是更好的类型检查和防止越界错误，还可以使用标准库的很多算法函数。性能和原生数组相同。使用前需要#include \u0026lt;array\u0026gt;。\n之所以要定义这个映射，是因为互为字母异位词的两个字符串，他们每个字母出现的频率是相同的。所以用array\u0026lt;int, 26\u0026gt;作为key，将字母异位词分组。而key相同的字符串需要插入映射的vector\u0026lt;string\u0026gt;中。\nunordered_map和unordered_set等基于哈希表的容器默认只能处理内置类型（如整数、指针）和一些标准库类型（string等）。对于自定义类型或复杂类型，如array\u0026lt;int, 26\u0026gt;，需要提供一个合适的哈希函数才能作为key。\n哈希函数的目的是区分不同的key。合适的哈希函数需要将相同的key映射到相同的slot，而哈希碰撞的概率很小。在实现哈希函数时，注意：\n函数返回值为size_t类型，这是因为unordered_map是基于哈希表的数据结构，索引是基于哈希值的，而哈希值就是一个无符号整数。不用int是为了避免符号扩展的问题。具体需要参考unordered_map的实现。 一般的设计方法有 利用std::hash\u0026lt;int\u0026gt;函数，需要#include \u0026lt;functional\u0026gt; 位运算：如位移、异或等操作可以帮助混合不同的比特位。 乘法和加法：通过乘以素数并加上其他数值，可以使结果更加随机化。 组合多个哈希值：如果输入数据由多个字段组成，可以分别对每个字段进行哈希计算，然后将这些哈希值组合起来。 总之，自定义哈希函数就是要保证输入相同的key时，返回值相同；输入不同的key时，返回值尽量不同。 本题中，key是一个整数数组。因此可以组合数组中每一位整数的哈希值。\n1 2 3 4 5 6 7 8 size_t my_hash(const std::array\u0026lt;int, 26\u0026gt; \u0026amp;k) { int n; size_t res = 0; for (n : k) { hash = (hash \u0026lt;\u0026lt; 1) ^ std::hash\u0026lt;int\u0026gt;{}(n); } return hash; } 至于哈希函数std::hash\u0026lt;int\u0026gt;{}()中为什么有{}，这是因为std::hash是一个类模板，()是他定义的函数运算操作符。std::hash\u0026lt;int\u0026gt;{}的意思是临时创建一个std::hash\u0026lt;int\u0026gt;类型的匿名临时对象，{}是参数列表（不需要参数）。这个对象后面再用()，就是调用了该临时对象定义的操作符。\n使用左移和异或就是想要哈希函数映射得更均匀，尽量保留每一位的特征。\n题解使用了嵌套的Lambda表达式，太复杂。还可以使用函数对象（仿函数）实现。函数对象是一个具有operator()的类实例。不仅可以像函数一样调用，还可以保存状态。此处不介绍。\n","date":"2025-02-13T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/self_define_hash/","title":"以力扣49为例 - 自定义哈希函数"},{"content":"YCSB 是一个评估数据库性能的测试框架，可以测试不同数据库在不同负载的表现。\n搭建自定义负载 一个工作负载定义了加载到数据库中的数据集（data set）和操作数据库的事务集（transaction set）。\n一个工作负载包括 2 个文件：\n一个用于生成数据记录和事务操作的 JAVA 类 一个用于调整负载特性（如读写比例）的参数文件 YCSB 默认只能使用随机生成的数据集，为了使用自定义数据集，可以自己实现一个 workload。下面是根据 CoreWorkload 修改的 MyWorkload. java。将其放在 YCSB/core/src/main/java/site/ycsb/workloads/目录下一起编译。\n修改内容主要是：将原来随机生成 value 的部分改为从文件读取指定长度的字符串。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 /** * Copyright (c) 2010 Yahoo! Inc., Copyright (c) 2016-2020 YCSB contributors. All rights reserved. * \u0026lt;p\u0026gt; * Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you * may not use this file except in compliance with the License. You * may obtain a copy of the License at * \u0026lt;p\u0026gt; * http://www.apache.org/licenses/LICENSE-2.0 * \u0026lt;p\u0026gt; * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or * implied. See the License for the specific language governing * permissions and limitations under the License. See accompanying * LICENSE file. */ package site.ycsb.workloads; import site.ycsb.*; import site.ycsb.generator.*; import site.ycsb.generator.UniformLongGenerator; import site.ycsb.measurements.Measurements; import java.io.IOException; // import java.nio.file.Files; // import java.nio.file.Paths; import java.util.*; /** * The core benchmark scenario. Represents a set of clients doing simple CRUD operations. The * relative proportion of different kinds of operations, and other properties of the workload, * are controlled by parameters specified at runtime. * \u0026lt;p\u0026gt; * Properties to control the client: * \u0026lt;UL\u0026gt; * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;fieldcount\u0026lt;/b\u0026gt;: the number of fields in a record (default: 10) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;fieldlength\u0026lt;/b\u0026gt;: the size of each field (default: 100) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;minfieldlength\u0026lt;/b\u0026gt;: the minimum size of each field (default: 1) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;readallfields\u0026lt;/b\u0026gt;: should reads read all fields (true) or just one (false) (default: true) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;writeallfields\u0026lt;/b\u0026gt;: should updates and read/modify/writes update all fields (true) or just * one (false) (default: false) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;readproportion\u0026lt;/b\u0026gt;: what proportion of operations should be reads (default: 0.95) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;updateproportion\u0026lt;/b\u0026gt;: what proportion of operations should be updates (default: 0.05) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;insertproportion\u0026lt;/b\u0026gt;: what proportion of operations should be inserts (default: 0) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;scanproportion\u0026lt;/b\u0026gt;: what proportion of operations should be scans (default: 0) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;readmodifywriteproportion\u0026lt;/b\u0026gt;: what proportion of operations should be read a record, * modify it, write it back (default: 0) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;requestdistribution\u0026lt;/b\u0026gt;: what distribution should be used to select the records to operate * on - uniform, zipfian, hotspot, sequential, exponential or latest (default: uniform) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;minscanlength\u0026lt;/b\u0026gt;: for scans, what is the minimum number of records to scan (default: 1) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;maxscanlength\u0026lt;/b\u0026gt;: for scans, what is the maximum number of records to scan (default: 1000) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;scanlengthdistribution\u0026lt;/b\u0026gt;: for scans, what distribution should be used to choose the * number of records to scan, for each scan, between 1 and maxscanlength (default: uniform) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;insertstart\u0026lt;/b\u0026gt;: for parallel loads and runs, defines the starting record for this * YCSB instance (default: 0) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;insertcount\u0026lt;/b\u0026gt;: for parallel loads and runs, defines the number of records for this * YCSB instance (default: recordcount) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;zeropadding\u0026lt;/b\u0026gt;: for generating a record sequence compatible with string sort order by * 0 padding the record number. Controls the number of 0s to use for padding. (default: 1) * For example for row 5, with zeropadding=1 you get \u0026#39;user5\u0026#39; key and with zeropading=8 you get * \u0026#39;user00000005\u0026#39; key. In order to see its impact, zeropadding needs to be bigger than number of * digits in the record number. * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;insertorder\u0026lt;/b\u0026gt;: should records be inserted in order by key (\u0026#34;ordered\u0026#34;), or in hashed * order (\u0026#34;hashed\u0026#34;) (default: hashed) * \u0026lt;LI\u0026gt;\u0026lt;b\u0026gt;fieldnameprefix\u0026lt;/b\u0026gt;: what should be a prefix for field names, the shorter may decrease the * required storage size (default: \u0026#34;field\u0026#34;) * \u0026lt;/ul\u0026gt; */ public class MyWorkload extends Workload { /** * The name of the database table to run queries against. */ public static final String TABLENAME_PROPERTY = \u0026#34;table\u0026#34;; /** * The default name of the database table to run queries against. */ public static final String TABLENAME_PROPERTY_DEFAULT = \u0026#34;usertable\u0026#34;; protected String table; /** * The name of the property for the number of fields in a record. */ public static final String FIELD_COUNT_PROPERTY = \u0026#34;fieldcount\u0026#34;; /** * Default number of fields in a record. */ public static final String FIELD_COUNT_PROPERTY_DEFAULT = \u0026#34;10\u0026#34;; private List\u0026lt;String\u0026gt; fieldnames; /** * The name of the property for the field length distribution. Options are \u0026#34;uniform\u0026#34;, \u0026#34;zipfian\u0026#34; * (favouring short records), \u0026#34;constant\u0026#34;, and \u0026#34;histogram\u0026#34;. * \u0026lt;p\u0026gt; * If \u0026#34;uniform\u0026#34;, \u0026#34;zipfian\u0026#34; or \u0026#34;constant\u0026#34;, the maximum field length will be that specified by the * fieldlength property. If \u0026#34;histogram\u0026#34;, then the histogram will be read from the filename * specified in the \u0026#34;fieldlengthhistogram\u0026#34; property. */ public static final String FIELD_LENGTH_DISTRIBUTION_PROPERTY = \u0026#34;fieldlengthdistribution\u0026#34;; /** * The default field length distribution. */ public static final String FIELD_LENGTH_DISTRIBUTION_PROPERTY_DEFAULT = \u0026#34;constant\u0026#34;; /** * The name of the property for the length of a field in bytes. */ public static final String FIELD_LENGTH_PROPERTY = \u0026#34;fieldlength\u0026#34;; /** * The default maximum length of a field in bytes. */ public static final String FIELD_LENGTH_PROPERTY_DEFAULT = \u0026#34;100\u0026#34;; /** * The name of the property for the minimum length of a field in bytes. */ public static final String MIN_FIELD_LENGTH_PROPERTY = \u0026#34;minfieldlength\u0026#34;; /** * The default minimum length of a field in bytes. */ public static final String MIN_FIELD_LENGTH_PROPERTY_DEFAULT = \u0026#34;1\u0026#34;; /** * The name of a property that specifies the filename containing the field length histogram (only * used if fieldlengthdistribution is \u0026#34;histogram\u0026#34;). */ public static final String FIELD_LENGTH_HISTOGRAM_FILE_PROPERTY = \u0026#34;fieldlengthhistogram\u0026#34;; /** * The default filename containing a field length histogram. */ public static final String FIELD_LENGTH_HISTOGRAM_FILE_PROPERTY_DEFAULT = \u0026#34;hist.txt\u0026#34;; /** * Generator object that produces field lengths. The value of this depends on the properties that * start with \u0026#34;FIELD_LENGTH_\u0026#34;. */ protected NumberGenerator fieldlengthgenerator; /** * The name of the property for deciding whether to read one field (false) or all fields (true) of * a record. */ public static final String READ_ALL_FIELDS_PROPERTY = \u0026#34;readallfields\u0026#34;; /** * The default value for the readallfields property. */ public static final String READ_ALL_FIELDS_PROPERTY_DEFAULT = \u0026#34;true\u0026#34;; protected boolean readallfields; /** * The name of the property for determining how to read all the fields when readallfields is true. * If set to true, all the field names will be passed into the underlying client. If set to false, * null will be passed into the underlying client. When passed a null, some clients may retrieve * the entire row with a wildcard, which may be slower than naming all the fields. */ public static final String READ_ALL_FIELDS_BY_NAME_PROPERTY = \u0026#34;readallfieldsbyname\u0026#34;; /** * The default value for the readallfieldsbyname property. */ public static final String READ_ALL_FIELDS_BY_NAME_PROPERTY_DEFAULT = \u0026#34;false\u0026#34;; protected boolean readallfieldsbyname; /** * The name of the property for deciding whether to write one field (false) or all fields (true) * of a record. */ public static final String WRITE_ALL_FIELDS_PROPERTY = \u0026#34;writeallfields\u0026#34;; /** * The default value for the writeallfields property. */ public static final String WRITE_ALL_FIELDS_PROPERTY_DEFAULT = \u0026#34;false\u0026#34;; protected boolean writeallfields; /** * The name of the property for deciding whether to check all returned * data against the formation template to ensure data integrity. */ public static final String DATA_INTEGRITY_PROPERTY = \u0026#34;dataintegrity\u0026#34;; /** * The default value for the dataintegrity property. */ public static final String DATA_INTEGRITY_PROPERTY_DEFAULT = \u0026#34;false\u0026#34;; /** * Set to true if want to check correctness of reads. Must also * be set to true during loading phase to function. */ private boolean dataintegrity; /** * The name of the property for the proportion of transactions that are reads. */ public static final String READ_PROPORTION_PROPERTY = \u0026#34;readproportion\u0026#34;; /** * The default proportion of transactions that are reads. */ public static final String READ_PROPORTION_PROPERTY_DEFAULT = \u0026#34;0.95\u0026#34;; /** * The name of the property for the proportion of transactions that are updates. */ public static final String UPDATE_PROPORTION_PROPERTY = \u0026#34;updateproportion\u0026#34;; /** * The default proportion of transactions that are updates. */ public static final String UPDATE_PROPORTION_PROPERTY_DEFAULT = \u0026#34;0.05\u0026#34;; /** * The name of the property for the proportion of transactions that are inserts. */ public static final String INSERT_PROPORTION_PROPERTY = \u0026#34;insertproportion\u0026#34;; /** * The default proportion of transactions that are inserts. */ public static final String INSERT_PROPORTION_PROPERTY_DEFAULT = \u0026#34;0.0\u0026#34;; /** * The name of the property for the proportion of transactions that are scans. */ public static final String SCAN_PROPORTION_PROPERTY = \u0026#34;scanproportion\u0026#34;; /** * The default proportion of transactions that are scans. */ public static final String SCAN_PROPORTION_PROPERTY_DEFAULT = \u0026#34;0.0\u0026#34;; /** * The name of the property for the proportion of transactions that are read-modify-write. */ public static final String READMODIFYWRITE_PROPORTION_PROPERTY = \u0026#34;readmodifywriteproportion\u0026#34;; /** * The default proportion of transactions that are scans. */ public static final String READMODIFYWRITE_PROPORTION_PROPERTY_DEFAULT = \u0026#34;0.0\u0026#34;; /** * The name of the property for the the distribution of requests across the keyspace. Options are * \u0026#34;uniform\u0026#34;, \u0026#34;zipfian\u0026#34; and \u0026#34;latest\u0026#34; */ public static final String REQUEST_DISTRIBUTION_PROPERTY = \u0026#34;requestdistribution\u0026#34;; /** * The default distribution of requests across the keyspace. */ public static final String REQUEST_DISTRIBUTION_PROPERTY_DEFAULT = \u0026#34;uniform\u0026#34;; /** * The name of the property for adding zero padding to record numbers in order to match * string sort order. Controls the number of 0s to left pad with. */ public static final String ZERO_PADDING_PROPERTY = \u0026#34;zeropadding\u0026#34;; /** * The default zero padding value. Matches integer sort order */ public static final String ZERO_PADDING_PROPERTY_DEFAULT = \u0026#34;1\u0026#34;; /** * The name of the property for the min scan length (number of records). */ public static final String MIN_SCAN_LENGTH_PROPERTY = \u0026#34;minscanlength\u0026#34;; /** * The default min scan length. */ public static final String MIN_SCAN_LENGTH_PROPERTY_DEFAULT = \u0026#34;1\u0026#34;; /** * The name of the property for the max scan length (number of records). */ public static final String MAX_SCAN_LENGTH_PROPERTY = \u0026#34;maxscanlength\u0026#34;; /** * The default max scan length. */ public static final String MAX_SCAN_LENGTH_PROPERTY_DEFAULT = \u0026#34;1000\u0026#34;; /** * The name of the property for the scan length distribution. Options are \u0026#34;uniform\u0026#34; and \u0026#34;zipfian\u0026#34; * (favoring short scans) */ public static final String SCAN_LENGTH_DISTRIBUTION_PROPERTY = \u0026#34;scanlengthdistribution\u0026#34;; /** * The default max scan length. */ public static final String SCAN_LENGTH_DISTRIBUTION_PROPERTY_DEFAULT = \u0026#34;uniform\u0026#34;; /** * The name of the property for the order to insert records. Options are \u0026#34;ordered\u0026#34; or \u0026#34;hashed\u0026#34; */ public static final String INSERT_ORDER_PROPERTY = \u0026#34;insertorder\u0026#34;; /** * Default insert order. */ public static final String INSERT_ORDER_PROPERTY_DEFAULT = \u0026#34;hashed\u0026#34;; /** * Percentage data items that constitute the hot set. */ public static final String HOTSPOT_DATA_FRACTION = \u0026#34;hotspotdatafraction\u0026#34;; /** * Default value of the size of the hot set. */ public static final String HOTSPOT_DATA_FRACTION_DEFAULT = \u0026#34;0.2\u0026#34;; /** * Percentage operations that access the hot set. */ public static final String HOTSPOT_OPN_FRACTION = \u0026#34;hotspotopnfraction\u0026#34;; /** * Default value of the percentage operations accessing the hot set. */ public static final String HOTSPOT_OPN_FRACTION_DEFAULT = \u0026#34;0.8\u0026#34;; /** * How many times to retry when insertion of a single item to a DB fails. */ public static final String INSERTION_RETRY_LIMIT = \u0026#34;core_workload_insertion_retry_limit\u0026#34;; public static final String INSERTION_RETRY_LIMIT_DEFAULT = \u0026#34;0\u0026#34;; /** * On average, how long to wait between the retries, in seconds. */ public static final String INSERTION_RETRY_INTERVAL = \u0026#34;core_workload_insertion_retry_interval\u0026#34;; public static final String INSERTION_RETRY_INTERVAL_DEFAULT = \u0026#34;3\u0026#34;; /** * Field name prefix. */ public static final String FIELD_NAME_PREFIX = \u0026#34;fieldnameprefix\u0026#34;; /** * Default value of the field name prefix. */ public static final String FIELD_NAME_PREFIX_DEFAULT = \u0026#34;field\u0026#34;; private static final String DATA_FILE = \u0026#34;datafile\u0026#34;; private static final String DATA_FILE_DEFAULT = \u0026#34;/data/publicdata/wikipedia/wikipedia.txt\u0026#34;; private static String datapath; public static String getDataFile() { return datapath; } // private static final int DATA_LENGTH = 1024; // 每个数据块的大小为1KB // private static byte[] fileData; // 存储文件的所有字节 // private static int totalBytesRead; // 文件中实际读取到的总字节数 protected NumberGenerator keysequence; protected DiscreteGenerator operationchooser; protected NumberGenerator keychooser; protected NumberGenerator fieldchooser; protected AcknowledgedCounterGenerator transactioninsertkeysequence; protected NumberGenerator scanlength; protected boolean orderedinserts; protected long fieldcount; protected long recordcount; protected int zeropadding; protected int insertionRetryLimit; protected int insertionRetryInterval; protected FileGenerator fg; private Measurements measurements = Measurements.getMeasurements(); public static String buildKeyName(long keynum, int zeropadding, boolean orderedinserts) { if (!orderedinserts) { keynum = Utils.hash(keynum); } String value = Long.toString(keynum); int fill = zeropadding - value.length(); String prekey = \u0026#34;user\u0026#34;; for (int i = 0; i \u0026lt; fill; i++) { prekey += \u0026#39;0\u0026#39;; } return prekey + value; } protected static NumberGenerator getFieldLengthGenerator(Properties p) throws WorkloadException { NumberGenerator fieldlengthgenerator; String fieldlengthdistribution = p.getProperty( FIELD_LENGTH_DISTRIBUTION_PROPERTY, FIELD_LENGTH_DISTRIBUTION_PROPERTY_DEFAULT); int fieldlength = Integer.parseInt(p.getProperty(FIELD_LENGTH_PROPERTY, FIELD_LENGTH_PROPERTY_DEFAULT)); int minfieldlength = Integer.parseInt(p.getProperty(MIN_FIELD_LENGTH_PROPERTY, MIN_FIELD_LENGTH_PROPERTY_DEFAULT)); String fieldlengthhistogram = p.getProperty( FIELD_LENGTH_HISTOGRAM_FILE_PROPERTY, FIELD_LENGTH_HISTOGRAM_FILE_PROPERTY_DEFAULT); if (fieldlengthdistribution.compareTo(\u0026#34;constant\u0026#34;) == 0) { fieldlengthgenerator = new ConstantIntegerGenerator(fieldlength); } else if (fieldlengthdistribution.compareTo(\u0026#34;uniform\u0026#34;) == 0) { fieldlengthgenerator = new UniformLongGenerator(minfieldlength, fieldlength); } else if (fieldlengthdistribution.compareTo(\u0026#34;zipfian\u0026#34;) == 0) { fieldlengthgenerator = new ZipfianGenerator(minfieldlength, fieldlength); } else if (fieldlengthdistribution.compareTo(\u0026#34;histogram\u0026#34;) == 0) { try { fieldlengthgenerator = new HistogramGenerator(fieldlengthhistogram); } catch (IOException e) { throw new WorkloadException( \u0026#34;Couldn\u0026#39;t read field length histogram file: \u0026#34; + fieldlengthhistogram, e); } } else { throw new WorkloadException( \u0026#34;Unknown field length distribution \\\u0026#34;\u0026#34; + fieldlengthdistribution + \u0026#34;\\\u0026#34;\u0026#34;); } return fieldlengthgenerator; } // private static final ThreadLocal\u0026lt;byte[]\u0026gt; THREAD_LOCAL_FILE_DATA = ThreadLocal.withInitial(() -\u0026gt; { // try { // return Files.readAllBytes(Paths.get(datapath)); // } catch (IOException e) { // throw new RuntimeException(\u0026#34;Failed to read data from file: \u0026#34; + datapath, e); // } // }); // private static final ThreadLocal\u0026lt;Integer\u0026gt; THREAD_LOCAL_TOTAL_BYTES_READ = // ThreadLocal.withInitial(() -\u0026gt; THREAD_LOCAL_FILE_DATA.get().length); // public static byte[] getThreadLocalFileData() { // return THREAD_LOCAL_FILE_DATA.get(); // } // public static int getThreadLocalTotalBytesRead() { // return THREAD_LOCAL_TOTAL_BYTES_READ.get(); // } /** * Initialize the scenario. * Called once, in the main client thread, before any operations are started. */ @Override public void init(Properties p) throws WorkloadException { table = p.getProperty(TABLENAME_PROPERTY, TABLENAME_PROPERTY_DEFAULT); datapath = p.getProperty(DATA_FILE, DATA_FILE_DEFAULT); // try { // // 在类加载时初始化并读取文件内容到内存中 // fileData = Files.readAllBytes(Paths.get(datapath)); // totalBytesRead = fileData.length; // } catch (IOException e) { // throw new RuntimeException(\u0026#34;Failed to read data from file: \u0026#34; + datapath, e); // } // fileData = getThreadLocalFileData(); // totalBytesRead = getThreadLocalTotalBytesRead(); fieldcount = Long.parseLong(p.getProperty(FIELD_COUNT_PROPERTY, FIELD_COUNT_PROPERTY_DEFAULT)); final String fieldnameprefix = p.getProperty(FIELD_NAME_PREFIX, FIELD_NAME_PREFIX_DEFAULT); fieldnames = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; fieldcount; i++) { fieldnames.add(fieldnameprefix + i); } fieldlengthgenerator = MyWorkload.getFieldLengthGenerator(p); recordcount = Long.parseLong(p.getProperty(Client.RECORD_COUNT_PROPERTY, Client.DEFAULT_RECORD_COUNT)); if (recordcount == 0) { recordcount = Integer.MAX_VALUE; } String requestdistrib = p.getProperty(REQUEST_DISTRIBUTION_PROPERTY, REQUEST_DISTRIBUTION_PROPERTY_DEFAULT); int minscanlength = Integer.parseInt(p.getProperty(MIN_SCAN_LENGTH_PROPERTY, MIN_SCAN_LENGTH_PROPERTY_DEFAULT)); int maxscanlength = Integer.parseInt(p.getProperty(MAX_SCAN_LENGTH_PROPERTY, MAX_SCAN_LENGTH_PROPERTY_DEFAULT)); String scanlengthdistrib = p.getProperty(SCAN_LENGTH_DISTRIBUTION_PROPERTY, SCAN_LENGTH_DISTRIBUTION_PROPERTY_DEFAULT); long insertstart = Long.parseLong(p.getProperty(INSERT_START_PROPERTY, INSERT_START_PROPERTY_DEFAULT)); long insertcount= Integer.parseInt(p.getProperty(INSERT_COUNT_PROPERTY, String.valueOf(recordcount - insertstart))); // Confirm valid values for insertstart and insertcount in relation to recordcount if (recordcount \u0026lt; (insertstart + insertcount)) { System.err.println(\u0026#34;Invalid combination of insertstart, insertcount and recordcount.\u0026#34;); System.err.println(\u0026#34;recordcount must be bigger than insertstart + insertcount.\u0026#34;); System.exit(-1); } zeropadding = Integer.parseInt(p.getProperty(ZERO_PADDING_PROPERTY, ZERO_PADDING_PROPERTY_DEFAULT)); readallfields = Boolean.parseBoolean( p.getProperty(READ_ALL_FIELDS_PROPERTY, READ_ALL_FIELDS_PROPERTY_DEFAULT)); readallfieldsbyname = Boolean.parseBoolean( p.getProperty(READ_ALL_FIELDS_BY_NAME_PROPERTY, READ_ALL_FIELDS_BY_NAME_PROPERTY_DEFAULT)); writeallfields = Boolean.parseBoolean( p.getProperty(WRITE_ALL_FIELDS_PROPERTY, WRITE_ALL_FIELDS_PROPERTY_DEFAULT)); dataintegrity = Boolean.parseBoolean( p.getProperty(DATA_INTEGRITY_PROPERTY, DATA_INTEGRITY_PROPERTY_DEFAULT)); // Confirm that fieldlengthgenerator returns a constant if data // integrity check requested. if (dataintegrity \u0026amp;\u0026amp; !(p.getProperty( FIELD_LENGTH_DISTRIBUTION_PROPERTY, FIELD_LENGTH_DISTRIBUTION_PROPERTY_DEFAULT)).equals(\u0026#34;constant\u0026#34;)) { System.err.println(\u0026#34;Must have constant field size to check data integrity.\u0026#34;); System.exit(-1); } if (dataintegrity) { System.out.println(\u0026#34;Data integrity is enabled.\u0026#34;); } if (p.getProperty(INSERT_ORDER_PROPERTY, INSERT_ORDER_PROPERTY_DEFAULT).compareTo(\u0026#34;hashed\u0026#34;) == 0) { orderedinserts = false; } else { orderedinserts = true; } keysequence = new CounterGenerator(insertstart); operationchooser = createOperationGenerator(p); transactioninsertkeysequence = new AcknowledgedCounterGenerator(recordcount); if (requestdistrib.compareTo(\u0026#34;uniform\u0026#34;) == 0) { keychooser = new UniformLongGenerator(insertstart, insertstart + insertcount - 1); } else if (requestdistrib.compareTo(\u0026#34;exponential\u0026#34;) == 0) { double percentile = Double.parseDouble(p.getProperty( ExponentialGenerator.EXPONENTIAL_PERCENTILE_PROPERTY, ExponentialGenerator.EXPONENTIAL_PERCENTILE_DEFAULT)); double frac = Double.parseDouble(p.getProperty( ExponentialGenerator.EXPONENTIAL_FRAC_PROPERTY, ExponentialGenerator.EXPONENTIAL_FRAC_DEFAULT)); keychooser = new ExponentialGenerator(percentile, recordcount * frac); } else if (requestdistrib.compareTo(\u0026#34;sequential\u0026#34;) == 0) { keychooser = new SequentialGenerator(insertstart, insertstart + insertcount - 1); } else if (requestdistrib.compareTo(\u0026#34;zipfian\u0026#34;) == 0) { // it does this by generating a random \u0026#34;next key\u0026#34; in part by taking the modulus over the // number of keys. // If the number of keys changes, this would shift the modulus, and we don\u0026#39;t want that to // change which keys are popular so we\u0026#39;ll actually construct the scrambled zipfian generator // with a keyspace that is larger than exists at the beginning of the test. that is, we\u0026#39;ll predict // the number of inserts, and tell the scrambled zipfian generator the number of existing keys // plus the number of predicted keys as the total keyspace. then, if the generator picks a key // that hasn\u0026#39;t been inserted yet, will just ignore it and pick another key. this way, the size of // the keyspace doesn\u0026#39;t change from the perspective of the scrambled zipfian generator final double insertproportion = Double.parseDouble( p.getProperty(INSERT_PROPORTION_PROPERTY, INSERT_PROPORTION_PROPERTY_DEFAULT)); int opcount = Integer.parseInt(p.getProperty(Client.OPERATION_COUNT_PROPERTY)); int expectednewkeys = (int) ((opcount) * insertproportion * 2.0); // 2 is fudge factor keychooser = new ScrambledZipfianGenerator(insertstart, insertstart + insertcount + expectednewkeys); } else if (requestdistrib.compareTo(\u0026#34;latest\u0026#34;) == 0) { keychooser = new SkewedLatestGenerator(transactioninsertkeysequence); } else if (requestdistrib.equals(\u0026#34;hotspot\u0026#34;)) { double hotsetfraction = Double.parseDouble(p.getProperty(HOTSPOT_DATA_FRACTION, HOTSPOT_DATA_FRACTION_DEFAULT)); double hotopnfraction = Double.parseDouble(p.getProperty(HOTSPOT_OPN_FRACTION, HOTSPOT_OPN_FRACTION_DEFAULT)); keychooser = new HotspotIntegerGenerator(insertstart, insertstart + insertcount - 1, hotsetfraction, hotopnfraction); } else { throw new WorkloadException(\u0026#34;Unknown request distribution \\\u0026#34;\u0026#34; + requestdistrib + \u0026#34;\\\u0026#34;\u0026#34;); } fieldchooser = new UniformLongGenerator(0, fieldcount - 1); if (scanlengthdistrib.compareTo(\u0026#34;uniform\u0026#34;) == 0) { scanlength = new UniformLongGenerator(minscanlength, maxscanlength); } else if (scanlengthdistrib.compareTo(\u0026#34;zipfian\u0026#34;) == 0) { scanlength = new ZipfianGenerator(minscanlength, maxscanlength); } else { throw new WorkloadException( \u0026#34;Distribution \\\u0026#34;\u0026#34; + scanlengthdistrib + \u0026#34;\\\u0026#34; not allowed for scan length\u0026#34;); } fg = new FileGenerator(datapath); insertionRetryLimit = Integer.parseInt(p.getProperty( INSERTION_RETRY_LIMIT, INSERTION_RETRY_LIMIT_DEFAULT)); insertionRetryInterval = Integer.parseInt(p.getProperty( INSERTION_RETRY_INTERVAL, INSERTION_RETRY_INTERVAL_DEFAULT)); } /** * Builds a value for a randomly chosen field. */ private HashMap\u0026lt;String, ByteIterator\u0026gt; buildSingleValue(String key) { HashMap\u0026lt;String, ByteIterator\u0026gt; value = new HashMap\u0026lt;\u0026gt;(); String fieldkey = fieldnames.get(fieldchooser.nextValue().intValue()); ByteIterator data; if (dataintegrity) { data = new StringByteIterator(buildDeterministicValue(key, fieldkey)); } else { // byte[] dataBlock = getNextDataBlock(0); // data = new ByteArrayByteIterator(dataBlock); // data = new RandomByteIterator(fieldlengthgenerator.nextValue().longValue()); data = new StringByteIterator(fg.nextValue()); } value.put(fieldkey, data); return value; } /** * Builds values for all fields. */ private HashMap\u0026lt;String, ByteIterator\u0026gt; buildValues(String key) { HashMap\u0026lt;String, ByteIterator\u0026gt; values = new HashMap\u0026lt;\u0026gt;(); int offset = 0; for (String fieldkey : fieldnames) { ByteIterator data; if (dataintegrity) { data = new StringByteIterator(buildDeterministicValue(key, fieldkey)); } else { // fill with random data // data = new RandomByteIterator(fieldlengthgenerator.nextValue().longValue()); // int len = fieldlengthgenerator.nextValue().longValue(); // byte[] dataBlock = getNextDataBlock(offset); // offset += DATA_LENGTH; // if (offset \u0026gt;= totalBytesRead) { // offset = 0; // } data = new StringByteIterator(fg.nextValue()); } values.put(fieldkey, data); } return values; } // private byte[] getNextDataBlock(int offset) { // byte[] dataBlock = new byte[DATA_LENGTH]; // // 如果偏移量在文件范围内 // if (offset \u0026lt; totalBytesRead) { // int remainingBytes = totalBytesRead - offset; // // 如果文件剩余数据大于或等于一个数据块，直接复制数据 // if (remainingBytes \u0026gt;= DATA_LENGTH) { // System.arraycopy(fileData, offset, dataBlock, 0, DATA_LENGTH); // } else { // // 如果文件剩余数据小于一个数据块，则重复最后一个完整的数据块填充 // int lastCompleteOffset = (totalBytesRead / DATA_LENGTH) * DATA_LENGTH; // for (int i = 0; i \u0026lt; DATA_LENGTH; i++) { // dataBlock[i] = fileData[(lastCompleteOffset + i) % totalBytesRead]; // } // } // } else { // // 如果超出文件末尾，回绕到文件开头 // System.arraycopy(fileData, 0, dataBlock, 0, DATA_LENGTH); // } // return dataBlock; // } /** * Build a deterministic value given the key information. */ private String buildDeterministicValue(String key, String fieldkey) { int size = fieldlengthgenerator.nextValue().intValue(); StringBuilder sb = new StringBuilder(size); sb.append(key); sb.append(\u0026#39;:\u0026#39;); sb.append(fieldkey); while (sb.length() \u0026lt; size) { sb.append(\u0026#39;:\u0026#39;); sb.append(sb.toString().hashCode()); } sb.setLength(size); return sb.toString(); } /** * Do one insert operation. Because it will be called concurrently from multiple client threads, * this function must be thread safe. However, avoid synchronized, or the threads will block waiting * for each other, and it will be difficult to reach the target throughput. Ideally, this function would * have no side effects other than DB operations. */ @Override public boolean doInsert(DB db, Object threadstate) { int keynum = keysequence.nextValue().intValue(); String dbkey = MyWorkload.buildKeyName(keynum, zeropadding, orderedinserts); HashMap\u0026lt;String, ByteIterator\u0026gt; values = buildValues(dbkey); Status status; int numOfRetries = 0; do { status = db.insert(table, dbkey, values); if (null != status \u0026amp;\u0026amp; status.isOk()) { break; } // Retry if configured. Without retrying, the load process will fail // even if one single insertion fails. User can optionally configure // an insertion retry limit (default is 0) to enable retry. if (++numOfRetries \u0026lt;= insertionRetryLimit) { System.err.println(\u0026#34;Retrying insertion, retry count: \u0026#34; + numOfRetries); try { // Sleep for a random number between [0.8, 1.2)*insertionRetryInterval. int sleepTime = (int) (1000 * insertionRetryInterval * (0.8 + 0.4 * Math.random())); Thread.sleep(sleepTime); } catch (InterruptedException e) { break; } } else { System.err.println(\u0026#34;Error inserting, not retrying any more. number of attempts: \u0026#34; + numOfRetries + \u0026#34;Insertion Retry Limit: \u0026#34; + insertionRetryLimit); break; } } while (true); return null != status \u0026amp;\u0026amp; status.isOk(); } /** * Do one transaction operation. Because it will be called concurrently from multiple client * threads, this function must be thread safe. However, avoid synchronized, or the threads will block waiting * for each other, and it will be difficult to reach the target throughput. Ideally, this function would * have no side effects other than DB operations. */ @Override public boolean doTransaction(DB db, Object threadstate) { String operation = operationchooser.nextString(); if(operation == null) { return false; } switch (operation) { case \u0026#34;READ\u0026#34;: doTransactionRead(db); break; case \u0026#34;UPDATE\u0026#34;: doTransactionUpdate(db); break; case \u0026#34;INSERT\u0026#34;: doTransactionInsert(db); break; case \u0026#34;SCAN\u0026#34;: doTransactionScan(db); break; default: doTransactionReadModifyWrite(db); } return true; } /** * Results are reported in the first three buckets of the histogram under * the label \u0026#34;VERIFY\u0026#34;. * Bucket 0 means the expected data was returned. * Bucket 1 means incorrect data was returned. * Bucket 2 means null data was returned when some data was expected. */ protected void verifyRow(String key, HashMap\u0026lt;String, ByteIterator\u0026gt; cells) { Status verifyStatus = Status.OK; long startTime = System.nanoTime(); if (!cells.isEmpty()) { for (Map.Entry\u0026lt;String, ByteIterator\u0026gt; entry : cells.entrySet()) { if (!entry.getValue().toString().equals(buildDeterministicValue(key, entry.getKey()))) { verifyStatus = Status.UNEXPECTED_STATE; break; } } } else { // This assumes that null data is never valid verifyStatus = Status.ERROR; } long endTime = System.nanoTime(); measurements.measure(\u0026#34;VERIFY\u0026#34;, (int) (endTime - startTime) / 1000); measurements.reportStatus(\u0026#34;VERIFY\u0026#34;, verifyStatus); } long nextKeynum() { long keynum; if (keychooser instanceof ExponentialGenerator) { do { keynum = transactioninsertkeysequence.lastValue() - keychooser.nextValue().intValue(); } while (keynum \u0026lt; 0); } else { do { keynum = keychooser.nextValue().intValue(); } while (keynum \u0026gt; transactioninsertkeysequence.lastValue()); } return keynum; } public void doTransactionRead(DB db) { // choose a random key long keynum = nextKeynum(); String keyname = MyWorkload.buildKeyName(keynum, zeropadding, orderedinserts); HashSet\u0026lt;String\u0026gt; fields = null; if (!readallfields) { // read a random field String fieldname = fieldnames.get(fieldchooser.nextValue().intValue()); fields = new HashSet\u0026lt;String\u0026gt;(); fields.add(fieldname); } else if (dataintegrity || readallfieldsbyname) { // pass the full field list if dataintegrity is on for verification fields = new HashSet\u0026lt;String\u0026gt;(fieldnames); } HashMap\u0026lt;String, ByteIterator\u0026gt; cells = new HashMap\u0026lt;String, ByteIterator\u0026gt;(); db.read(table, keyname, fields, cells); if (dataintegrity) { verifyRow(keyname, cells); } } public void doTransactionReadModifyWrite(DB db) { // choose a random key long keynum = nextKeynum(); String keyname = MyWorkload.buildKeyName(keynum, zeropadding, orderedinserts); HashSet\u0026lt;String\u0026gt; fields = null; if (!readallfields) { // read a random field String fieldname = fieldnames.get(fieldchooser.nextValue().intValue()); fields = new HashSet\u0026lt;String\u0026gt;(); fields.add(fieldname); } HashMap\u0026lt;String, ByteIterator\u0026gt; values; if (writeallfields) { // new data for all the fields values = buildValues(keyname); } else { // update a random field values = buildSingleValue(keyname); } // do the transaction HashMap\u0026lt;String, ByteIterator\u0026gt; cells = new HashMap\u0026lt;String, ByteIterator\u0026gt;(); long ist = measurements.getIntendedStartTimeNs(); long st = System.nanoTime(); db.read(table, keyname, fields, cells); db.update(table, keyname, values); long en = System.nanoTime(); if (dataintegrity) { verifyRow(keyname, cells); } measurements.measure(\u0026#34;READ-MODIFY-WRITE\u0026#34;, (int) ((en - st) / 1000)); measurements.measureIntended(\u0026#34;READ-MODIFY-WRITE\u0026#34;, (int) ((en - ist) / 1000)); } public void doTransactionScan(DB db) { // choose a random key long keynum = nextKeynum(); String startkeyname = MyWorkload.buildKeyName(keynum, zeropadding, orderedinserts); // choose a random scan length int len = scanlength.nextValue().intValue(); HashSet\u0026lt;String\u0026gt; fields = null; if (!readallfields) { // read a random field String fieldname = fieldnames.get(fieldchooser.nextValue().intValue()); fields = new HashSet\u0026lt;String\u0026gt;(); fields.add(fieldname); } db.scan(table, startkeyname, len, fields, new Vector\u0026lt;HashMap\u0026lt;String, ByteIterator\u0026gt;\u0026gt;()); } public void doTransactionUpdate(DB db) { // choose a random key long keynum = nextKeynum(); String keyname = MyWorkload.buildKeyName(keynum, zeropadding, orderedinserts); HashMap\u0026lt;String, ByteIterator\u0026gt; values; if (writeallfields) { // new data for all the fields values = buildValues(keyname); } else { // update a random field values = buildSingleValue(keyname); } db.update(table, keyname, values); } public void doTransactionInsert(DB db) { // choose the next key long keynum = transactioninsertkeysequence.nextValue(); try { String dbkey = MyWorkload.buildKeyName(keynum, zeropadding, orderedinserts); HashMap\u0026lt;String, ByteIterator\u0026gt; values = buildValues(dbkey); db.insert(table, dbkey, values); } finally { transactioninsertkeysequence.acknowledge(keynum); } } /** * Creates a weighted discrete values with database operations for a workload to perform. * Weights/proportions are read from the properties list and defaults are used * when values are not configured. * Current operations are \u0026#34;READ\u0026#34;, \u0026#34;UPDATE\u0026#34;, \u0026#34;INSERT\u0026#34;, \u0026#34;SCAN\u0026#34; and \u0026#34;READMODIFYWRITE\u0026#34;. * * @param p The properties list to pull weights from. * @return A generator that can be used to determine the next operation to perform. * @throws IllegalArgumentException if the properties object was null. */ protected static DiscreteGenerator createOperationGenerator(final Properties p) { if (p == null) { throw new IllegalArgumentException(\u0026#34;Properties object cannot be null\u0026#34;); } final double readproportion = Double.parseDouble( p.getProperty(READ_PROPORTION_PROPERTY, READ_PROPORTION_PROPERTY_DEFAULT)); final double updateproportion = Double.parseDouble( p.getProperty(UPDATE_PROPORTION_PROPERTY, UPDATE_PROPORTION_PROPERTY_DEFAULT)); final double insertproportion = Double.parseDouble( p.getProperty(INSERT_PROPORTION_PROPERTY, INSERT_PROPORTION_PROPERTY_DEFAULT)); final double scanproportion = Double.parseDouble( p.getProperty(SCAN_PROPORTION_PROPERTY, SCAN_PROPORTION_PROPERTY_DEFAULT)); final double readmodifywriteproportion = Double.parseDouble(p.getProperty( READMODIFYWRITE_PROPORTION_PROPERTY, READMODIFYWRITE_PROPORTION_PROPERTY_DEFAULT)); final DiscreteGenerator operationchooser = new DiscreteGenerator(); if (readproportion \u0026gt; 0) { operationchooser.addValue(readproportion, \u0026#34;READ\u0026#34;); } if (updateproportion \u0026gt; 0) { operationchooser.addValue(updateproportion, \u0026#34;UPDATE\u0026#34;); } if (insertproportion \u0026gt; 0) { operationchooser.addValue(insertproportion, \u0026#34;INSERT\u0026#34;); } if (scanproportion \u0026gt; 0) { operationchooser.addValue(scanproportion, \u0026#34;SCAN\u0026#34;); } if (readmodifywriteproportion \u0026gt; 0) { operationchooser.addValue(readmodifywriteproportion, \u0026#34;READMODIFYWRITE\u0026#34;); } return operationchooser; } } 将其复制到 YCSB/core/src/main/java/site/ycsb/workloads/目录下。复制命令：sudo cp /home/zufs/share/YCSB/core/src/main/java/site/ycsb/workloads/MyWorkload.java /home/zufs/YCSB/core/src/main/java/site/ycsb/workloads/\n修改 FileGenerator. java，将其放到 YCSB/core/src/main/java/site/ycsb/目录下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 package site.ycsb.generator; import java.io.BufferedReader; import java.io.FileReader; import java.io.IOException; import java.io.Reader; /** * A generator, whose sequence is the lines of a file, looping over the file content. */ public class FileGenerator extends Generator\u0026lt;String\u0026gt; { private final String filename; private String current; private BufferedReader reader; private boolean fileEndReached; /** * Create a FileGenerator with the given file. * @param filename The file to read lines from. */ public FileGenerator(String filename) { this.filename = filename; this.fileEndReached = false; reloadFile(); } /** * Return the next string of the sequence, ie the next line of the file. * If the end of the file is reached, the file is reloaded and reading starts again from the beginning. */ @Override public synchronized String nextValue() { try { if (fileEndReached) { reloadFile(); // Reload the file once the end is reached fileEndReached = false; } current = reader.readLine(); if (current == null) { fileEndReached = true; // End of file reached return nextValue(); // Recurse to reload the file and get the first line again } return current; } catch (IOException e) { throw new RuntimeException(e); } } /** * Return the previous read line. */ @Override public String lastValue() { return current; } /** * Reopen the file to reuse values. */ public synchronized void reloadFile() { try (Reader r = reader) { System.err.println(\u0026#34;Reload \u0026#34; + filename); reader = new BufferedReader(new FileReader(filename)); } catch (IOException e) { throw new RuntimeException(e); } } } 编译 YCSB 安装必要环境\n1 sudo apt install maven openjdk-8-jdk libgflags-dev libsnappy-dev zlib1g-dev libbz2-dev liblz4-dev libzstd-dev 配置环境变量\n1 export JAVA_HOME=\u0026#34;/usr/lib/jvm/java-8-openjdk-amd64\u0026#34; 如果使用 RocksDB，需要在 ./rocksdb/pom.xml 手动添加 htrace 和 hdrhistogram 依赖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.rocksdb\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rocksdbjni\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${rocksdb.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.htrace\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;htrace-core4\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.0-incubating\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.hdrhistogram\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;HdrHistogram\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;site.ycsb\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${project.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 切换到 YCSB 目录下编译\n1 mvn -pl site.ycsb:rocksdb-binding -am clean package 测试时修改 workload 文件，添加一个参数 datafile 是数据集负载路径。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 recordcount=5000 operationcount=100000 workload=site.ycsb.workloads.MyWorkload readallfields=true datafile=/home/zufs/share/YCSB_dataset/dataset/nci readproportion=0.5 updateproportion=0.5 scanproportion=0 insertproportion=0 requestdistribution=zipfian 测试命令：\n1 2 3 sudo ./bin/ycsb.sh load rocksdb -s -P workloads/fileworkloada -p rocksdb.dir=/mnt/tmp/ycsb-rocksdb-data -threads 64 sudo ./bin/ycsb.sh run rocksdb -s -P workloads/fileworkloada -p rocksdb.dir=/mnt/tmp/ycsb-rocksdb-data -threads 64 参考资料 https://blog.ywang-wnlo.xyz/posts/4bc1e607/\nhttps://github.com/brianfrankcooper/YCSB/wiki/Implementing-New-Workloads\n","date":"2024-12-30T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/ycsb_customize/","title":"修改YCSB负载测试自定义数据集"},{"content":"什么是LFS LFS即日志结构文件系统（log-structured file system）。日志结构文件系统是一种只允许顺序写的文件系统。原始的LFS叫做Sprite(精灵) LFS，是 Sprite 网络操作系统的一部分。\n为什么要将文件系统设计成日志结构的 LFS的基本假设是IO 瓶颈在写不在读，因为文件在内存有 cache。\n在写入许多小文件时，将许多同步小写转化成一个大的异步写，从而充分利用磁盘带宽。\n日志结构文件系统和日志型文件系统的区别 日志型文件系统的 log 仅用作临时存储，在崩溃恢复时使用\n日志结构文件系统将 log 作为主要存储区域，并且磁盘上没有其他的结构（这是原始的 LFS）\n怎样实现LFS 仅允许顺序写 将文件的改动暂存在 file cache 中，并向磁盘一次将所有的数据顺序写到 log 中（包括数据及元数据）。\n支持随机读 每一个文件有对应的 inode，inode 包含访问控制等信息以及指向起始 10 个数据块地址的指针、指向其他数据块地址或者其他 indirect block 的 indirect block。\n空闲空间的管理 将磁盘划分为一系列固定大小的 segment（512KB，这样使得找到一个 segment 不会比遍历 segment 本身更慢） segment 中的有效数据搬移之后才能重用（垃圾回收） 有些 segment 中存放寿命较长的数据，可以在分配空间的时候跳过，以免重复搬移（冷热数据分离） 垃圾回收 将一些 segment 读入内存，识别有效数据，并将有效数据写回干净的 segment\n每个 segment 都有一个或多个 segment summary block，包含一个块属于哪个文件（ino）以及 index（为了 GC 修改映射关系）。用于识别有效数据（trivial： 检查文件 index 处的指针是否指向这个块；sprite lfs：检查版本号）\n在Sprite LFS中写几十个 seg 就清理。\n崩溃一致性 LFS 采用 checkpoints 和前滚恢复保证崩溃一致性。\n崩溃恢复快，只需扫描最近的 log。\n（待续）\nLFS的问题 wandering tree 问题/滚雪球式更新 在 LFS 中，修改一个文件的数据块会导致其位置发生变化，即追加到尾部。这就导致指向该数据块的直接指针需要修改。然而修改其指针会导致指向直接指针的间接指针也需要修改。因此 inode、inode map 和 cp block 都需要递归修改。\n清理开销 由于 LFS 的顺序写和异地更新特性，更新一个块后原来的块就作废了。这样导致盘上存在大量作废的垃圾块，需要做垃圾回收。垃圾回收的开销需要对用户隐藏，并且移动的数据量应该尽可能少，移动应该尽可能快。\nLFS 中的垃圾回收严重影响性能，缩短 SSD 寿命（写放大）\nSFS: Random write considered harmful in solid state drives FAST 12\nF2FS的优点以及与LFS的区别 日志结构文件系统的固有优点 f2fs 采用顺序写，因此具有适合闪存介质特性的特点。\n闪存介质只支持异地更新，不支持就地更新。 随机写导致闪存内部碎片化。 解决了wandering tree问题 在 LFS 中，修改一个文件的数据块会导致其位置发生变化，即追加到尾部。这就导致指向该数据块的直接指针需要修改。然而修改其指针会导致指向直接指针的间接指针也需要修改。因此 inode、inode map 和 cp block 都需要递归修改。\n在F2FS中，增加了一个随机写的元数据区域。其中，引入 NAT 表记录 node 位置，切断递归更新。\n更新文件数据块-\u0026gt;更新 dnode 内容-\u0026gt;更新 NAT 表中 dnode位置-\u0026gt;结束 解决了LFS的高GC开销问题 采用multi-head logging实现冷热数据分离 F2FS中存在6个log，即{Hot, Warm, Cold}* {Node, Data}。\nLFS中只有一个全局的大log，而F2FS中通过将空间划分为6个log实现了冷热数据分离。冷热数据分离就是为了减少GC开销。\n自适应切换写入方式 当空间利用率不高时，F2FS采用append方式顺序写入。\n当空间利用率太高时，为了找到干净的segment需要频繁GC。这时F2FS可以采用threaded logging方式写入数据。\nthreaded logging是指直接在脏segment里面利用内碎片接着写，不用提前清理。\nGC单位与FTL操作单元对齐 采用Section作为GC单位，与FTL操作单元对齐。\n在ZNS SSD上，Section就和Zone对齐，因此只需要做一次GC。\n未列出的参考资料 https://github.com/RiweiPan/F2FS-NOTES/blob/master/F2FS-Layout/总体结构.md\nThe design and implementation of a log-structured file system\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/f2fs_lfs/","title":"F2FS的优点以及与LFS的区别"},{"content":"崩溃恢复包括正常状态下的fsync和上电后的前滚恢复。因3.8版本中的f2fs是初版，较为简单。因此本文分析Linux 3.8版本中的崩溃恢复流程，借以了解崩溃恢复。\nfsync分析 1 2 3 4 5 6 7 8 flowchart LR A[f2fs_sync_file]--\u0026gt;B[filemap_write_and_wait_range] A--\u0026gt;C[f2fs_balance_fs] A--need_cp--\u0026gt;D[f2fs_sync_fs] A--!need_cp--\u0026gt;E[sync_node_pages] A--!need_cp--\u0026gt;F[f2fs_write_inode] A--!need_cp--\u0026gt;G[filemap_fdatawait_range] fsync函数的原型为int f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)，其作用是将file指向的文件中，偏移量从start到end的部分刷写下盘。\n刷写数据部分 文件系统通过wbc结构体来控制写回过程的行为。这里设置wbc为\n1 2 3 4 5 6 7 struct writeback_control wbc = { .sync_mode = WB_SYNC_ALL, // 同步写回，等待所有页面的写回完成 .nr_to_write = LONG_MAX, // 暂时不设置在本次操作中最多写回的页面数 .for_reclaim = 0, // 表示当前的写回操作不是为了回收页面触发的，而是由于fsync // 在内存不足的情况下，内核通过回收页面来释放内存， // 在回收页面之前要先将页面写回存储设备 }; 如果文件系统在挂载时设置为只读的，即sb中设置了MS_RDONLY标志位，则直接返回0，因为无法在只读的文件系统中写回。\n接下来通过filemap_write_and_wait_range函数写回start到end的数据。函数原型是int filemap_write_and_wait_range(struct address_space * mapping, loff_t lstart, loff_t lend);\n该函数是一个同步操作，将mapping中lstart至lend中的脏页刷盘，等待写回完成后再返回。也即将文件指定范围的内容持久化。mapping是文件在内存中的地址空间。\n在写回数据之后，调用f2fs_balance_fs检测sbi以确定是否还有足够的空闲section，如果没有就做一次主动GC。实际上这个函数经常在和写入相关的操作中被调用，每次写入后都要检查一下。\nfsync和fdatasync都通过f2fs_sync_file函数执行。如果当前执行的时fdatasync，则到这里就已经结束返回了。下面是fsync中针对元数据的操作。\n刷写元数据部分 1 2 3 4 if (F2FS_I(inode)-\u0026gt;data_version != cur_version \u0026amp;\u0026amp; !(inode-\u0026gt;i_state \u0026amp; I_DIRTY)) goto out; F2FS_I(inode)-\u0026gt;data_version--; 检查该文件的data_version是否等于当前CP的cur_version。如果不等于，说明该文件是上一次CP之前就存在的，盘上有它的inode结构和目录结构。如果该文件的I_DIRTY标志位没有置位，则该文件没有发生任何变化。因此不需要做任何操作。\n问题：如果该文件没有发生任何变化，为何不在最开始就做这个判断？并且5.15中没有做这个判断。\n问题：为何需要F2FS_I(inode)-\u0026gt;data_version\u0026ndash;;？\n接下来判断是否需要CP。\n1 2 3 4 5 6 7 8 if (!S_ISREG(inode-\u0026gt;i_mode) || inode-\u0026gt;i_nlink != 1) need_cp = true; if (is_inode_flag_set(F2FS_I(inode), FI_NEED_CP)) need_cp = true; if (!space_for_roll_forward(sbi)) need_cp = true; if (need_to_sync_dir(sbi, inode)) need_cp = true; 第一个if：如果不是普通文件或硬链接计数不为1（文件已被删除或有多个硬链接） 问题：为什么有多个硬链接也要CP？ 第二个if：如果具有需要CP的标记 第三个if：如果没有足够的空间做前滚，也就是距离上一次CP太远了，需要前滚的数据太多了 第四个if：如果该文件所在目录文件不在上一个CP点中 如果需要CP则做CP，否则只落盘元数据，包括dnode和inode。\n接下来使用sync_node_pages落盘该文件的inode和脏dnode。f2fs在做fsync的时候先将所有修改过的数据落盘（即上面的filemap_write_and_wait_range），再将其node元数据落盘（即这里的sync_node_pages）。必须保证这个顺序，否则如果fsync没有做完就断电了，上电后根据node恢复数据的时候dnode指向的数据是空的或错的。\n这里需要对照具体的代码看。\n在sync_node_pages中，用到的地址空间是sbi→node_inode→i_mapping。其中node_inode是一个特殊的文件，用于管理内存中的node。\n扫描node_inode中的所有node，pagevec_lookup_tag用于查找其中具有PAGECACHE_TAG_DIRTY标记的脏node，并将其放入pvec中。\n扫描pvec。当扫描到的node属于该文件时，为其打上fsync标记。如果扫描到的是inode，还要打dentry标记。dentry标记是指如果某个文件没有持久化过，则盘上的目录文件没有相应的结构，也没办法寻找到这个文件。因此需要为其单独创建一个inode结构，并在目录中加入这个文件。（在5.15版本中，当文件没有CP过并且也没有做过fsync才打dentry标记，第二次fsync不打）\n打完标记后，通过f2fs_write_node_page和f2fs_submit_bio将node page刷写下盘。这里前者应当只是构造合并了bio，放在sbi-\u0026gt;bio[NODE]还没有提交。后者提交bio。\n如果sync_node_pages没有写下任何node page，说明该文件的node都不脏。这种情况应当是truncate过，也就是删除了文件的一部分，因此没有脏node。这种情况下仅通过f2fs_write_inode刷写其inode。\n问题：truncate后不应该做CP吗？可能这个早期版本没有考虑周全。\n最后调用filemap_fdatawait_range确保所有的脏node都已经刷盘，这是一个同步等待操作。\n前滚回复分析 1 2 3 4 5 6 7 flowchart A[f2fs_fill_super]--\u0026gt;B[recover_fsync_data] B--\u0026gt;C[find_fsync_dnodes] B--\u0026gt;D[recover_data] C--\u0026gt;E[recover_inode] E--\u0026gt;F[recover_dentry] D--\u0026gt;G[do_recover_data] 在挂载文件系统后，f2fs_fill_super函数调用recover_fsync_data函数做前滚回复。前滚恢复的过程包括两次扫描，即find_fsync_dnodes和recover_data。\n第一次扫描find_fsync_dnodes的作用是通过扫描恢复区域的node page，确定有哪些文件是需要恢复的，并将其加入inode_list链表中。第二次扫描recover_data的作用是通过扫描恢复区域的node page，利用这些node page的内容重建文件。\n注意：\n两次扫描的范围相同。 扫描的对象是普通文件的dnode，也就是直接存储文件数据地址的node。inode中也存储了923个数据地址，也算作dnode。这些node在warm node log中。 恢复数据实际上是修改文件中dnode指向数据的地址。因为数据已经通过fsync落盘了，本身是不需要恢复的。但是dnode的指向关系还没有更新，因此才需要前滚回复来完成这一流程。 在前滚回复开始之前，文件系统的状态是回滚到上一次CP点。所以上一次CP点没有的文件，直接读是读不到的。因此才需要对具有dentry标记的文件单独从头恢复。 如果P是一个 inode，并具有 dentry 标记DENT_BIT_SHIFT，就为P对应的 inode 打FI_INC_LINK 标记。 如果某个 dnode 在 footer 中没有 checkpointed 标记，则要打上 dentry 标记。意思是这个文件没有持久化过，因此盘上的目录文件没有相应的结构，也没办法寻找到这个文件。因此需要为其单独创建一个inode结构，并在目录中加入这个文件。 既然这个表项已经存在于inode_list链表中了，那么说明P的inode结构已经创建出来了，增加引用计数是恢复目录结构中目录对该inode的引用。 问题：我感觉FI_INC_LINK标记没有用到，因为recover_dentry中增加计数的时候并没有根据这个标记判断。 第一次扫描 find_fsync_dnodes 第一次扫描的作用是找到需要恢复的文件，并将其inode插入记录链表inode_list中。注意，inode_list并非直接存储inode结构，而是存储fsync_inode_entry结构。\n扫描范围 扫描的范围是上一次CP后写入的第一个node到最后一个有记录的node。其中，起始地址这样确定：\n1 2 curseg = CURSEG_I(sbi, CURSEG_WARM_NODE); blkaddr = START_BLOCK(sbi, curseg-\u0026gt;segno) + curseg-\u0026gt;next_blkoff; 需要扫描的node都在warm node log中，通过CURSEG_I(sbi, CURSEG_WARM_NODE)找到上一次CP记录的warm node log对应的segment（这个信息存储在sbi中，只有CP的时候才落盘sbi）。并且找到下一个segment的起始地址。\nnode的footer中记录了下一个node的地址，通过blkaddr = next_blkaddr_of_node(page)可以获得。扫描终止条件是（1）下一个地址不是合法地址、取不出页面；（2）取得的 node 是上一次 CP 之前的；（3）函数返回错误、资源分配错误，可能因为盘上数据已损坏。\n在扫描过程中，仅处理通过fsync落盘、具有fsync标记（FSYNC_BIT_SHIFT）的node page，通过is_fsync_dnode(page)判断。\n扫描过程 设当前正在扫描的node page为P，则\n首先检查P所属的inode是否已经插入inode_list。 如果已有，则\n将entry的blkaddr字段更新成P。这个地址指的是该 entry 对应的文件，在盘上扫描到的最后一个 node 的地址。也即是恢复数据的时候，处理完这个 node，就代表该文件恢复完毕了。\n如果P是一个 inode，并具有 dentry 标记DENT_BIT_SHIFT，就为P对应的 inode 打FI_INC_LINK 标记。\n如果某个 dnode 在 footer 中没有 checkpointed 标记，则要打上 dentry 标记。意思是这个文件没有持久化过，因此盘上的目录文件没有相应的结构，也没办法寻找到这个文件。因此需要为其单独创建一个inode结构，并在目录中加入这个文件。 既然这个表项已经存在于inode_list链表中了，那么说明P的inode结构已经创建出来了，增加引用计数是恢复目录结构中目录对该inode的引用。 问题：我感觉FI_INC_LINK标记没有用到，因为recover_dentry中增加计数的时候并没有根据这个标记判断。\n如果没有，则\n如果P是一个 inode，并具有 dentry 标记DENT_BIT_SHIFT，就根据P创建它的inode结构。 根据P的inode创建一个entry表项，并插入到inode_list链表中。 将entry的blkaddr字段设置成P。 如果P是一个inode，则根据P恢复文件系统中对应inode的结构。其中如果P是刚创建出来的，则还需要通过recover_dentry恢复目录结构。 recover_inode 函数中恢复一些时间戳、文件大小和 mode。然后调用 recover_dentry 恢复目录结构。 只有具有 DENT_BIT_SHIFT 标记的 page，也就是之前过的没有持久化过的文件才需要单独在这里恢复目录结构。 得到其目录文件的 inode，并建立相应的目录结构 dent。 如果在目录文件中存在本文件名字的目录项则没事，如果不存在，就将 dent 加入目录文件中（f2fs_add_link）。 第二次扫描 recover_data 扫描范围 第二次扫描的范围和第一次相同。\n扫描过程 将当前扫描到的 node page （设为P）读上来，检查是否属于第一轮扫描中加入到 inode_list 链表中的文件。如果是，则通过 do_recover_data 利用P的内容恢复数据。\n获取P中包含数据的范围 首先通过P中的标志位获知P是第几个dnode，从而计算其中存储数据的index范围。\nofs_of_node 函数通过获取P的 node_ofs 获取该 dnode 在整个文件中的偏移量。这个 node_ofs 保存在中 footer.flag 的高位中（低 3 位保存的是 COLD_BIT、FSYNC_BIT 和 DENT_BIT），右移 OFFSET_BIT_SHIFT 位即可得到 node_ofs。\nnode_ofs 指的是 dnode 在整个文件中的偏移量，即第几个 dnode。从而可以得到该 dnode 指向的数据在整个文件中的 index。\nF2FS数据块寻址（linux5.18.11）_f2fs node和data-CSDN博客\n总之，这里获得了P中保存的数据之首尾 index，分别叫 start 和 end。\n根据P恢复文件中对应index的数据 注意恢复数据是指将上一给CP点中该文件指向的旧的数据地址更新为P中包含的地址，还涉及涉及对 summary、extent cache 和 SIT 等的修改。\n接下来从 start 开始一直恢复到 end。在一个 for 循环内，逐个恢复每个 index 对应的数据块。src 表示盘上读出的数据块地址、dest 表示扫描到的 page 里得到的数据块地址。\n如果 src== dest，表示盘上读出的地址就是最新的地址，没必要恢复。如果 dest== NEW_ADDR 或 NULL_ADDR，表示 page 上扫描到的这个数据块有问题，没有办法恢复。\n恢复完数据后，如果P和刚恢复完的都是inode，就刷下去。\n用P恢复读上来的 node page自身结构，如footer等。\n回到 recover_data 在用 do_recover_data 恢复数据后，如果P就是 inode list entry 记录的最后一个 blkaddr，则表示该 inode 对应的文件已经恢复完成，删除这个表项。\n找到下一个 node 的地址，继续下一轮恢复。\n收尾工作 在恢复完 inode_list 中的所有表项以后，就删除临时创建的所有数据结构、收回分配的所有空间。\n最后再做一次 CP。\n注意 要使用一个 page 的内容，先要通过 page_address 取得其内容在内存中的虚拟地址，然后通过结构体访问。 盘上的 node page 实际是以 f2fs_node 的形式存放的。并不是 vfs 的 inode 结构体。 文中未列出的参考资料 （https://blog.csdn.net/luckyapple1028/article/details/61413724）\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/crash_recovery_f2fs_linux3.8/","title":"Linux 3.8中的F2FS崩溃恢复"},{"content":"什么是SSD 固态硬盘（Solid State Drive, SSD）是一种基于闪存芯片的存储设备。为了增加IO并发性、提高带宽，每个通道（Channel）连接了多个闪存芯片（Die）。每个die包含了多个擦除块（Erase Block），每个block上包含了多个页（Page）。每个page上包含两个区域：（1）数据区域，用于存储用户数据；（2）元数据区域，用于存储纠错码（Error-Correcting Code, ECC）等其它信息。\nSSD的性能更优、功耗更低，因此已被广泛整合应用于个人计算机、移动终端设备、数据中心以及云存储之中，逐渐取代传统的HDD设备。与传统的机械硬盘（Hard Disk Drive, HDD）相比较，SSD延迟更低、带宽更高。SSD没有机械运动部件，使其抗震动性能优越、能源消耗更低且没有运行噪声，并显著减少了体积和重量。\n什么是全闪存储 全闪存储是指存储阵列全部由固态存储介质构成，行业普遍认为全闪存储是存储行业的发展趋势。华为的全闪存储产品OceanStor Dorado相较于传统机械存储，在SPC-1基准测试下业务性能提升了5倍、在数据库场景下业务性能提升了10倍、在虚拟桌面场景下应用启动相应时间缩短了80%。充分体现了全闪存储产品优异的性能。\n然而，目前全闪存储还未成为存储领域应用主流，SSD还未完全取代HDD。其中两个重要原因在于（1）SSD与HDD相比，成本较高、容量较小；（2）块接口不适合闪存的特性。\n（1）SSD比HDD的容量更小、成本更高\nSSD系列产品容量普遍小于HDD系列产品，而SSD的平均每GB单价是HDD的11倍。成本和容量问题是制约SSD取代HDD的一个重要因素。\n容量 整盘价格（美元） 每GB单价（美元） 平均每GB单价（美元） 西部数据 Gold系列HDD 1TB 84.36 0.082 0.032 4TB 159.99 0.039 6TB 194.99 0.032 8TB 229.99 0.028 10TB 269.99 0.026 12TB 314.99 0.026 14TB 355.8 0.025 16TB 349.99 0.021 18TB 379.99 0.021 22TB 459.99 0.020 西部数据 Gold系列SSD 960GB 499.99 0.521 0.356 1.92TB 699.99 0.356 3.84TB 899.99 0.229 7.68TB 2499 0.318 （2）块接口不适合闪存特性\nSSD的闪存芯片支持读取、写入和擦除3种操作。读写操作的粒度是页，而擦除操作的粒度是块。闪存具有不允许就地更新和芯片寿命有限的特性，即每个块在更新其中页时必须先全部擦除、每个闪存单元具有有限的读写循环（Program/Erase, P/E）次数。\nSSD为了兼容HDD时代的各种硬件，采用传统的块接口（Block Interface）抽象下层物理设备。块接口要求SSD提供一段连续的逻辑空间，并允许按照任意顺序读取、写入和就地更新数据。而一个擦除块内各个页面的生命周期不同，页会先后失效。为了重新利用擦除块内失效页的空间，SSD需要实现垃圾回收（Garbage Collection, GC）将块内有效数据迁移出去。从而需要在盘内预留过度配置（Over Privisioning, OP）空间以承接有效数据、降低垃圾回收操作带来的性能影响。垃圾回收操作还会导致吞吐量下降、写放大、尾延迟高、闪存寿命下降等一系列问题。\nSSD由于先擦后写、擦写粒度不一致的特性存在垃圾回收等数据迁移开销，造成了写放大和空间浪费，写放大还会损耗SSD的寿命。而HDD可以在盘内任意位置擦写，不存在垃圾回收和有效数据迁移。因此，解决块接口和闪存特性不匹配是SSD取代HDD过程中的一个重要问题。\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/brief_on_ssd/","title":"SSD简介"},{"content":"操作系统通过trim命令通知SSD哪些块不再使用，可以擦除或回收。\n文件系统在删除文件时，仅将数据块标记为未使用。因此文件所在的位置只有下一次写入时才会被覆盖。但是对于SSD，数据页在重写之前必须擦除其所在块。因此存在垃圾回收和写放大。\ntrim命令使得SSD知道哪些数据页是无效的，在做垃圾回收时不搬移这些页。\n因此，在SSD做过垃圾回收之后几乎不可能恢复数据。但是HDD中被删除的数据仍有可能恢复。\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/what_is_trim_command/","title":"什么是trim命令"},{"content":"什么是内存 内存是与CPU直接交换数据的内部存储器。内存中存放的是计算机运行时必须的数据，包括操作系统、进程等。任何数据在被CPU操作之前，必须先读入内存。\n内存属于计算机系统内部，而外存属于计算机系统的外设。内存可以看作是外存的cache。\n一种比喻是：如果您有一张书桌和一个文件柜，书桌代表电脑的内存。您需要马上使用的材料留在书桌上，以便轻松找到。当您离开办公室时，留在桌上的任何文件都会被扔掉。文件柜中的任何东西都会保留下来。 内存通常由随机访问存储器（Random Access Memory, RAM）构成。因此内存和RAM有时是混用的。\nRAM分为动态RAM（DRAM）和静态RAM（SRAM），内存由DRAM构成。 SRAM比DRAM更快、功耗更低。但是容量更小、价格更高、设计更复杂，通常用于CPU内部的高速缓存。详见：（本站）存储的层次结构（Memory Hierarchy） 、https://www.amazonaws.cn/knowledge/what-is-memory/ 下图的黑色部分是存储器模块，通过存储控制器管理。存储器模块中有 DRAM 芯片。\n内存的作用 内存的数据是临时的、易失的，而外存的数据是持久存储的。电脑关机时，内存是空的，所有的数据都保存在外存上。\n电脑启动时，BIOS先自检，然后通过引导加载程序将操作系统加载进内存，这样才能通过操作系统使用计算机。\n操作系统想要执行程序，必须将程序的代码读入内存，并在内存上分配程序必须的地址空间。此外，进程想要使用任何文件，都必须先将其内容读入内存。\nCPU只能通过内存访问数据，因此内存容量与程序运行速度、并发处理任务的效率很有关系。\n什么是DDR、LPDDR？ 经常看到DDR4、DDR5等表述，这些表述是什么意思？\nDDR 双倍数据率同步动态随机存储器（Double Data Rate Synchronous Dynamic Random Access Memory, DDR SDRAM）。DDR是DDR SDRAM的简称。\n其中\nSDRAM代表同步动态随机存取存储器，SDRAM是在DRAM的基础上增加同步和双区域的功能，使得CPU能与SDRAM的时钟同步，因此SDRAM比DRAM访问更快。 DDR代表双倍数据传输率。一个时钟周期内，SDRAM只在上升沿传输一次数据，而DDR则在上升沿和下降沿各传输一次数据。所以数据传输速度是CPU主频的两倍。因此DDR SDRAM比SDRAM更快。 参考：https://zh.wikipedia.org/wiki/DDR_SDRAM、https://zh.wikipedia.org/wiki/SDRAM\nDDR5是最新的第5代DDR，在2021年推出。与DDR4相比，DDR5具有以下优势。\n参考：https://www.rambus.com/blogs/ddr5-%E4%B8%8E-ddr4-%E6%89%80%E6%9C%89%E8%AE%BE%E8%AE%A1%E6%8C%91%E6%88%98%E4%B8%8E%E4%BC%98%E5%8A%BF/?lang=zh-hans\n此外，DRAM和各代DDR SDRAM的数据如下。\nDRAM DDR DDR2 DDR3 DDR4 DDR5 預取 1 - 位元 2 - 位元 4 - 位元 8 - 位元 每記憶庫位元 16 - 位元 資料速率 (MT/s) 100 - 166 266 - 400 533 - 800 1066 - 1600 2133 - 5100 3200 - 6400 傳輸速率 (GB/s) 0.8 - 1.3 2.1 - 3.2 4.2 - 6.4 8.5 - 14.9 17 - 25.6 38.4 - 51.2 電壓 (V) 3.3 2.5 - 2.6 1.8 1.35 - 1.5 1.2 1.1 https://www.crucial.tw/articles/about-memory/difference-among-ddr2-ddr3-ddr4-and-ddr5-memory\n注意：一般一种主板只支持一种类型的内存。因为各代DDR之间的物理结构（插槽不一样）、电气规范、工作电压、时钟速度和通信协议上都有差异。不同代内存不能混用。\nLPDDR LPDDR的全称是Low Power Double Data Rate SDRAM，也即移动DDR（Mobile DDR, MDDR或Low Power DDR, LPDDR）。\nMDDR的运行电压（工作电压）低于DDR的标准电压，因此MDDR具有低功耗、高可靠性的特点。适合移动设备使用。\nLPDDR5是2019年推出的，最新规范为LPDDR5X（2021）。\n参考：https://zh.wikipedia.org/wiki/移动DDR#LPDDR5\n未列出的参考资料 https://www.crucial.cn/articles/about-memory/what-does-ram-stand-for\nhttps://www.kingston.com/cn/blog/pc-performance/difference-between-memory-storage\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/what_is_memory/","title":"什么是内存"},{"content":"挂载F2FS 文件系统可以挂载在盘上，也可以挂载在文件上。有时候为了实验会在文件上挂载一个文件系统。\n创建一个100M的空文件： 1 dd if=/dev/zero of=/data/local/tmp/f2fs_100MB.bin bs=1M count=100 其中，if是指从zero设备中取数据，取出的都是0。of是指将0写入后面路径的文件中，大小是100个1M。\n格式化文件系统 1 make_f2fs -f -d1 -g android -O compression -O extra_attr /data/local/tmp/f2fs_100MB.bin (参数待查)\n获取文件系统信息 使用dump.f2fs命令获取文件系统的信息\n1 dump.f2fs -d 1 f2fs_100MB.bin (参数待查)\n未列出的参考资料 https://mp.weixin.qq.com/s/FbGyxclb1Gesk8_apEexPQ\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/use_f2fs_filesystem/","title":"使用F2FS文件系统"},{"content":"数据放置技术简介 在固态硬盘（SSD）领域，数据放置技术是用来使主机能够控制数据在 SSD 内部如何放置的技术。这是因为传统上主机只能控制数据在逻辑上的地址，而不能真正控制其在物理上如何放置。\n数据放置技术的作用有：1. 减少 SSD 的写放大；2. 提高 SSD 的服务质量（QoS）；3. 减少过度配置（OP）；4. 优化 SSD 的总体拥有成本（TCO）。\n为什么需要数据放置技术 下图是 SSD 内部结构示意。SSD 用 NAND 闪存颗粒存储数据，SSD 内部的 NAND 颗粒通过 die、plane、block 和 page 组织。\n传统的 SSD 中，主机通过逻辑地址 LBA 读写 SSD 的数据。NAND SSD 是不能覆盖写的，必须要写前擦除。擦除是以闪存块为单位的，一般是 MB 级别；写操作以闪存页为单位，一般是 KB 级别。因此 SSD 实际上是不能就地更新的，必须将待写的页写入一个新的块，再通过 FTL 将 LBA 映射到这个新的块。\n这导致了垃圾回收和写放大。垃圾回收和写放大是指 FTL 将 LBA 映射到新的物理地址后，原来物理地址中的数据就失效了。当一个闪存块中失效页的数量达到一定程度时，为了释放失效页的空间，需要将有效数据迁移到别的闪存块，再擦除该块，这叫做垃圾回收。由于垃圾回收而引起的有效数据迁移叫做写放大。写放大不仅加速了 SSD 的磨损老化，降低了 SSD 的寿命；还增加了读写延迟，降低了 SSD 的性能和 QoS。\n为了减少写放大，主机可以利用数据放置技术将生命周期相近的数据块放置在相近的位置，以便使他们同时失效，从而减少垃圾回收的开销。但是由于 FTL 决定了数据的放置，传统 SSD是不能做到这一点的。\n有哪些数据放置技术 （CNS 是指传统 SSD）\nOpen Channel SSD（OCSSD） Open Channel SSD (OCSSD) 是最早的数据放置技术。Open Channel 将盘内布局完全暴露给主机，也即主机需要完成 FTL 的所有工作，包括映射、垃圾回收和磨损均衡等。\n但是由于各个厂商生产的 SSD 各不相同，且 SSD 的管理变得越来越复杂，主机无法适配每一种 SSD。因此 OCSSD 技术从未标准化并且最终退出市场。\nMulti-Stream 多流（Multi-Stream）是随后出现的数据放置技术。原理是主机根据数据类型的不同，如温度，给写命令附上一个 hint。SSD 通过 hint 将数据放置到不同区域。\n多流的实现很简单，并且兼容性好。如果主机不给 hint，就和传统盘完全一样。\n多流曾经标准化过，并且也进入过 Linux 主线。但是多流太简单，效果也不好。最终缺乏市场吸引力，Linux 也移除了支持。\nZoned Namespace SSD (ZNS SSD) ZNS SSD 根据 LBA 将 SSD 空间顺序地划分为若干 zone，并以 zone 为读写的基本单位。在 zone 内只允许主机顺序写。主机需要顺序发送写请求，并且完成 zone 的管理和垃圾回收。\nZNS 利用已经存在的 ZAC/ZBC 标准，并且将物理介质的管理留给设备的 FTL。因此较为容易标准化。但是顺序写入的限制使得主机兼容难度较大。因此 ZNS 只适合特定的应用场景，例如日志结构的文件系统。\nOCSSD Multi-Stream ZNS SSD 主机干预程度 主机完全控制 主机轻量级控制 主机控制数据放置，但不管理介质 采用难度 需要大幅修改主机存储栈 简单修改主机存储栈 需要大幅修改主机存储栈 兼容性 不兼容 兼容 不兼容 写放大 可以到 1 不能保证 可以到 1 现状 从未标准化，已经退出市场 标准化，但退出市场 标准化，但对市场吸引力低 Flexible Data Placement (FDP) 从上面数据放置技术的失败可以看出，好的数据放置技术必须要\n将介质管理的工作留给 SSD，这是因为在主机管理 SSD 介质是非常复杂的，考虑到 SSD 厂商和种类的不同，主机几乎不可能兼容所有的 SSD。 不要对主机软件栈做过多的修改，这是因为主机的应用软件已经非常稳定，而数据放置技术却在频繁更新发展。软件不可能为了兼容一种新的存储协议，如 ZNS，做全面的修改。 产业支持和切实的应用场景非常重要。对于多流来说，虽然使用简单，但是没有应用场景，所以市场也不会买账。 FDP 技术将 SSD 划分为 Reclaim Group，一个 Reclaim Group 上有若干 Reclaim Unit。主机通过指定 Reclaim Group 和 Reclaim Unit Handle 确定要将数据放置在哪个 Reclaim Unit 中。\nFDP 可以兼容传统软件栈，如果不指定 Reclaim Group 和 Reclaim Unit Handle ，就按照传统 SSD 的方式写入。\nFDP 还可以将盘内信息反馈给主机，下面是多流、ZNS 和 FDP 的对比。\n总结 数据放置技术的历史并不长，第一个数据放置技术 OCSSD 的产生距今还不到 10 年。目前数据放置领域最重要的两个技术是 ZNS 和 FDP，两者各有优劣。\nZNS 优势：没有 OP 和盘内 GC、消除映射表 劣势：顺序写限制、只能在单一命名空间内写入 FDP 优势：支持随机写、兼容性好、可以写入多个命名空间 劣势：需要完整映射表、OP 和盘内 GC 由于 FDP 的兼容性好，没有顺序写入的限制，预计 FDP 将会成为新一代主流数据放置技术，极大改变 SSD 的生态。而 ZNS 技术将仅限于适合顺序写入的场景，例如日志结构文件系统 F2FS 等。\n参考资料 https://semiconductor.samsung.com/news-events/tech-blog/a-brief-history-of-data-placement-technologies/\nhttps://download.semiconductor.samsung.com/resources/white-paper/getting-started-with-fdp-v4.pdf\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/brief_on_data_placement/","title":"固态硬盘数据放置（Data Placement）技术简介"},{"content":"什么是存储的层次结构 计算机系统中的各级存储设备具有访问速度从快到慢、存储容量从小到大、存储单价由高到低的金字塔层次结构。\n存储的层次结构基于缓存原理。缓存的意思是，一方面某个存储设备中存了大量的数据，而当前需要使用的只是其中的一小部分；另一方面通常访问速度越快的存储设备其容量就越小。因此，可以采用访问速度较快而容量较小的存储设备去存放当前需要使用的一小部分数据，从而加快数据的访问速度。\n缓存之所以能够存放当前需要使用的一小部分数据，是因为局部性原理。局部性原理分为时间局部性和空间局部性。时间局部性是指当前访问过的数据，很可能马上又需要访问；空间局部性是指和当前访问数据相邻的数据，很可能马上需要访问。因此缓存中可以存放刚刚访问过的数据以及其邻近的数据。\n根据存储的金字塔层级结构，可以认为每一级存储都可以看成是上一级存储的缓存。\n在访问数据的时候，总是先查找数据是否存在于缓存中。如果存在则称为缓存命中，否则就从下一级存储中将数据读入缓存。下一级没有则去更下一级找，在本地直到查找到硬盘，如果可以访问远程存储还可以从远程读取数据。\n存储的金字塔层次结构 Level 0：寄存器 寄存器（register）有32、64等不同位的。可以存储4B、8B的数据。一个CPU内的寄存器数量和CPU架构有关，少的有几十个，多的有几百个。\n寄存器的访问速度是最快的：\n一方面是因为寄存器在CPU内部，距离CPU更近。但是距离在单机不是一个主要因素。\n以3GHz的CPU为例，电流每秒钟可以振荡30亿次，每次耗时大约为0.33纳秒。光在1纳秒的时间内，可以前进30厘米。也就是说，在CPU的一个时钟周期内，光可以前进10厘米。因此，如果内存距离CPU超过5厘米，就不可能在一个时钟周期内完成数据的读取，这还没有考虑硬件的限制和电流实际上达不到光速。相比之下，寄存器在CPU内部，当然读起来会快一点。\n距离对于桌面电脑影响很大，对于手机影响就要小得多。手机CPU的时钟频率比较慢（iPhone 5s为1.3GHz），而且手机的内存紧挨着CPU。\n另一方面由于CPU直接存取寄存器，读写速度在一个时钟周期以内。设CPU主频为2GHz，则时钟周期为0.5ns。因此寄存器的访问延迟在1纳秒级。\nLevel 1、2、3：高速缓存 CPU内部的高速缓存由SRAM组成，比内存使用的DRAM更快、功耗更低。但是容量更小、价格更高、设计更复杂。因此适合作为缓存。\nL1 高速缓存 L1 高速缓存的访问速度几乎和寄存器一样快，通常只需要 2~4 个时钟周期，而大小在几十 KB 到几百 KB 不等。\n每个 CPU 核心都有一块属于自己的 L1 高速缓存，指令和数据在 L1 是分开存放的，所以 L1 高速缓存通常分成指令缓存和数据缓存。\n在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L1 Cache 「数据」缓存的容量大小：\n1 2 $ cat /sys/devices/system/cpu/cpu0/cache/index0/size 32K 而查看 L1 Cache 「指令」缓存的容量大小，则是：\n1 2 $ cat /sys/devices/system/cpu/cpu0/cache/index1/size 32K L2 高速缓存 L2 高速缓存同样每个 CPU 核心都有，但是 L2 高速缓存位置比 L1 高速缓存距离 CPU 核心更远，它大小比 L1 高速缓存更大，CPU 型号不同大小也就不同，通常大小在几百 KB 到几 MB 不等，访问速度则更慢，速度在 10~20 个时钟周期。\n在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L2 Cache 的容量大小：\n1 2 $ cat /sys/devices/system/cpu/cpu0/cache/index2/size 256K L3 高速缓存 L3 高速缓存通常是多个 CPU 核心共用的，位置比 L2 高速缓存距离 CPU 核心 更远，大小也会更大些，通常大小在几 MB 到几十 MB 不等，具体值根据 CPU 型号而定。访问速度相对也比较慢一些，访问速度在 20~60个时钟周期。\n在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L3 Cache 的容量大小：\n1 2 3 $ cat /sys/devices/system/cpu/cpu0/cache/index3/size 3072K Level 4：内存（主存） 内存（memory、primary storage）是计算机运行过程中用到的主要存储空间。内存中存放着操作系统、进程运行所需的数据、代码等内容。\n内存由DRAM组成，访问速度比硬盘（HDD、SSD等）要快很多，但是是易失性的，也即断电后数据就没有了。\nDRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。\nDRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问的速度会更慢，内存速度大概在 200~300 个 时钟周期之间。属于百纳秒级别（0.1μs）。\nCPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 CPU L1 Cache 比内存快 100 倍左右。\n详见：（本站）什么是内存\nLevel 5 外存（辅存） 外存是用于持久化保存数据的存储设备。外存的容量比内存更大，数据是非易失的。但外存的访问速度更慢。\n目前主流的外存有机械硬盘（HDD）、固态硬盘（SSD）。\n详见：（本站）SSD简介\nSSD 比机械硬盘快 70 倍左右；内存比机械硬盘快 100000 倍左右；CPU L1 Cache 比机械硬盘快 10000000 倍左右。\nLevel 6 远程存储 此处略。\n参考资料 https://hansimov.gitbook.io/csapp/ch01-a-tour-of-computer-systems/1.6\nhttps://blog.csdn.net/sinat_31608641/article/details/110684664\nhttps://blog.csdn.net/qq_37442469/article/details/124180541\nhttps://www.geeksforgeeks.org/difference-between-sram-and-dram/\nhttps://en.wikipedia.org/wiki/Static_random-access_memory\nhttps://en.wikipedia.org/wiki/Cache_(computing)\nhttps://en.wikipedia.org/wiki/Processor_register\n","date":"2024-12-08T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/hierarchy_of_storage/","title":"存储的层次结构"},{"content":" 可参看https://github.com/RiweiPan/F2FS-NOTES/\n什么是F2FS F2FS, Flash Friendly File System，是一种采用日志结构，适合闪存特性的文件系统。\nF2FS是从原始LFS的基础上发展来的。详见：[[原始LFS设计]]\nF2FS 的设计主要解决了原始 LFS 中的滚雪球式更新元数据（wandering tree）问题。\nF2FS的优点 详见：（本站）F2FS的优点以及与LFS的区别\nF2FS文件系统设计 F2FS在盘上组织为两个区域：随机写区域和顺序写区域。\n因此在ZNS SSD上使用F2FS时，还需挂载另一块用于存放元数据的可随机写的盘。\n随机写区域存放的是文件系统的元数据，包括超级块、CP、SIT、NAT和SSA。\n顺序写区域，也叫Main Area。存放的是Data Page和Node Page。顺序写区域还分为冷热分离的6个log。\nF2FS文件系统的元数据 F2FS中的元数据有超级块、CP、SIT、NAT、SSA。这些元数据放在随机写区域。\n这里说的元数据是整个F2FS文件系统的元数据，而不是data的元数据（各种node）。\n超级块 每个文件系统都有自己的超级块，超级块中存放文件系统整体的关键元数据。超级块（Superblock）存放在F2FS文件系统的起始位置，这样挂载后不用寻址就能直接读到超级块。\n超级块区域包含两个struct f2fs_super_block，互为备份。超级块区域的大小为2M，其中前1K留空，每个超级块占4K，后面留空。\n文件系统相关的数据结构通常都有三个：盘上的、内存中文件系统专属的和内存中vfs使用的。\n盘上数据结构struct f2fs_super_block f2fs_super_block中的主要内容有：\nzone、section和segment的数量对应关系 section、segment、block数量 元数据区各个部分、主区域segment数量 元数据区各个部分、主区域起始地址 特殊文件的inode（见下文） 用于辨识冷数据的后缀表 详见：https://mp.weixin.qq.com/s/FbGyxclb1Gesk8_apEexPQ\n内存管理结构f2fs_sb_info f2fs_sb_info是内存中用于管理超级块的结构。\n在挂载文件系统时，通过init_sb_info将f2fs_super_block中的数据填入f2fs_sb_info中。\n包含vfs superblock指针、f2fs_super_block指针以及锁、SIT、NAT对应的内存管理结构等。\nvfs superblock结构 略\nCP Checkpoint维护了f2fs的数据一致性，例如segment、node、active segment的状态等。\nF2FS在满足一定的条件的情况下，会将当前系统的分配状态写入到Checkpoint中，万一系统出现突然宕机，可以从Checkpoint中恢复到上次回写时的状态。\nF2FS维护了两个Checkpoint结构，其中一个是当前正在使用的Checkpoint。另外一个上次回写的稳定的cp。\n如果系统出现了宕机，就恢复到上次的稳定cp中。\nCP的盘上结构 CP占用2个segment大小（4M）。\n有资料说CP区域就是两个struct f2fs_checkpoint。但是还有资料说还包含orphan node和active segments。\n暂且认为盘上的cp区域由两套f2fs_checkpoint+orphan node+active segment组成。\nF2FS使用数据结构f2fs_checkpoint表示Checkpoint结构，它保存在磁盘中f2fs_super_block之后区域中。\n详见 https://github.com/RiweiPan/F2FS-NOTES/blob/master/F2FS-Layout/Checkpoint%E7%BB%93%E6%9E%84.md\nCP中存的是做CP时候的segment journal等，在恢复的时候会恢复到CP时的curseg状态。为了减少写盘次数，f2fs不会每次分配seg、node的时候就去写NAT、SSA等区域，而是放在journal保存在CP结构中。只有这个seg写完的时候才去更新一下。\nSIT Segment Info Table 占据大小由Main Area中Segment数量决定。主要用于记录Main Area中Segment的分配信息，例如哪些Segment可用，Segment最后修改时间等信息。\nSIT的作用是维护每一个segment的block的使用状态以及有效无效状态，目的是便于分配block以及垃圾回收。\n详见：https://github.com/RiweiPan/F2FS-NOTES/blob/master/F2FS-Layout/Segment%20Infomation%20Table%E7%BB%93%E6%9E%84.md\nNAT 用于记录Node的地址，这是解决wandering tree的关键设计。\n详见：https://github.com/RiweiPan/F2FS-NOTES/blob/master/F2FS-Layout/Node%20Address%20Table%E7%BB%93%E6%9E%84.md\nSSA 用于反向索引。通过SIT和NAT可以正向索引到主区域中的block。在这里可以通过block找到所属的node。也就是可以实现通过物理地址找到逻辑地址。GC的时候很有用。\n同时它也具有通过journal缓存sit或者nat的操作用于数据恢复的作用。\n详见：https://github.com/RiweiPan/F2FS-NOTES/blob/master/F2FS-Layout/Segment%20Summary%20Area%E7%BB%93%E6%9E%84.md\nF2FS中的journal 在f2fs读写文件的时候，每修改一个块，都会改动f2fs_node的地址映射和NAT、SIT信息。为了避免频繁刷盘，f2fs将这种小变化写到journal里，在CP的时候再写入磁盘。\njournal相关的数据结构有\n1 2 3 4 5 6 7 8 9 10 11 12 struct f2fs_journal { union { __le16 n_nats; /* 这个journal里面包含多少个nat_journal对象 */ __le16 n_sits; /* 这个journal里面包含多少个sit_journal对象 */ }; /* spare area is used by NAT or SIT journals or extra info */ union { struct nat_journal nat_j; struct sit_journal sit_j; struct f2fs_extra_info info; }; } __packed; 这表示一个f2fs_journal要么保存nat，要么保存sit。\nnat journal:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 struct nat_journal { struct nat_journal_entry entries[NAT_JOURNAL_ENTRIES]; __u8 reserved[NAT_JOURNAL_RESERVED]; } __packed; struct nat_journal_entry { __le32 nid; struct f2fs_nat_entry ne; } __packed; struct f2fs_nat_entry { __u8 version;\t/* latest version of cached nat entry */ __le32 ino;\t/* inode number */ __le32 block_addr;\t/* block address */ } __packed; sit journal\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 struct sit_journal { struct sit_journal_entry entries[SIT_JOURNAL_ENTRIES]; __u8 reserved[SIT_JOURNAL_RESERVED]; } __packed; struct sit_journal_entry { __le32 segno; struct f2fs_sit_entry se; } __packed; struct f2fs_sit_entry { __le16 vblocks;\t/* reference above */ __u8 valid_map[SIT_VBLOCK_MAP_SIZE];\t/* SIT_VBLOCK_MAP_SIZE = 64，64 * 8 = 512 可以表示每一个块的valid状态 */ __le64 mtime;\t/* segment age for cleaning */ } __packed; 在f2fs_get_node_info中，首先在journal中找到地址。如果没找到，再去读盘上的f2fs_nat_block。\n在cp的时候，将journal的信息写盘。\nf2fs_flush_nat_entries 和 f2fs_flush_sit_entries 函数将entry都写入到 curseg_info-\u0026gt;f2fs_summary-\u0026gt;journal 的变量中。 do_checkpoint函数 读取 curseg_info-\u0026gt;f2fs_summary，然后通过函数f2fs_write_node_summaries或f2fs_write_data_summaries 刷写到磁盘中。 F2FS中文件的组织 文件分为数据和元数据两部分：数据是指文件保存的内容，元数据是指为了管理和组织数据而附加的结构。\n一般文件采用inode-data模式，即一个inode代表管理一个文件的抓手，通过inode的操作可以访问文件数据。\nF2FS中的inode包括Metadata部分和数据块寻址部分。Metadata部分是用于描述文件的元数据，数据块寻址部分是用来找到文件中给定index的块。\n文件寻址 直接记录数据块地址的元数据称为dnode。一个dnode中包含许多index-\u0026gt;数据地址的映射条目。\ndnode数量太多，无法全部放入inode中，因此存在indnode，用来存放index-\u0026gt;dnode的映射条目。\n由于inode自己保存了923个数据块地址，所以inode也可以算作dnode。\n每个node page的大小都是4K。\ninode本身保存了923个数据块的地址，称为direct pointer f2fs_inode-\u0026gt;i_nid[0~1] 保存了两个 direct_node 的地址，这里可以保存 2 x 1018个block的数据 f2fs_inode-\u0026gt;i_nid[2~3] 保存了两个indirect_node 的地址，这两个其中2个indirect_node保存的是 direct_node 的nid，因此可以保存 2 x 1018 x 1018个block的数据; f2fs_inode-\u0026gt;i_nid[4] 保存了一个indirect_node 的地址，这个indirect_node保存的是 indirect_node 的nid，因此可以保存 1018 x 1018 x 1018个页的数据 可以得到如下计算公式: 4KB x (923 + 2 x 1018 + 2 x 1018 x 1018 + 1 x 1018 x 1018 x 1018) = 3.93TB 因此F2FS单个文件最多了保存3.93TB数据。\nnode 结构 这里说的是盘上结构。为了方便管理，inode、dnode和indnode都用f2fs_node描述。用footer区分，nid == ino则是inode。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct f2fs_node { union { struct f2fs_inode i; struct direct_node dn; struct indirect_node in; }; struct node_footer footer; // footer用于记录node的类型 } __packed; struct node_footer { __le32 nid;\t/* node id */ __le32 ino;\t/* inode nunmber */ __le32 flag;\t/* include cold/fsync/dentry marks and offset */ __le64 cp_ver;\t/* checkpoint version */ __le32 next_blkaddr;\t/* next node page block address */ } __packed; 其中，inode的结构为\n1 2 3 4 5 6 struct f2fs_inode { ... __le32 i_addr[DEF_ADDRS_PER_INODE]; // DEF_ADDRS_PER_INODE=923 __le32 i_nid[DEF_NIDS_PER_INODE];\t// DEF_NIDS_PER_INODE=5 ... } __packed; 数组i_addr保存了直接指向的923个地址，下标是数据块的index。\n数组i_nid保存了指向的dnode和indnode。下标已经解释过。\ndnode的结构为\n1 2 3 struct direct_node { __le32 addr[ADDRS_PER_BLOCK]; // ADDRS_PER_BLOCK=1018 } __packed; 数组addr保存了直接指向的1018个地址，下标是处理后的index。\nindnode的结构为\n1 2 3 struct indirect_node { __le32 nid[NIDS_PER_BLOCK]; // NIDS_PER_BLOCK=1018 } __packed; 数组nid保存了指向的dnode的nid，下标是处理后的index。\n注意：指向node的都是nid而不是node的地址，因为node的地址在NAT中。\n假设一个文件的数据块发生修改，只需要将数据块异地更新后的新地址填入dnode中（这也是异地更新），再在NAT中更新dnode的地址（就地更新）。这样就解决了wandering tree问题。\n内联数据 小于922 * 4=3688B的文件，数据可以直接放在inode中。\n详见：https://github.com/RiweiPan/F2FS-NOTES/blob/master/Reading-and-Writing/file_data_structure.md\nF2FS中的特殊文件 f2fs中有3个特殊的文件，其inode号分别为1、2、3。\n1 2 3 set_sb(node_ino, 1); set_sb(meta_ino, 2); set_sb(root_ino, 3); node_inode 盘上除了数据外，还有node。文件系统将node读入内存后，放在node_inode对应的文件中便于管理和访问。\nnode_inode文件的data page 里保存了文件系统中读入内存的 node 的数据，顺序更新。根据 nid 可以从 node inode 的 mapping 里获取对应的 page，之后通过 f2fs_get_node_info 从 NAT 获取 nid 对应的 blkaddr，最后提交 io 获取 page。接口为 f2fs_get_node_page。\n在 inode 创建时，通过 f2fs_new_inode_page 创建 node page。另外在保留 data block 时，如果现有 node 空间不足以存放 data block addr，则需要通过 ALLOC_NODE 模式调用 f2fs_get_dnode_of_data，在内部创建 node page。\nmeta_inode 盘上还有元数据区域，如SSA、NAT等。这些数据在读入内存后，放在meta_inode文件中管理。\nroot_inode 文件系统在挂载之后有一个根目录。root_inode对应的就是根目录的目录文件。\ncompress_inode 如果开启了压缩功能，还有一个compress_inode，这里略过。\n特殊文件的ino号是从1开始的，因为 nid = 0 被视作未分配，或者无效。参考 __get_node_page。另外在 truncate_dnode 里，nid == 0 即视作已经 truncate。\nIPU 和 OPU ipu，in-place-update 就地更新：在原地更新数据。传统文件系统如 ext4 都采用。\nopu，out-of-place-update 异地更新：将更新后的数据写在新的地址，修改映射到新地址。\n分配一个新的物理地址 将数据写入新的物理地址 将旧的物理地址无效掉，然后等GC回收 更新逻辑地址和物理地址的映射关系 异地更新更适合闪存特性。 OPU 的缺点在于（1）产生无效块，造成 GC 开销；（2）更新元数据的开销；（3）数据碎片化\n参考资料 https://github.com/RiweiPan/F2FS-NOTES/blob/master/F2FS-Layout/Superblock结构.md\nhttps://mp.weixin.qq.com/s/9bQSO1rAZIAv9-VMXOfaCA\nhttps://mp.weixin.qq.com/s/FbGyxclb1Gesk8_apEexPQ\nhttps://www.cnblogs.com/liuchao719/p/some_special_node_id_in_F2FS.html\nhttps://www.cnblogs.com/liuchao719/p/some_special_node_id_in_F2FS.html\n","date":"2024-12-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/concepts_on_f2fs_filesystem/","title":"F2FS 文件系统相关概念"},{"content":"\n开机以后查看内核信息，总是报fd0相关的错误。这是因为系统启动的时候加载了软盘驱动，但是没有软盘。所以fd0是用不了的。可以忽略这些信息，也可以禁用软盘。\n","date":"2024-12-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/fd0/","title":"linux中查看内核信息总是报fd0错"},{"content":"Linux中的数据完整性（Data Integrity） 文件系统在检验数据完整性的时候，总是在读取数据的时候才会计算元数据的校验和（checksum），以查验数据是否有损坏。然而此时可能数据刚刚写入，也可能数据已经写入几个月甚至更久了。如果数据刚写入不久，也许还能找到原始数据重新写入。如果间隔太久了，就只能丢失了（或采用纠删码技术等）。\n为了解决这一问题，SCSI协议中增加了完整性元数据（integrity metadata, IMD)，也叫保护信息(Protective Information, PI)。也就是说在写入数据的时候，向每个扇区附加8个字节的保护信息。\n注意：在这里，扇区（sector）、块（block）、页（page）可以理解成同样的概念。为了兼容以前HDD时代的叫法，虽然SSD中没有扇区的概念，但我们还是把硬盘中的最小单位叫做扇区。也就是说，扇区是硬盘中的概念，在数据从硬盘读到内存的过程中，会经过文件系统。而在文件系统中，数据的最小处理单位是块。当数据读到内存中以后，便以页的形式表现，并由内存管理系统中的段、页管理系统等管理。而SSD的扇区大小通常是4KB，文件系统中的块、内存中的页通常也是4KB。总而言之，这三个概念只是同一段数据在不同载体上的不同名称。\n附加保护信息以后，IO链路中的每个节点，如文件系统，SCSI控制器等收到上层发来的文件数据以后，都可以计算校验码检测数据是否完整。\n在实现中会遇到一些问题，一个是传统的块大小都是4KB，如果加上8B变成4104B，不方便操作系统处理。另一个是软件方法计算CRC校验码的开销很大。\n针对第一个问题，可以采用聚散列表（scatter-gather list）的方式管理保护信息。控制器将在写时交叉（interleave）缓冲区，在读时分割（split）缓冲区。这意味着Linux可以在不改变页缓存（page cache）的情况下将数据缓冲区DMA到主机内存。这个具体的实现要之后看一下代码才知道。\n针对第二个问题，可以在操作系统层面采用开销较小的校验和，如IP校验和等。\n数据和完整性元数据缓冲区的分离，以及校验和的选择被称为数据完整性扩展（Data Integrity Extensions）\n附：Linux关于数据完整性的文档\nhttps://www.kernel.org/doc/Documentation/block/data-integrity.rst\n","date":"2024-12-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/data_integrity_in_linux/","title":"Linux中的数据完整性（Data Integrity）"},{"content":"事务的原子性和文件系统的崩溃一致性 事务的原子性是指一个事务要么被执行，要么不被执行，不能执行一半。\n在文件系统中，由于存在突然掉电的情况，会导致文件系统的不一致。这就是文件系统的崩溃一致性（Crash Consistency）问题。\n例 1：\n一个事务包含多个写 IO，删除文件的步骤有\n删除该文件的条目； 将文件占用的块在 bitmap 中置为 0。 如果在 1、2 之间掉电，会导致上电后文件系统不一致。 例 2：\n一个事务包含一个写 IO，一个写 IO 在从文件系统到硬件设备的每一个环节都很有可能被分解为多个 IO。如 write_data_pages-\u0026gt;write_data_page；在 IO 控制器、硬盘上往往会由缓冲 buffer 分割写入。如果一个文件没有完全落盘就突然掉电，则该文件可能修改了一部分，造成不一致。\n注意：读 IO 不会影响原子性和一致性。\n关于文件系统不一致的例子参看file-journaling.pdf，一般会有垃圾数据、空间泄漏和文件系统元数据结构不一致等问题。只有一种情况，即只写了数据却没有更新 inode 和 bitmap 这两个元数据的时候，才没有造成文件系统不一致，只是用户丢失了数据。\n解决方法 要解决文件系统的崩溃一致性问题，需要保证文件系统从一个稳定的状态原子地转移到另一个稳定的状态。\nlazy approach：fsck （fs check）检查文件系统状态 早期文件系统放任不一致，上电以后再解决这个问题。fsck 是一个在挂载文件系统之前检查并修复文件系统不一致的 Unix 工具。\n检查超级块，成员数值是否合理。 检查 inode、dnode 等和 bitmap 是否一致。不一致则以前者为准。 检查 inode 是否完整，如有没有类型等。有问题的 inode 直接清除并更新 bitmap。 检查 inode 的实际引用计数和 inode 自己记录的计数是否一致，以前者为准。如果找不到对于某个文件的引用，就放到 lost+found 目录下。 检查块的引用。如果有两个 inode 指向同一个块，要么清除一个坏的 inode，要么给他们各自复制一份。 检查 inode、dnode 指针合理性，如果指向不存在的地址，就清除。 检查目录合理性，如首先要含有.和..，目录包含的 inode 是分配过的，目录要满足树形结构。 构建 fsck 需要非常了解该文件系统的只是，并且 fsck 太慢了，不适用于 RAID。 日志文件系统 为了避免写 IO 途中断电带来的不一致性，在文件系统层面可以采用日志技术（Journaling 或者 Write-Ahead-Logging）。很多文件系统，如 ext3/4、JFS、XFS 和 NTFS 等都是日志文件系统。 文件系统日志（Journal 或 log）是硬盘中一个单独的文件。向盘中写数据的时候，先不写入真正的位置，而是将这个动作记录在 journal 里。这样，在真正写盘的时候即使掉电，也可以通过 journal 恢复。\n写入 以 ext3 （ext 系列文件系统参考这里）为例\n如果向某个文件追加一个 Db 块，则在 log 中写入新的 inode、bitmap 和 Db 以及 addr。并标记首尾。接下来将这三个 IO 实际写入盘上。这叫做 checkpoint。\n在 journal 的时候断电怎么办？应该先写前 4 个块，成功返回后再写 TxE，保证原子性。写前 4 个块叫做 journal write，写 TxE 叫做 journal commit。 注意 HDD 可以保证 512B 以下写入的原子性。或者，在 ext4 中，首尾标志引入了 checksum，可以通过比较存储的 checksum 和计算出的是否一致判断有没有原子写入 journal。\n恢复 在 journal commit 之前断电，我们不恢复。在 checkpoint 途中断电，可以通过 journal 恢复。\nwrite barrier 为了保证写入的顺序，可以等待上一个写入返回后再下发下一个写入。但是由于数据写到写 buffer 里面就返回了，这样还是没办法保证写入顺序。write barrier 是一个操作系统发送给盘的命令，当这个命令完成时，保证命令之前的写入全部完成，并且命令之后的写入全部没有完成。\nredo logging 扫描所有已经 commit 的 journal，在盘上把 journal 的内容重新执行一遍。存在冗余写但是肯定一致。\nbatching log updates 不必每个操作都作为事务。可以积攒一段时间再写入 log。\ncircular log 已经 cp 的事务，它的 journal 可以 free 了。循环使用 log 空间。可以在 journal super block 里面标记有用的 journal。\nmetadata journaling 不把数据写到 journal 里，只写元数据。这要求先将数据落盘，然后将元数据的更新写到 journal 里，最后对这个元数据的事务做 cp，并释放相关 journal。NTFS 和 XFS 都这样干，Ext3 可以选择要不要这样干。 不过，数据和元数据其实是可以一起写，只要保证数据写下去之后再 commit 就可以了。 崩溃一致性的核心：先写内容（数据），再写指向内容的指针（元数据）。\n参考资料 https://blog.csdn.net/TV8MbnO2Y2RfU/article/details/78103771\nhttps://pages.cs.wisc.edu/~remzi/OSTEP/file-journaling.pdf\n","date":"2024-12-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/atomicity_of_transaction_and_crash_consistency_of_file_system/","title":"事务的原子性和文件系统的崩溃一致性"},{"content":"inode inode 用来存储文件的元数据，包括普通文件、目录文件、块设备文件和字符设备文件等。读取某个文件之前，需要先将其 inode 读到内存中来，通过 inode 索引其数据块位置。\nVFS 中的 inode 就叫 struct inode，具体文件系统的 inode 一般叫做 xxx_inode_info。\n除根目录 inode 外，所有 inode 都在一个全局哈希表 inode_hashtable 中，便于查找某个文件系统特定 inode。这是为了管理内存中的 inode。如果 inode 在内存中，就通过哈希表找到，而不是从盘上再读出来。毕竟我们只知道 inode 号。\n每个 inode 还包含在所属文件系统的链表中（super_block-\u0026gt;s_inodes、inode-\u0026gt;i_sb_list），对文件的操作通过 inode 中的 inode 操作表（i_op）和 file 操作表（i_fop）完成。\ninode 操作表中提供的操作与文件的元数据有关，file 操作表中的操作与文件内容本身的读写。\ndentry dentry 用来存储文件在内核文件系统树中的位置。目录、常规文件、符号链接、块设备文件、字符设备文件等文件系统对象都有 dentry。同样分为盘上 dentry、VFS dentry 和文件系统 dentry。\nVFS dentry 通过 d_fsdata 指向文件系统 dentry（f2fs 中没有这一项），通过 d_op 指向文件系统提供的 dentry 操作表（f2fs 中也没有这个）。\ndentry 和 inode 是多对一的关系，每个 dentry 只有一个 inode，由 d_inode 指向。每个 inode 可能有多个 dentry，由 i_dentry 指向 dentry 链表，例如硬链接。\ndentry 之间有一对多的父子关系，d_parent 指向父 dentry，根目录的 dentry 是自己的父 dentry。d_subdirs 指向子 dentry 链表。\n除了根 dentry 外，所有内存中的 dentry 加入到 dentry_hashtable 全局哈希表，便于查找给定目录下的文件。\n孤儿节点 orphan node orphan node 是无主的 node，orphan inode 是指一个文件不在任何目录中，不能被用户访问到。这样的文件仍然被某些进程占用，当没有进程使用时，该文件就被删除了。\n文件的地址空间 文件的地址空间用来管理文件映射到内存的页面，将文件系统中 file 的数据与内存 page cache 或者 swap cache 相关页面绑定到一起。这样就可以将盘上实际不连续的数据以页面为单位连续呈现出来。\n表示地址空间的数据结构是 address_space 结构体，表示地址空间操作表的数据机构是 address_space_operations（a_ops）。\n地址空间的属主（host）是 inode。对于块设备文件是其主 inode。\n文件的地址空间以页面为单位组织成基数树。\n地址空间的操作一般是对具体 page 的操作。包括 writepage、readpage 等。将内存中的某个 page 落盘或者将盘上某个 page 的数据读取到内存中。\n注意，i_fop 中那些操作都是针对内存的。只有在这里才是真正的 IO。\n读取文件时\n以文件的f_mapping为参数，通过find_get_page查找page cache 查找到page cache且数据是最新的，就通过copy_page_to_iter，将数据拷贝到用户空间 没找到，就通过page_cache_alloc分配一个新页面，并将其加入page cache和LRU链表 然后调用对应的readpage函数，从磁盘中读入文件数据 最后还是通过copy_page_to_iter，将数据拷贝到用户空间 文件系统中的扩展属性xattr 文件的基础属性包括 inode ID、创建时间和大小等，比较有限。扩展属性 xattr 是一种允许用户为文件添加自定义属性的方法。xattr 以键值对的方式储存在文件外部。\n通过 API 使用 xattr setfattr 设置属性，getfattr 获取属性。\nxattr 的实现，参看文件系统相关概念。\n参考资料 https://blog.csdn.net/jinking01/article/details/106490467\nhttps://blog.csdn.net/u010039418/article/details/115773253\n《文件系统技术内幕》\n","date":"2024-12-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/concepts_on_file/","title":"文件的相关概念"},{"content":" Cover image from Unsplash\n文件系统相关概念 硬盘、文件系统和 VFS 上的数据结构 很多数据结构都有三个版本，即持久化到盘上的版本、内存中文件系统的版本和内存中 VFS 使用的版本。例如超级块就有盘上的超级块、VFS 使用的 super_block 和文件系统使用的 xxx_sb_info。 这是因为 Linux 使用 VFS 向应用提供统一的文件系统操作接口，而每个文件系统具体的实现不一样。因此应用通过 VFS 的超级块来访问 xxx_sb_info 中的文件系统元数据和调用具体文件系统的操作函数。\n但是上面的结构都是在内存中的，掉电后就没有了。而文件系统存在的意义就是管理持久化的数据。因此还需要将整个文件系统的元数据也保存在盘上。这就是盘上的超级块。这个超级块保存在盘上特定的区域，这样一上电就能读出来。\n文件系统类型 要使用某种文件系统，必须先将其注册到 VFS 核心。注册的内容是文件系统类型 file_system_type。注册之后才能编译、装载。不再使用的文件系统类型应从 VFS 核心注销。\n注册文件系统就是将该文件系统类型插入一个 file_systems 单链表中。目的是向 VFS 提供 get_sb 和 kill_sb 回调函数，从而装载或卸载该类型的文件系统。\n超级块 超级块用来保存整个文件系统的元数据。\n超级块有盘上的和内存中的。内存中的超级块有 VFS 使用的共有的超级块和文件系统特有的超级块。前者叫做 super_block，后者一般叫做 xxx_sb_info。\nVFS 超级块可以指向 fs 超级块（s_fs_info）以及 fs 提供的超级块操作表(s_op)。file_system_type 结构体将该类型文件系统实例对应的超级块实例放在 fs_supers 链表中，并通过 VFS 超级块的 s_instances 成员访问该链表。所有的 VFS 超级块对象还存在于另一个循环双链表中，通过 s_list 域链接。\n对文件系统的操作通过 VFS 超级块调用具体文件系统提供的操作表（s_op）中的回调函数完成。文件系统提供的操作一般有\n分配 inode：alloc_inode。文件系统可能有自己的 inode 结构 xxx_inode_info，也要一起分配。 销毁 inode：destroy_inode。若有则同时销毁 xxx_inode_info。 置脏 inode：dirty_inode。标记修改过的 inode ，便于写回操作。 写回 inode：write_inode。将磁盘上的 inode 读出来，用内存中的 inode 去更新它。然后往往不会执行 IO 写回，而是将更新过的 inode 置脏。 在删除 inode 之前做善后工作：drop_inode 删除 inode：delete_inode 释放超级块：put_super 将超级块写到磁盘上：write_super 做 cp：sync_fs 锁住和解锁文件系统：freeze_fs 和 unfreeze_fs 省略 inode inode 用来存储文件系统对象的元数据，包括普通文件、目录文件、块设备文件和字符设备文件等。读取某个文件之前，需要先将其 inode 读到内存中来，通过 inode 索引其数据块位置。\nVFS 中的 inode 就叫 struct inode，具体文件系统的 inode 一般叫做 xxx_inode_info。\n除根目录 inode 外，所有 inode 都在一个全局哈希表 inode_hashtable 中，便于查找某个文件系统特定 inode。这是为了管理内存中的 inode。如果 inode 在内存中，就通过哈希表找到，而不是从盘上再读出来。毕竟我们只知道 inode 号。\n每个 inode 还包含在所属文件系统的链表中（super_block-\u0026gt;s_inodes、inode-\u0026gt;i_sb_list），对文件的操作通过 inode 中的 inode 操作表（i_op）和 file 操作表（i_fop）完成。\ninode 操作表中提供的操作与文件的元数据有关，file 操作表中的操作与文件内容本身的读写。\ndentry dentry 用来存储文件在内核文件系统树中的位置。目录、常规文件、符号链接、块设备文件、字符设备文件等文件系统对象都有 dentry。同样分为盘上 dentry、VFS dentry 和文件系统 dentry。\nVFS dentry 通过 d_fsdata 指向文件系统 dentry（f2fs 中没有这一项），通过 d_op 指向文件系统提供的 dentry 操作表（f2fs 中也没有这个）。\ndentry 和 inode 是多对一的关系，每个 dentry 只有一个 inode，由 d_inode 指向。每个 inode 可能有多个 dentry，由 i_dentry 指向 dentry 链表，例如硬链接。\ndentry 之间有一对多的父子关系，d_parent 指向父 dentry，根目录的 dentry 是自己的父 dentry。d_subdirs 指向子 dentry 链表。\n除了根 dentry 外，所有内存中的 dentry 加入到 dentry_hashtable 全局哈希表，便于查找给定目录下的文件。\nvfsmount vfsmount 反映了一个已经装载的文件系统实例，将文件系统连接到全局文件系统树。\nvfsmount 也有父子关系，例如 linux 的根文件系统是 ext4，将 f2fs 装载到某个目录下，则 ext4 是 f2fs 的父文件系统。\n除了根 vfsmount 外，所有的 vfsmount 也加入 mount_hashtable 全局哈希表，为了查找装载到特定装载点的文件系统。\n在 linux 内核中一个文件的位置需要\u0026lt;vfsmount, dentry\u0026gt;二元组来确定，先确定挂载点，再从挂载点的相对位置找到文件。\nxattr 在文件系统中的实现 xattr参看文件相关概念。\nNTFS 在 Windows 的 NTFS 文件系统中没有扩展属性的概念，而是有 ADS（Alternate Data Stream）的概念。\nExt2 在 Linux 中，以 Ext2 为例。xattr 的内容存储在一个单独的逻辑块中，由描述头、扩展属性项和值组成。 534520072.jpg entry 存储了键和值的偏移量等信息。键向下生长、值向上生长。但是由于键和值的长度是可变的，因此 entry 和 value 的长度也是可变的，必须通过遍历 entry 才能找到键，通过 offs 才能找到 value。\nF2FS 为了搞清楚 F2FS 中 xattr 的实现方法，必须解答三个问题：xattr 怎样存储，怎样设置，怎样查询。\n数据结构 f2fs 中也存在 xattr 的 header 和 entry。\n1 2 3 4 5 6 7 8 9 10 11 12 struct f2fs_xattr_header { __le32 h_magic; /* magic number for identification */ __le32 h_refcount; /* reference count */ __u32 h_reserved[4]; /* zero right now */ }; struct f2fs_xattr_entry { __u8 e_name_index; __u8 e_name_len; __le16 e_value_size; /* size of attribute value */ char e_name[]; /* attribute name */ }; 接口 xattr 是 linux 的文件系统中广泛支持的功能，VFS 通过一系列函数接口调用具体文件系统的实现。f2fs/xattr.c 中的f2fs_xattr_user_handler、f2fs_xattr_trusted_handler、f2fs_xattr_advise_handler 和f2fs_xattr_security_handler 是 f2fs 中针对 xattr 的接口。\n装载文件系统 装载子文件系统时，装载点目录不一定为空。装载后原来的子目录和文件都无法访问，只有卸载子文件系统后才能重新访问。\n检查装载目录路径有效性 构造子文件系统装载实例 使用现有的或创建新的超级块实例，关联到 vfsmount 对象 填充超级块 fill_super 将该装载实例挂到父文件系统的装载点，添加到内核的文件系统树，实际就是一些链表操作和填充变量操作 ","date":"2024-12-07T00:00:00Z","image":"https://realcedriccheng.github.io/p/concepts_on_filesystem/filesystem_hu_210c968fd401f57.jpg","permalink":"https://realcedriccheng.github.io/p/concepts_on_filesystem/","title":"文件系统相关概念"},{"content":"由于实验用到的汇编代码是riscv的，不能直接用gdb调试，而要用riscv64-unknown-elf-gdb。这个版本的gdb属于riscv工具链。\n要安装riscv工具链，可以下载源代码自己编译，也可以下载预编译的二进制文件。这里我从ttps://mirror.iscas.ac.cn/riscv-toolchains/release/riscv-collab/riscv-gnu-toolchain/LatestRelease/或者https://github.com/riscv-collab/riscv-gnu-toolchain/releases选择适合的版本下载，我下载的是riscv64-elf-ubuntu-18.04-nightly-2022.11.12-nightly.tar.gz。\n在ubuntu中解压后，将riscv目录复制到要安装的地点，再将安装目录下的riscv/bin添加到环境变量。查看版本证明安装成功。\n1 riscv64-unknown-elf-gdb --version 注意：\n从https://static.dev.sifive.com/dev-tools/freedom-tools/v2020.08/riscv64-unknown-elf-gcc-10.1.0-2020.08.2-x86_64-linux-ubuntu14.tar.gz下载的预编译工具不好，没有TUI界面。\n要把riscv/bin目录添加到环境变量，而不是riscv\n其实解压出来也就是安装了，只是要把用到的二进制文件添加到PATH以便使用\n","date":"2024-12-07T00:00:00Z","permalink":"https://realcedriccheng.github.io/p/riscv64-unknown-elf-gdb/","title":"配置riscv64-unknown-elf-gdb环境"}]